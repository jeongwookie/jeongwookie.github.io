<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JW&#39;s Blog</title>
  
  <subtitle>Make stuff people want</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jeongwookie.github.io/"/>
  <updated>2020-02-26T11:19:37.624Z</updated>
  <id>https://jeongwookie.github.io/</id>
  
  <author>
    <name>Jeongwook, Kim</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>패키지 없이 트위터 데이터 수집하기 with Python</title>
    <link href="https://jeongwookie.github.io/2020/02/25/200225-twitter-data-crawling-without-package/"/>
    <id>https://jeongwookie.github.io/2020/02/25/200225-twitter-data-crawling-without-package/</id>
    <published>2020-02-25T09:27:19.000Z</published>
    <updated>2020-02-26T11:19:37.624Z</updated>
    
    <content type="html"><![CDATA[<p>이전에 트위터 데이터를 키워드를 기준으로 크롤링하는 글을 쓴적이 있다. 최근 내가 진행하는 연구에서도 트위터 크롤링이 계속 요구되었고, 정말 다양한 패키지와 방법을 사용해 왔다.</p><p>Tweepy나 TwitterScraper 등 좋은 패키지들이 github에 많이 공유되어 있는데, 뭔가 내 맘에 드는 게 없어서 순정으로 돌아가보기로 했다. </p><p>이번 포스트에서는 <u>일절 패키지 없이 트위터 API로만 크롤링</u>을 시도해 볼 것이다. 요즘 대단히 이슈가 되는 코로나 바이러스에 대해서 키워드를 설정하고, 관련 트윗을 수집해 보자!</p><p><img src="https://user-images.githubusercontent.com/25416425/75331914-d8a9d880-58c6-11ea-8781-1ae4dce07e5d.jpg" width="400"></p><a id="more"></a><h2 id="트위터-개발자-등록하기"><a href="#트위터-개발자-등록하기" class="headerlink" title="트위터 개발자 등록하기"></a>트위터 개발자 등록하기</h2><p>먼저, 트위터 공식 API를 사용하려면 인증키를 받아야 한다.<br>본 포스트에서는 관련 프로세스들을 다루지 않겠다. 너무 기본적인 세팅이라.. 각자 알아서 등록하도록!</p><p>간단하게 노트하면,</p><ol><li><a href="https://developer.twitter.com/" rel="external nofollow noopener noreferrer" target="_blank">트위터 개발자 홈페이지</a>에 접속해서 개발자로 신청하기</li><li>App 만들고, OAuth Key 발급 받기</li></ol><p>2번 프로세스에 대해 다른 블로그들 중 설명이 잘 되어있는 곳을 찾았다. <a href="https://ericnjennifer.github.io/python_crawling/2018/01/05/PythonCrawling_Chapt3.html" rel="external nofollow noopener noreferrer" target="_blank">Mark Lee 님의 블로그</a>를 참고해서 키를 발급 받아 옵시다..!!</p><p><img src="https://user-images.githubusercontent.com/25416425/75333305-3b9c6f00-58c9-11ea-9b7f-b65d52b57b6b.png" width="400"></p><p>위와 같이 키를 확인할 수 있다면, 준비는 끝났다.</p><h2 id="트위터-API-연결하기"><a href="#트위터-API-연결하기" class="headerlink" title="트위터 API 연결하기"></a>트위터 API 연결하기</h2><p>이제 파이참 또는 주피터를 열 시간이다.<br>본격적으로 트위터를 수집하기 전, 위에서 발급받은 인증키를 연동시켜, 제대로 연결되었는지 status를 먼저 확인해 보자.</p><p>트위터 공식 API 문서에서는 다양한 형태의 OAuth를 지원하는데, 우리는 OAuth2 를 사용하여 인증할 것이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 트위터 API 개발자 키를 아래에 입력</span></span><br><span class="line">client_key = <span class="string">'YOUR-CLIENT-KEY'</span></span><br><span class="line">client_secret = <span class="string">'YOUR-CLIENT-SECRET-KEY'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># b64 encoded 형태로 만드는 과정 </span></span><br><span class="line">key_secret = <span class="string">'&#123;&#125;:&#123;&#125;'</span>.format(client_key, client_secret).encode(<span class="string">'ascii'</span>)</span><br><span class="line">b64_encoded_key = base64.b64encode(key_secret)</span><br><span class="line">b64_encoded_key = b64_encoded_key.decode(<span class="string">'ascii'</span>)</span><br></pre></td></tr></table></figure><p>위에서는 b64 형태로 인코딩 된 키를 만들었다. 이제 이를 통해서 트위터 API와 연결하는 코드를 작성한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># request에 필요한 url 만들기</span></span><br><span class="line">base_url = <span class="string">'https://api.twitter.com/'</span></span><br><span class="line">auth_url = <span class="string">'&#123;&#125;oauth2/token'</span>.format(base_url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># HEADER 구성하기</span></span><br><span class="line">auth_headers = &#123;</span><br><span class="line">    <span class="string">'Authorization'</span>: <span class="string">'Basic &#123;&#125;'</span>.format(b64_encoded_key),</span><br><span class="line">    <span class="string">'Content-Type'</span>: <span class="string">'application/x-www-form-urlencoded;charset=UTF-8'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Authentication Data section 만들기</span></span><br><span class="line">auth_data = &#123;</span><br><span class="line">    <span class="string">'grant_type'</span>: <span class="string">'client_credentials'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># POST request를 보내서 status 확인!</span></span><br><span class="line">auth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)</span><br><span class="line">print(auth_resp.status_code)</span><br></pre></td></tr></table></figure><p>status_code 가 200이 출력되면, 정상적으로 연결된 것이다.</p><h2 id="데이터-수집하기"><a href="#데이터-수집하기" class="headerlink" title="데이터 수집하기"></a>데이터 수집하기</h2><p>위의 코드까지는 세팅이라고 할 수 있으며, 지금부터가 실제 트위터 검색에 사용될 파라미터를 정의하는 구간이다.<br>코로나 바이러스와 연관된 트윗을 수집하는 것이 목표이므로, 키워드를 <strong>“우한폐렴”</strong> 및 <strong>“코로나”</strong> 라고 정했다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bearer token 정의하기</span></span><br><span class="line">access_token = auth_resp.json()[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Search HEADER 구성하기</span></span><br><span class="line">search_headers = &#123;</span><br><span class="line">    <span class="string">'Authorization'</span>: <span class="string">'Bearer &#123;&#125;'</span>.format(access_token)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># SEARCH TWEET</span></span><br><span class="line"><span class="comment"># Maximum number of tweets returned from a single token is 18,000 </span></span><br><span class="line"></span><br><span class="line">search_params = &#123;</span><br><span class="line">    <span class="string">'q'</span>:<span class="string">'우한폐렴 OR 코로나'</span>,</span><br><span class="line">    <span class="string">'result_type'</span>: <span class="string">'recent'</span>, <span class="comment"># 'mixed' or 'popular' 로도 지정 가능</span></span><br><span class="line">    <span class="string">'count'</span>:<span class="number">10</span>, <span class="comment"># 디폴트 값은 15이며, 최대 100까지 지정 가능</span></span><br><span class="line">    <span class="string">'retryonratelimit'</span>:<span class="literal">True</span>, <span class="comment"># rate limit에 도달했을 때 자동으로 다시 trial</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">search_url = <span class="string">'&#123;&#125;1.1/search/tweets.json'</span>.format(base_url)</span><br><span class="line">search_resp = requests.get(</span><br><span class="line">    search_url, headers=search_headers, </span><br><span class="line">    params=search_params</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>위 코드를 살행하면, <code>search_resp</code>에 우리가 원하는 결과값이 저장된다. 위에서는 간단하게 <strong>“최근 10개의 우한폐렴 또는 코로나 라는 단어가 포함된 트윗을 가져와!”</strong> 라고 <code>search_params</code>을 지정했지만, 더 다양한 옵션들이 존재한다. 궁금하면 <a href="https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets" rel="external nofollow noopener noreferrer" target="_blank">트위터 공식 API 문서</a>를 참고하자.</p><h2 id="Rate-Limit-확인하기"><a href="#Rate-Limit-확인하기" class="headerlink" title="Rate Limit 확인하기"></a>Rate Limit 확인하기</h2><p>위에서는 간단하게 10개의 트윗만 수집했지만, 1000개 또는 그 이상의 트윗을 수집하려는 사용자도 분명 있을 것이다. (물론 나도 위 방법으로 100만개 이상을 수집해 왔으니..) 이런 경우에는 트위터에서 명시해 놓은 Rate Limit에 대해서 민감하게 코드를 작성할 필요가 있다.</p><p>방금 위 코드를 실행시켰다면, 트위터 서버에 1번 데이터를 요청한 셈이 된다. 트위터에서 얼마나 요청을 받아줄까?<br>아래의 코드는 rate limit을 확인할 수 있게 해준다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rate limit URL</span></span><br><span class="line">url = <span class="string">'https://api.twitter.com/1.1/application/rate_limit_status.json'</span></span><br><span class="line">search_resp = requests.get(url, headers=search_headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 확인하기</span></span><br><span class="line">json.loads(search_resp.content)[<span class="string">'resources'</span>][<span class="string">'search'</span>]</span><br></pre></td></tr></table></figure><p>위 코드를 그대로 실행시키면, 트위터에서 제공하는 limit가 얼마이고 이 중 현재 남은 사용량이 얼마나 되는지 직관적으로 확인할 수 있다.</p><p>다만, 현재는 출력되는 limit가 450 으로 나오는데 직접 코드를 돌려보니 허용량을 초과하지 않았는데도 크롤러가 멈추는 현상이 있었다. 정확하게 확인하려면 <a href="https://developer.twitter.com/en/docs/developer-utilities/rate-limit-status/api-reference/get-application-rate_limit_status" rel="external nofollow noopener noreferrer" target="_blank">트위터 공식 API Ref</a>을 참고하자. 여기서는 <strong>15분 당 180번 요청 허용</strong> 이라고 나와 있었다.</p><p>참고로, 나는 <code>search_resp.content</code> 내의 정보를 확인하고, error가 존재하는 경우 크롤러를 강제로 15분동안 쉬도록 코드를 작성해서 사용했다.</p><h2 id="데이터-확인하기"><a href="#데이터-확인하기" class="headerlink" title="데이터 확인하기"></a>데이터 확인하기</h2><p>데이터 수집은 완료했고.. 이제 수집된 데이터를 확인해보자.<br>10개의 트윗을 예시로 수집하였고, 이를 <code>pandas</code>로 읽으면 편하다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">Data = json.loads(search_resp.content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dict -&gt; Dataframe</span></span><br><span class="line">df = pd.DataFrame(Data[<span class="string">'statuses'</span>])</span><br><span class="line">df[[<span class="string">"id"</span>,<span class="string">'created_at'</span>,<span class="string">'text'</span>,<span class="string">'retweet_count'</span>]] <span class="comment"># 몇 개의 칼럼만 확인해보기</span></span><br></pre></td></tr></table></figure><p><img src="https://user-images.githubusercontent.com/25416425/75337214-e879ea80-58cf-11ea-91d8-aeb1a00fe3b8.png" width="700"></p><p>잘 보인다! 총 28개의 칼럼이 있는데, 이 중 관심이 있는 4개의 칼럼만 예시로 출력한 것이다. GOOD!!</p><h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h2><p>이런 식으로 패키지 없이 트위터에서 제공하는 공식 API로만 트위터를 수집할 수 있다.<br>다만, 공짜로 이용하는 것이기 때문에 기본적으로 standard API와 동일한 제약이 따른다. 무엇인고 하니, rate limit와 같은 속성도 있지만 무엇보다도 full-archive를 보장받지 못한다. 즉, 2월 20일자 트윗 중 “우한폐렴” 및 “코로나” 가 포함된 트윗을 모두 수집하라고 코드를 실행시켜도 모든 트윗을 긁어왔다고 보장해 주지 않는다.</p><p>아래는 트윗 10개 말고 100개를 긁어왔을 때 간단하게 frequency를 1초 별로 그려본 것이다. 예쁘게 잘 나오는군 후후</p><p><img src="https://user-images.githubusercontent.com/25416425/75339811-92f40c80-58d4-11ea-9cb2-c38f48489bf6.png" width="400"></p><p>끝!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;이전에 트위터 데이터를 키워드를 기준으로 크롤링하는 글을 쓴적이 있다. 최근 내가 진행하는 연구에서도 트위터 크롤링이 계속 요구되었고, 정말 다양한 패키지와 방법을 사용해 왔다.&lt;/p&gt;
&lt;p&gt;Tweepy나 TwitterScraper 등 좋은 패키지들이 github에 많이 공유되어 있는데, 뭔가 내 맘에 드는 게 없어서 순정으로 돌아가보기로 했다. &lt;/p&gt;
&lt;p&gt;이번 포스트에서는 &lt;u&gt;일절 패키지 없이 트위터 API로만 크롤링&lt;/u&gt;을 시도해 볼 것이다. 요즘 대단히 이슈가 되는 코로나 바이러스에 대해서 키워드를 설정하고, 관련 트윗을 수집해 보자!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25416425/75331914-d8a9d880-58c6-11ea-8781-1ae4dce07e5d.jpg&quot; width=&quot;400&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Data Crawling" scheme="https://jeongwookie.github.io/categories/Programming/Data-Crawling/"/>
    
    
      <category term="Python" scheme="https://jeongwookie.github.io/tags/Python/"/>
    
      <category term="Datamining" scheme="https://jeongwookie.github.io/tags/Datamining/"/>
    
      <category term="Twitter" scheme="https://jeongwookie.github.io/tags/Twitter/"/>
    
  </entry>
  
  <entry>
    <title>Khaiii 형태소 분석기 사용하기</title>
    <link href="https://jeongwookie.github.io/2019/11/17/191117-khaiii-korean-tokenizer/"/>
    <id>https://jeongwookie.github.io/2019/11/17/191117-khaiii-korean-tokenizer/</id>
    <published>2019-11-17T09:36:23.000Z</published>
    <updated>2019-11-17T12:54:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>한국어로 된 데이터를 분석할 때, 이를 적절한 형태로 토크나이즈 (tokenize)하는 과정은 반드시 필요하다.</p><p>특히나 한국어는 영어와 달리 최소 의미 전달이 단어로 이루어진 언어가 아니기 때문에 형태소 단위로 잘라주는 패키지를 자주 사용한다.</p><p>오늘은 내가 평소에 자주 사용하는 형태소 분석기 중 <strong>Khaiii 형태소 분석기</strong> 에 대해 포스팅하고자 한다.</p><p><strong>Khaiii</strong> 는 카카오에서 18년도 말에 공개한 딥러닝 기반의 형태소 분석기이다. 기존의 사전 의존 방법과는 달리 세종 코퍼스 약 1000만 어절을 학습하여 형태소 단위로 분리한다고 한다.</p><p><img src="https://user-images.githubusercontent.com/25416425/69005815-8b82a600-096a-11ea-85da-f30967fa6f26.png" width="450"></p><a id="more"></a><h2 id="설치하기"><a href="#설치하기" class="headerlink" title="설치하기"></a>설치하기</h2><p>설치가 생각보다 까다롭다. 오래전에 설치하여 정확한 프로세스는 잘 기억이 나지 않지만, 상당히 많은 에러 동반 끝에 설치한 기억이..</p><p>기본적으로 리눅스 환경에서만 지원한다. 윈도우에서도 할 수 있는 방법이 있는지는 잘 모르겠다.</p><p>나는 한번 설치의 어려움을 경험한 후, 다시는 리눅스 커멘드로 일일히 설치하지 않는다. 대신 도커파일을 받아서 그대로 활용한다. 도커 파일의 경우 <a href="https://github.com/kakao/khaiii/blob/master/docker/Dockerfile" rel="external nofollow noopener noreferrer" target="_blank">여기</a>를 참고하자.</p><p>위 도커파일로 Khaiii를 설치 후, 원하는 세팅을 해서 나만의 도커 이미지를 만들어 놓으면 세팅 하는데 시간을 획기적으로 줄일 수 있다. 도커 사용법의 경우 시간 날때 천천히 정리할 계획.</p><h2 id="사용하기"><a href="#사용하기" class="headerlink" title="사용하기"></a>사용하기</h2><p>설치를 완료했다면, 이제 사용해 보자. 주피터 노트북에서 아래와 같은 코드를 실행한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> khaiii <span class="keyword">import</span> KhaiiiApi</span><br><span class="line">api = KhaiiiApi(rsc_dir=<span class="string">'/deps/khaiii/build/share/khaiii'</span>) <span class="comment"># 내 설치 경로</span></span><br><span class="line"></span><br><span class="line">morphs = []</span><br><span class="line">sentence = <span class="string">"하스스톤 전장이 새로 나왔는데 재밌어요!"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> api.analyze(sentence):</span><br><span class="line">    <span class="keyword">for</span> morph <span class="keyword">in</span> word.morphs:</span><br><span class="line">        morphs.append((morph.lex, morph.tag))</span><br></pre></td></tr></table></figure><p>적당한 sentence로 출력을 해보면 아래와 같이 형태소 단위로 잘 토크나이즈 된 것을 확인할 수 있다.</p><p><img src="https://user-images.githubusercontent.com/25416425/69006362-81b07100-0971-11ea-87cf-aae710ba18c1.png" width="350"></p><p>위 코드에서 <code>rsc_dir</code> 부분은 현재는 중요하지 않지만, 기분석 사전이나 오분석 패치 기능을 활용할 때 엄청 중요하다. 먼저 짚고 넘어가보자.</p><p>대부분이 비슷한 경로에 설치가 될 것 같은데 혹시 모르니 커멘드 창에 아래와 같은 코드를 입력한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find / -name <span class="string">'khaiii*'</span> -type d</span><br></pre></td></tr></table></figure><p>khaiii 라는 이름을 가진 디렉토리를 모두 찾으라는 뜻이다. 코드를 실행하여 <code>/deps/khaiii/build/share/khaiii&#39;</code> 와 비슷하게 share 폴더 안의 khaiii의 경로를 찾으면 된다.</p><h2 id="기분석-사전-추가"><a href="#기분석-사전-추가" class="headerlink" title="기분석 사전 추가"></a>기분석 사전 추가</h2><p>기분석 사전이란 내가 원하는 단일 어절에 대해, 문맥에 상관없이 일괄적인 분석 결과를 얻고 싶을 때 추가한다.</p><p>기본적인 사용 방법은 <a href="https://github.com/kakao/khaiii/wiki/%EA%B8%B0%EB%B6%84%EC%84%9D-%EC%82%AC%EC%A0%84" rel="external nofollow noopener noreferrer" target="_blank">Khaiii 깃헙 페이지</a>에 잘 정리되어 있다.</p><p>페이지에 적힌 대로 사전 형식은 따라하면 되는데, 내가 문제를 겪은 부분은 다름 아닌 사전 빌드였다.</p><p><img src="https://user-images.githubusercontent.com/25416425/69006983-d99fa580-097a-11ea-8517-d54d5ee830af.png" width="550"></p><p>요약하자면, modulenotfounderror: no module named ‘khaiii.munjong’ 이런 에러가 뜬다.</p><p>대부분이 <code>PYTHONPATH</code> 경로 문제인데, 위와 유사한 방법으로 찾아보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find / -name <span class="string">'munjong*'</span> -type d</span><br></pre></td></tr></table></figure><p>위 코드로 <code>munjong</code> 이 있는 디렉토리 위치를 찾는다. 나는 약 4개의 디렉토리 위치가 떴는데, 이중에서 적당히 파이썬 위치가 어딜까 하고 보니 보인다.</p><p><img src="https://user-images.githubusercontent.com/25416425/69007060-2768dd80-097c-11ea-881d-3a4a3dbe53e4.png" width="550"></p><p>이제 위치를 찾았으니, <code>PYTHONPATH</code> 경로를 수정한 코드를 차례대로 커멘드 창에 입력하여 사전을 빌드하면 된다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd rsc <span class="comment"># 어디에 있던 rsc로 와서 실행하기</span></span><br><span class="line">mkdir -p ../build/share/khaiii</span><br><span class="line">PYTHONPATH=/deps/khaiii/src/main/python/ ./bin/compile_preanal.py --rsc-src=./src --rsc-dir=/deps/khaiii/build/share/khaiii</span><br></pre></td></tr></table></figure><p>빌드 완료! 혹시 빌드한 사전이 적용이 안된다면, <code>rsc_dir</code> 경로를 다시 확인하자.</p><h2 id="오분석-패치"><a href="#오분석-패치" class="headerlink" title="오분석 패치"></a>오분석 패치</h2><p>오분석 패치는 기계학습 모델의 결과로 출력된 결과가 오분석일 경우, 이를 원하는 정분석으로 바로잡을 수 있는 사용자 사전이다.</p><p>기분석 사전과 마찬가지로 기본적인 사용 방법은 <a href="https://github.com/kakao/khaiii/wiki/%EC%98%A4%EB%B6%84%EC%84%9D-%ED%8C%A8%EC%B9%98" rel="external nofollow noopener noreferrer" target="_blank">Khaiii 깃헙 페이지</a>에 잘 정리되어 있다.</p><p>정해진 포멧을 맞추어 등록하고, 이를 빌드하면 되는데 위의 오분석 패치와 마찬가지로 약간의 코드 수정이 필요하다. (PYTHONPATH 부분)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd rsc <span class="comment"># 어디에 있던 rsc로 와서 실행하기</span></span><br><span class="line">mkdir -p ../build/share/khaiii</span><br><span class="line">PYTHONPATH=/deps/khaiii/src/main/python/  ./bin/compile_errpatch.py --model-size=base --rsc-src ./src --rsc-dir=/deps/khaiii/build/share/khaiii</span><br></pre></td></tr></table></figure><p>빌드 완료!</p><h2 id="맺음말"><a href="#맺음말" class="headerlink" title="맺음말"></a>맺음말</h2><p>Khaiii 형태소 분석기를 사용해보면 딥러닝 기반인데 사전 기반처럼 정확하게 작동함을 확인할 수 있다.</p><p>그러나, 아쉬운 점은 띄어쓰기가 잘 안된 비문들을 넣으면 기존에 잘 사용하는 <code>Mecab</code> 형태소 분석기보다 성능이 조금 떨어진다.</p><p>그리고 내가 다루는 데이터 안에 ‘후쿠시마’ 라는 단어가 있는데, 이를 <code>Mecab</code>을 사용했을 땐 몰랐는데 자꾸 후 + 쿠시마 또는 후쿠시 + 마 로 오분석이 되더라.. 기분석 사전에 추가하는 번거로움이 있었다.</p><p>서로 장단점이 있으니 적절하게 사용하면 될 것 같다.</p><p>기분석 사전 및 오분석 패치 사전 작성 시에 기본 한글 형태소 품사표를 알고 싶다면 <a href="http://kkma.snu.ac.kr/documents/?doc=postag" rel="external nofollow noopener noreferrer" target="_blank">한글 형태소 품사 (POS) 태그표</a>를 참고하자.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;한국어로 된 데이터를 분석할 때, 이를 적절한 형태로 토크나이즈 (tokenize)하는 과정은 반드시 필요하다.&lt;/p&gt;
&lt;p&gt;특히나 한국어는 영어와 달리 최소 의미 전달이 단어로 이루어진 언어가 아니기 때문에 형태소 단위로 잘라주는 패키지를 자주 사용한다.&lt;/p&gt;
&lt;p&gt;오늘은 내가 평소에 자주 사용하는 형태소 분석기 중 &lt;strong&gt;Khaiii 형태소 분석기&lt;/strong&gt; 에 대해 포스팅하고자 한다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Khaiii&lt;/strong&gt; 는 카카오에서 18년도 말에 공개한 딥러닝 기반의 형태소 분석기이다. 기존의 사전 의존 방법과는 달리 세종 코퍼스 약 1000만 어절을 학습하여 형태소 단위로 분리한다고 한다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25416425/69005815-8b82a600-096a-11ea-85da-f30967fa6f26.png&quot; width=&quot;450&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Data preprocessing" scheme="https://jeongwookie.github.io/categories/Programming/Data-preprocessing/"/>
    
    
      <category term="Python" scheme="https://jeongwookie.github.io/tags/Python/"/>
    
      <category term="Datamining" scheme="https://jeongwookie.github.io/tags/Datamining/"/>
    
      <category term="Korean NLP" scheme="https://jeongwookie.github.io/tags/Korean-NLP/"/>
    
  </entry>
  
  <entry>
    <title>2019년 여름, 연구실에서 근황</title>
    <link href="https://jeongwookie.github.io/2019/07/23/190723-my-summer-these-days/"/>
    <id>https://jeongwookie.github.io/2019/07/23/190723-my-summer-these-days/</id>
    <published>2019-07-23T12:53:57.000Z</published>
    <updated>2019-07-23T13:11:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>쓰고 싶은 주제들은 많은데.. 요즘 블로그를 거의 못하고 있다 ㅠㅠ</p><p>덥기도 하고, 연구실에서 해야할 일도 많고 행사도 있고 정신이 없는듯.</p><p>매일 운동도 빠짐없이 하고 싶은데 밥을 제때 못먹어서 힘이 없어서 못하기도 하고, 미팅이 길어져서 타이밍을 놓치기도 하고..</p><p>그래도 간간히 블로그 들러주셔서 메일 주시는 분들이 몇 계시다는 데에 만족중 ㅋㅋ</p><p>현재 두 가지의 프로젝트에 involve 하는 중인데, 열심히는 하고 있지만 아직 갈길이 멀었다.</p><ol><li><p>네이버 뉴스 및 댓글 데이터를 바탕으로 정치적 편향성을 가진 사용자의 반응 연구</p></li><li><p>원자력 발전소와 관련한 트윗을 크롤링하여 루머의 확산에 관한 연구</p></li></ol><p>방학 때 좀 빡세게 해서 결과가 좀 나오면.. 탑 티어 컨퍼런스인 CHI나 ICWSM에 도전해 보고 싶다!! </p><p>이외에도, 최근 한 일중에 미래에 도움될 만한 일이 있다면.. <strong>도커 (Docker)</strong> 세팅을 해본 것?</p><p>처음 만져보니까 버벅대긴 했는데.. 어찌어찌 해서 나의 전용 도커파일도 만들고 내가 원하는 자연어 처리 세팅을 완료했다.</p><p>또 막상 끝내고 나니까 가르쳐주는 건 쉽더라. 시간 될 때 리마인드 겸 도커 세팅과 관련된 포스트를 작성할 계획이다.</p><p>친구들한테 물어보니, 기업에서 도커 다루는 것은 거의 필수라고 하더라고 ㅋㅋㅋ 공부한 셈 치고 머리 박은 시간들은 넘어가는 걸로. 다음엔 안헤메겠지뭐!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;쓰고 싶은 주제들은 많은데.. 요즘 블로그를 거의 못하고 있다 ㅠㅠ&lt;/p&gt;
&lt;p&gt;덥기도 하고, 연구실에서 해야할 일도 많고 행사도 있고 정신이 없는듯.&lt;/p&gt;
&lt;p&gt;매일 운동도 빠짐없이 하고 싶은데 밥을 제때 못먹어서 힘이 없어서 못하기도 하고,
      
    
    </summary>
    
      <category term="Diary" scheme="https://jeongwookie.github.io/categories/Diary/"/>
    
    
      <category term="Daily life" scheme="https://jeongwookie.github.io/tags/Daily-life/"/>
    
  </entry>
  
  <entry>
    <title>Data Crawling - 크롤러 속도를 높이는 멀티프로세싱 (multiprocessing)</title>
    <link href="https://jeongwookie.github.io/2019/06/29/190629-multiprocessing-crawler/"/>
    <id>https://jeongwookie.github.io/2019/06/29/190629-multiprocessing-crawler/</id>
    <published>2019-06-29T09:01:17.000Z</published>
    <updated>2019-06-29T16:01:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>이번에는 크롤러의 속도를 높이는 방법 중 하나를 알아보자.</p><p>우리가 10000개의 유저 데이터를 수집한다고 가정하면, 지금까지는 처음부터 차례대로 하나씩 수집한 것이다.</p><p>그런데 만약 동일한 기능을 하는 프로그램 창을 여러개 띄우고, 2500개씩 나누어 4개의 창으로 동시에 데이터를 수집하면 어떨까?</p><p>이런 것을 가능하게 하는 것이 바로 파이썬의 기본 모듈 중 하나인 멀티프로세싱 (multiprocessing)이다.</p><p><img src="https://user-images.githubusercontent.com/25416425/60382111-6237ab00-9a99-11e9-8500-c76666317467.png" width="450"></p><a id="more"></a><p>지금부터, 트위터에서 유저 데이터 (username, joined date, total tweets, followings, followers)를 멀티프로세싱을 통해 속도를 개선한 크롤러로 수집해 보겠다.</p><p>수집할 트윗은 요즘 결승 라운드를 시작하여 매우 핫한 “슈퍼밴드” 를 포함한 트윗으로, 6월 21일부터 6월 28일까지의 기간으로 정했다.</p><p>편의를 위해 트윗을 수집할 때 직접 수집하지 않고 앞서 사용하였던 <strong>GetOldTweet3</strong> 을 임포트 하였다. 자세한 내용은 <a href="https://jeongwookie.github.io/2019/06/10/190610-twitter-data-crawling/">여기</a>를 참고하자.</p><h2 id="트윗-수집하기"><a href="#트윗-수집하기" class="headerlink" title="트윗 수집하기"></a>트윗 수집하기</h2><p>먼저, <strong>GetOldTweet3</strong> 을 사용하여 특정 검색어를 포함한 트윗을 먼저 수집하고, 이런 트윗을 작성한 유저들의 닉네임 (username)을 리스트로 반환해 보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import packages</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> GetOldTweets3 <span class="keyword">as</span> got</span><br><span class="line"></span><br><span class="line"><span class="comment"># 트윗 수집하는 함수 정의</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tweets</span><span class="params">(start_date, end_date, keyword)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 범위 끝을 포함하게 만듬</span></span><br><span class="line">    end_date = (datetime.datetime.strptime(end_date, <span class="string">"%Y-%m-%d"</span>) </span><br><span class="line">                + datetime.timedelta(days=<span class="number">1</span>)).strftime(<span class="string">"%Y-%m-%d"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 트윗 수집 기준 설정</span></span><br><span class="line">    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(<span class="string">'&#123;&#125;'</span>.format(keyword))\</span><br><span class="line">                                            .setSince(start_date)\</span><br><span class="line">                                            .setUntil(end_date)\</span><br><span class="line">                                            .setMaxTweets(<span class="number">-1</span>) <span class="comment"># 모두 수집</span></span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"==&gt; Collecting data start.."</span>)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    tweets = got.manager.TweetManager.getTweets(tweetCriteria)</span><br><span class="line">    print(<span class="string">"==&gt; Collecting data end.. &#123;0:0.2f&#125; minutes"</span>.format((time.time() - start_time)/<span class="number">60</span>))</span><br><span class="line">    print(<span class="string">"=== Total number of tweets is &#123;&#125; ==="</span>.format(len(tweets)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tweets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 유저 리스트 반환하는 함수 정의</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_users</span><span class="params">(tweets)</span>:</span></span><br><span class="line">    </span><br><span class="line">    user_list = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> tweets:</span><br><span class="line">        username = index.username</span><br><span class="line">        user_list.append(username)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> user_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 유저 리스트 수집하기</span></span><br><span class="line">tweets = get_tweets(<span class="string">"2019-06-21"</span>, <span class="string">"2019-06-28"</span>, <span class="string">"슈퍼밴드"</span>)</span><br><span class="line">users = get_users(tweets)</span><br><span class="line"></span><br><span class="line">&gt; ==&gt; Collecting data start..</span><br><span class="line">&gt; ==&gt; Collecting data end.. 4.45 minutes</span><br><span class="line">&gt; === Total number of tweets <span class="keyword">is</span> <span class="number">3456</span> ===</span><br></pre></td></tr></table></figure><p><code>get_tweets()</code>으로 2019년 06월 21일부터 2019년 06년 28일까지 키워드 “슈퍼밴드” 를 포함한 트윗을 먼저 수집하고, <code>get_users()</code>으로 트윗을 작성한 유저의 닉네임을 리스트로 만들어 <code>users</code>에 저장하였다.</p><p>총 5분 가량 소요되었고, 조건을 만족하는 트윗은 3456개임을 확인하였다.</p><h2 id="logger-정의하기"><a href="#logger-정의하기" class="headerlink" title="logger 정의하기"></a>logger 정의하기</h2><p>본격적으로 멀티프로세싱에 들어가기 전, 효과적으로 결과를 볼 수 있도록 도와주는 로깅 (logging)을 먼저 소개하고자 한다.</p><p>로깅 (logging)이란 현재 우리의 프로그램이 어떤 상태를 가지고 있는지 외부 출력을 하게 만들어서, 개발자들이 프로그램의 상황을 직접 눈으로 확인할 수 있도록 하는 것이다.</p><p>얼핏 보면 우리가 지금까지 사용한 <code>print()</code>와 유사하지만, 다양한 옵션의 출력을 미리 세팅해 둘 수 있어서 훨씬 유연하게 상황에 따라 대처할 수 있다.</p><p>코드부터 살펴보자. 파이썬 기본 모듈이므로 따로 설치할 필요는 없다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> logging.handlers</span><br><span class="line"></span><br><span class="line"><span class="comment"># logging 설정</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_logger</span><span class="params">()</span>:</span></span><br><span class="line">    logger = logging.getLogger(<span class="string">"my"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> len(logger.handlers) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> logger</span><br><span class="line">    </span><br><span class="line">    logger.setLevel(logging.INFO)</span><br><span class="line">    stream_hander = logging.StreamHandler()</span><br><span class="line">    logger.addHandler(stream_hander)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> logger</span><br></pre></td></tr></table></figure><p><em>my</em> 라는 로거 (logger)를 정의하고, <code>INFO</code> 이상의 등급에 해상하는 로그만 출력하도록 <code>setLevel()</code>을 통해 설정하였다. 또한 프로그램 실행과 동시에 결과를 보여줄 수 있도록 핸들러를 정의하고, 이를 콘솔창에 출력하도록 하였다.</p><p>또한, 로그가 중복되어 두번씩 출력되는 현상을 방지하기 위해 <code>logging.handlers</code>를 이미 불러온 경우에는 또 불러오지 않도록 중간에 if문을 넣었다.</p><p>대단히 간단한 옵션들만 사용한 것인데, 자세한 내용은 <a href="https://hamait.tistory.com/880" rel="external nofollow noopener noreferrer" target="_blank">이승현님의 블로그</a>를 참고하자. 여러가지 옵션에 대해서 상세히 다루어 놓았다.</p><h2 id="유저-데이터-수집하기"><a href="#유저-데이터-수집하기" class="headerlink" title="유저 데이터 수집하기"></a>유저 데이터 수집하기</h2><p>아까 수집해 놓은 유저 닉네임 (username)을 바탕으로, 유저 데이터를 수집해 보자.</p><p>수집에 사용할 툴은 <code>bs4</code> 패키지의 <code>BeautlfulSoup</code>이며, <em>lxml</em> 형식으로 데이터를 받아올 것이다. </p><p>수집할 유저 데이터는 유저 닉네임, 가입일, 전체 작성 트윗수, 팔로워수, 팔로잉수 이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl_userdata</span><span class="params">(username)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># setting</span></span><br><span class="line">    url = <span class="string">'https://twitter.com/&#123;&#125;'</span>.format(username)</span><br><span class="line">    mylogger.info(<span class="string">"&#123;&#125; 유저의 데이터 수집 시작"</span>.format(username))</span><br><span class="line">    HEADER = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'</span>&#125;</span><br><span class="line">    response = requests.get(url, headers=HEADER)</span><br><span class="line">    html = response.text</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parsing</span></span><br><span class="line">    soup = BeautifulSoup(html, <span class="string">"lxml"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parsing fail</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        user_profile_header = soup.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>:<span class="string">'ProfileHeaderCard'</span>&#125;)</span><br><span class="line">        user_profile_canopy = soup.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>:<span class="string">'ProfileCanopy-nav'</span>&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># data collect</span></span><br><span class="line">        user = user_profile_header.find(<span class="string">'a'</span>, &#123;<span class="string">'class'</span>:<span class="string">'ProfileHeaderCard-nameLink u-textInheritColor js-nav'</span>&#125;)[<span class="string">'href'</span>].strip(<span class="string">"/"</span>) </span><br><span class="line"></span><br><span class="line">        date_joined = user_profile_header.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">"ProfileHeaderCard-joinDate"</span>&#125;).find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>:<span class="string">'ProfileHeaderCard-joinDateText js-tooltip u-dir'</span>&#125;)[<span class="string">'title'</span>]</span><br><span class="line">        date_joined = date_joined.split(<span class="string">"-"</span>)[<span class="number">1</span>].strip()</span><br><span class="line">        <span class="keyword">if</span> date_joined <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            data_joined = <span class="string">"Unknown"</span></span><br><span class="line"></span><br><span class="line">        tweets = user_profile_canopy.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>:<span class="string">"ProfileNav-value"</span>&#125;)[<span class="string">'data-count'</span>]</span><br><span class="line">        <span class="keyword">if</span> tweets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tweets = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">except</span> AttributeError:</span><br><span class="line">        mylogger.info(<span class="string">"&#123;&#125; 유저의 데이터 수집 중 알수없는 오류가 발생했습니다."</span>.format(username))</span><br><span class="line">        mylogger.info(<span class="string">"링크 : &#123;&#125;"</span>.format(url))</span><br><span class="line">        user, date_joined, tweets, following, followers = username, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 블락 계정 특징 : 팔로워, 팔로잉 수가 안보임</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line"></span><br><span class="line">        test_following = user_profile_canopy.find(<span class="string">'li'</span>, &#123;<span class="string">'class'</span>:<span class="string">"ProfileNav-item ProfileNav-item--following"</span>&#125;)</span><br><span class="line">        test_followers = user_profile_canopy.find(<span class="string">'li'</span>, &#123;<span class="string">'class'</span>:<span class="string">"ProfileNav-item ProfileNav-item--followers"</span>&#125;)</span><br><span class="line"></span><br><span class="line">        following = test_following.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>:<span class="string">"ProfileNav-value"</span>&#125;)[<span class="string">'data-count'</span>]</span><br><span class="line">        followers = test_followers.find(<span class="string">'span'</span>, &#123;<span class="string">'class'</span>:<span class="string">"ProfileNav-value"</span>&#125;)[<span class="string">'data-count'</span>]</span><br><span class="line"></span><br><span class="line">        mylogger.info(<span class="string">"&#123;&#125; 유저의 데이터 수집 완료"</span>.format(username))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> AttributeError:</span><br><span class="line">        mylogger.info(<span class="string">"&#123;&#125; 유저는 블락된 계정입니다."</span>.format(username))</span><br><span class="line">        following = <span class="string">"Block"</span></span><br><span class="line">        followers = <span class="string">"Block"</span></span><br><span class="line"></span><br><span class="line">    result = [user, date_joined, tweets, following, followers]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>트위터 데이터를 어려번 수집하여 보니 안 사실인데, 트위터의 정책상 블락 (block)된 유저들은 가입일이나 전체 작성한 트윗 수 등은 조회가 가능하지만, 팔로잉 수나 팔로워 수는 보이지 않는다. 처음에는 이런 정보를 유저가 선택적으로 오픈할 수 있는 옵션이 있는지 의심했지만, 그런 옵션은 공식적으로 제공하지 않았다.</p><p>중간중간에 <code>try-except</code>문을 사용하여 위와 같은 경우를 방지하였다. <code>BeautifulSoup</code>을 통해 원하는 데이터를 수집하는 부분은 따로 설명하지 않았다. 좀 더 알고 싶다면 <a href="https://jeongwookie.github.io/2019/03/18/190318-naver-finance-data-crawling-using-python/">과거에 작성한 글</a>이 있으니 참고.</p><h2 id="멀티프로세싱-사용하기"><a href="#멀티프로세싱-사용하기" class="headerlink" title="멀티프로세싱 사용하기"></a>멀티프로세싱 사용하기</h2><p>모든 준비가 끝났다. 이제 실제 코드를 멀티프로세싱을 통해서 병렬화 하여 돌리면 된다. (병렬 크롤링)</p><p>유의할 점은, 멀티프로세싱을 사용할 때, <code>main()</code>이 무엇인지 정확히 언급해 주어야 오류를 방지할 수 있다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="comment"># 유저 정보 Multiprocessing</span></span><br><span class="line"><span class="keyword">global</span> user_info</span><br><span class="line">user_info = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    user_list = users</span><br><span class="line">    pool_size = len(user_list)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> pool_size &lt; <span class="number">8</span>:</span><br><span class="line">        pool = Pool(pool_size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pool = Pool(<span class="number">8</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> pool.map(crawl_userdata, user_list):</span><br><span class="line">        user_info.append(user)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    </span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    mylogger = get_logger()</span><br><span class="line">    mylogger.info(<span class="string">"유저 정보 수집 시작"</span>)</span><br><span class="line">    </span><br><span class="line">    main()</span><br><span class="line">    </span><br><span class="line">    end_time = (time.time() - start_time)/<span class="number">60</span></span><br><span class="line">    mylogger.info(<span class="string">"유저 정보 수집 종료.. &#123;0:0.2f&#125; 분 소요"</span>.format(end_time))</span><br><span class="line">    mylogger.info(<span class="string">"총 수집된 유저 정보는 &#123;&#125; 개 입니다."</span>.format(len(user_info)))</span><br></pre></td></tr></table></figure><p><code>Pool</code>은, 몇개의 창을 열어서 동시에 프로그램을 돌릴지 정의하는 함수이다. 위 코드에서는 우리가 수집할 데이터의 갯수가 8 이하일 경우에는 그 숫지만큼, 아닐 경우 8개를 동시에 실행해서 수집하도록 셋팅하였다.</p><p>이어서 <code>pool.map()</code>으로 적용하고 싶은 함수 <code>crawl_userdata()</code>와 그 적용 대상이 되는 <code>user_list</code>을 차례로 적고 for문을 완성한다.</p><p>위 코드를 실행시켜보면, 총 3456개의 유저 정보가 <u>단 10분만에 크롤링 됨</u>을 확인할 수 있다.</p><p><img src="https://user-images.githubusercontent.com/25416425/60386591-e1e16c00-9ad1-11e9-8c41-fca71061bddb.png" width="700"></p><p>결과가 어떤 형태로 나오는지 보여주기 위해, 해당 트윗도 추가해서 위와 같이 출력해 보았다.</p><p>위의 정보 뿐만 아니라 트윗의 업로드 시각, 리트윗 수, 관심글 수, 지역 등의 데이터를 추가로 수집 가능하다.</p><p>이전 다루었던 트위터 데이터 크롤링의 코드로 실행하면 거의 3시간씩 걸린 작업을 멀티 프로세싱을 통해 대단히 빠른 속도로 시간을 단축시켜 보았다.</p><p>하지만, 이는 공격적인 크롤링으로 오인받아 일부 사이트의 경우 차단당할 우려가 있으므로.. 상황에 맞게 조심스레(?) 사용하자.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;이번에는 크롤러의 속도를 높이는 방법 중 하나를 알아보자.&lt;/p&gt;
&lt;p&gt;우리가 10000개의 유저 데이터를 수집한다고 가정하면, 지금까지는 처음부터 차례대로 하나씩 수집한 것이다.&lt;/p&gt;
&lt;p&gt;그런데 만약 동일한 기능을 하는 프로그램 창을 여러개 띄우고, 2500개씩 나누어 4개의 창으로 동시에 데이터를 수집하면 어떨까?&lt;/p&gt;
&lt;p&gt;이런 것을 가능하게 하는 것이 바로 파이썬의 기본 모듈 중 하나인 멀티프로세싱 (multiprocessing)이다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25416425/60382111-6237ab00-9a99-11e9-8500-c76666317467.png&quot; width=&quot;450&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Data Crawling" scheme="https://jeongwookie.github.io/categories/Programming/Data-Crawling/"/>
    
    
      <category term="Python" scheme="https://jeongwookie.github.io/tags/Python/"/>
    
      <category term="Datamining" scheme="https://jeongwookie.github.io/tags/Datamining/"/>
    
      <category term="Twitter" scheme="https://jeongwookie.github.io/tags/Twitter/"/>
    
  </entry>
  
  <entry>
    <title>19년 카이스트 봄학기 후기</title>
    <link href="https://jeongwookie.github.io/2019/06/28/190628-spring-semester-finish/"/>
    <id>https://jeongwookie.github.io/2019/06/28/190628-spring-semester-finish/</id>
    <published>2019-06-28T07:03:25.000Z</published>
    <updated>2019-06-29T08:51:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>카이스트에서의 도전적인(?) 한 학기가 드디어 끝났다.</p><p>정말 많은 일들을 한꺼번에 진행하느라 힘들었지만, 그만큼 의미있었던 한 학기였다.</p><p>나의 무기를 가지고 싶어 시작했던 일들이었던 만큼, 재미있고 가슴뛰는 무언가를 찾아서 움직였던 과거와는 달리 모든 일에 조금 진지하게 임하였던 것 같다.</p><a id="more"></a><p>나는 데이터 가반의 의사 결정 방법이 스타트업의 서비스 설계에 있어 대단히 중요하다고 생각한다.</p><p>단순히 데이터를 쌓아놓고, 그 후에 어떻게 이 데이터를 읽어낼 것인가를 이야기 하는 것이 아니라, 첫 단계부터 어떤 유저들에게 어떤 데이터를 수집할 것인지, 그것을 어떻게 처리하여 다시 우리의 유저에게 어떤 영향을 미칠 것인지 그 시스템 자체를 설계하기를 원했다.</p><p>지금까지 해왔던 것 처럼, 경영 분야의 지식을 익히고 또 다른 창업 팀에서 활동하는 선택지도 분명 있었다.</p><p>하지만, 스스로 해보고 싶어졌다. 창업을 경험하면서 조금만 진전이 없으면 따라오던 답없는 질문.. 내가 정말 잘하는 것이 무엇인가? 나는 정말 대체가능한 인원이 아닌건가? 우리 회사의 성장에 꼭 필요한 인재일까? 하는 의문들…</p><p>나 스스로에게 무기를 쥐어주고 싶었다. 창업이 원래 답없고 실패 확률이 매우 높은 일 아닌가. 뭐라도 믿는 구석이 있어야 뚝심있게 밀고 나갈 수 있는 것 같다. 스스로에게 자신감이 떨어지면 이도저도 어렵지 않을까. </p><p>그래서 다시 코딩을 시작했다. 학부때 기본 과정은 들었고, 수학에 자신이 있었기 때문에 어떻게든 할 수 있지 않을까 생각했다.</p><p>기회가 되어 기초과학연구원의 데이터 사이언스 연구실에서 일할 수 있게 되었고, 데이터를 직접 수집하기 시작했다. </p><p>동시에 카이스트 전산학부 및 산업공학과의 빅데이터, 알고리즘, 머신러닝 및 딥러닝 관련 전공 수업들을 전부 수강했다.</p><p>솔직히 엄청 힘들었다.. 사실 전공 수업만 따라가기도 벅찬데 연구실에서 맡은 프로젝트도 있었고, 창업석사의 메인 트랙인 창업 수업이 기본적으로 조별과제라서 그냥 개인 시간이 거의 없었던 것 같다.</p><p>그래도 끝내고 나니 확실히 기본적인 개념들은 전부 익힌 것 같고, 최신 논문을 읽어도 무슨 말인지 따라갈 수 있게 되었다!</p><p>그리고.. 힘든 것과는 별개로, 더 해보고 싶어졌다. 내가 수집한 데이터에 새로운 모델을 적용해 보고 싶고, 전처리를 더 잘하고 싶고, 수집도 더 우아하게 (?) 해보고 싶고 막 하고 싶은 것들이 쑥쑥 늘어났다. 욕심이 생긴다고 해야할까?</p><p>이번 방학은 연구실에 오롯히 매진할 생각이다. 현재 맡은 프로젝트는 네이버 뉴스 데이터를 기반으로 한 의견성 기사 분류 및 분석, 특정 키워드를 포함한 트위터 데이터 수집 및 분석 등이다. </p><p>프로젝트의 주제들을 보면 한국어 NLP인데, 이번에 구글브레인에서 발표한 XLNet을 적용해보고 싶고.. 이때까지는 형태소 분석기를 Mecab을 사용했는데 딥러닝 기반의 Khaiii를 사용해서 여러 모델들을 적용해 볼 생각이다.</p><p>이와 별개로 연구실에서 Twitch 채팅 로그를 기반으로 Hatespeech 분석, 비언어적 특성 (이모티콘 등) 분석 등을 진행하고 계신 분들이 있는데, 여력이 된다면 조인하고 싶다.</p><blockquote><p>2019년 한국정보과학회 참여 사진</p></blockquote><p><img src="https://user-images.githubusercontent.com/25416425/60381891-34049c00-9a96-11e9-8103-00d00ea2743e.jpeg" width="500"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;카이스트에서의 도전적인(?) 한 학기가 드디어 끝났다.&lt;/p&gt;
&lt;p&gt;정말 많은 일들을 한꺼번에 진행하느라 힘들었지만, 그만큼 의미있었던 한 학기였다.&lt;/p&gt;
&lt;p&gt;나의 무기를 가지고 싶어 시작했던 일들이었던 만큼, 재미있고 가슴뛰는 무언가를 찾아서 움직였던 과거와는 달리 모든 일에 조금 진지하게 임하였던 것 같다.&lt;/p&gt;
    
    </summary>
    
      <category term="Diary" scheme="https://jeongwookie.github.io/categories/Diary/"/>
    
    
      <category term="Daily life" scheme="https://jeongwookie.github.io/tags/Daily-life/"/>
    
  </entry>
  
  <entry>
    <title>Data Crawling - 파이썬을 이용한 트위터 수집하기</title>
    <link href="https://jeongwookie.github.io/2019/06/10/190610-twitter-data-crawling/"/>
    <id>https://jeongwookie.github.io/2019/06/10/190610-twitter-data-crawling/</id>
    <published>2019-06-10T09:24:31.000Z</published>
    <updated>2019-06-10T15:37:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>어떤 사건에 대한 사람들의 즉각적인 관심과 반응, 그리고 영향력자들이 어떻게 일반 사용자에게 영향을 미치는지 분석하는 데는 트위터 만한 SNS가 없다.</p><p>본 포스트에서는 예제로 19년 4월 4주차에서 <u>어벤져스</u> 또는 <u>스포</u>가 포함된 트윗을 수집해 볼 것이다.</p><p>어벤져스 엔드게임은 19년 4월 24일에 개봉해서 엄청난 인기를 끌었는데, 한동안 SNS에서 일명 스포 (스포일러)를 한다고 절대 아무거나 클릭하지 말라고 소동이 있었던 것으로 기억한다.</p><p>개봉일 전후로 트윗 수가 차이가 나고, <u>스포</u>와 같이 작성된 트윗이 많을 것으로 기대된다. </p><p><img src="https://user-images.githubusercontent.com/25416425/59194117-ebf30780-8bc2-11e9-91f1-ab6b84883cd6.png" width="400"></p><a id="more"></a><h2 id="수집-도구-선택"><a href="#수집-도구-선택" class="headerlink" title="수집 도구 선택"></a>수집 도구 선택</h2><p>트위터에서는 공식적으로 데이터 수집을 지원하는 API를 제공하고 있는데, 이름은 <strong>Tweepy</strong>. 최근 자료들을 얻는 데 있어서는 간단하고 빠르다.</p><p>그러나, 이 방법에는 치명적인 단점이 존재하는데.. 바로 현재 시간부터 7일 이전까지의 데이터만 수집이 가능하다는 점이다.</p><p>그 이전의 데이터를 수집하고 싶다면, Premium-Api를 구매해야 한다.. <a href="https://developer.twitter.com/en/premium-apis" rel="external nofollow noopener noreferrer" target="_blank">트위터 개발자 홈페이지</a>에 접속하면 아래와 같은 가격 플랜을 안내하고 있다.</p><p><em>Pricing for the elevated tiers of the Search Tweets: 30-day endpoint start at <strong>$149/month</strong> for 500 requests, while pricing for the Search Tweets: Full-archive endpoint starts at <strong>$99/month</strong> for 100 requests.</em>   </p><p>상당한 비용이 발생함을 알 수 있다. 그럼 다른 방법은 없을까?</p><p>직접 크롤러를 만들수도 있지만 그건 최후의 (?) 수단이고, 누군가 만들어 놓은 도구가 있다면 쓰는 것이 인지상정! 다행히 오래된 트윗을 수집할 수 있는 <a href="https://github.com/Mottl/GetOldTweets3" rel="external nofollow noopener noreferrer" target="_blank"><strong>GetOldTweet3</strong></a> 이라는 패키지가 있다.</p><p><img src="https://user-images.githubusercontent.com/25416425/59187501-901f8300-8bb0-11e9-954d-ca8994b0c726.png" width="700"></p><p>이 패키지를 사용해서 수집할 수 있는 변수들을 확인해 보니, 내가 원하는 것은 대부분 수집이 가능했다.</p><ul><li>업로드 유저 아이디 (username)</li><li>트윗 링크 (permalink)</li><li>트윗 내용 (text)</li><li>업로드 시간 (date)</li><li>리트윗 수 (retweets)</li><li>관심글 수 (favorites)</li></ul><p>또한, 다양한 기준으로 데이터 수집 범위를 설정할 수 있었다.</p><ul><li>특정 유저 아이디로 트윗 검색 (setUsername)</li><li>기간 안의 트윗 검색 (setSince / setUntil)</li><li>특정 검색어가 포함된 트윗 검색 (setQuerySearch)</li><li>기준 위치를 설정하고 근처에서 생성된 트윗 검색 (setNear / setWithin)</li><li>출력할 최대 트윗 수 지정 (setMaxTweets)</li></ul><p>좀 더 자세한 옵션 및 사용 방법은 <a href="https://github.com/Mottl/GetOldTweets3" rel="external nofollow noopener noreferrer" target="_blank">GetOldTweet3 github</a> 을 참고하자.</p><h2 id="패키지-준비"><a href="#패키지-준비" class="headerlink" title="패키지 준비"></a>패키지 준비</h2><p>본격적으로 크롤링에 앞서, 먼저 <strong>GetOldTweet3</strong> 패키지를 설치한다. 본 포스트에서는 python 3.7, ubuntu 18.04 의 개발 환경을 기본으로 한다. Jupyter Notebook으로 코드를 작성하였다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GetOldTweet3 사용 준비</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> GetOldTweets3 <span class="keyword">as</span> got</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    !pip install GetOldTweets3</span><br><span class="line">    <span class="keyword">import</span> GetOldTweets3 <span class="keyword">as</span> got</span><br></pre></td></tr></table></figure><p>또한, 추가적인 데이터 수집을 위해서 이전 포스트에서도 사용하였던 <strong>Beautifulsoup4</strong> 가 설치되어 있는지 확인한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BeautifulSoup4 사용 준비</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    !pip install bs4</span><br><span class="line">    <span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br></pre></td></tr></table></figure><h2 id="수집-기간-정의하기"><a href="#수집-기간-정의하기" class="headerlink" title="수집 기간 정의하기"></a>수집 기간 정의하기</h2><p>본격적으로 크롤러를 만들어 보자. 먼저 <code>datetime</code>을 사용하여 원하는 수집 기간을 정의한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 가져올 범위를 정의</span></span><br><span class="line"><span class="comment"># 예제 : 2019-04-21 ~ 2019-04-24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">days_range = []</span><br><span class="line"></span><br><span class="line">start = datetime.datetime.strptime(<span class="string">"2019-04-21"</span>, <span class="string">"%Y-%m-%d"</span>)</span><br><span class="line">end = datetime.datetime.strptime(<span class="string">"2019-04-25"</span>, <span class="string">"%Y-%m-%d"</span>)</span><br><span class="line">date_generated = [start + datetime.timedelta(days=x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>, (end-start).days)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> date <span class="keyword">in</span> date_generated:</span><br><span class="line">    days_range.append(date.strftime(<span class="string">"%Y-%m-%d"</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"=== 설정된 트윗 수집 기간은 &#123;&#125; 에서 &#123;&#125; 까지 입니다 ==="</span>.format(days_range[<span class="number">0</span>], days_range[<span class="number">-1</span>]))</span><br><span class="line">print(<span class="string">"=== 총 &#123;&#125;일 간의 데이터 수집 ==="</span>.format(len(days_range)))</span><br><span class="line"></span><br><span class="line">&gt; === 설정된 트윗 수집 기간은 <span class="number">2019</span><span class="number">-04</span><span class="number">-21</span> 에서 <span class="number">2019</span><span class="number">-04</span><span class="number">-24</span> 까지 입니다 ===</span><br><span class="line">&gt; === 총 <span class="number">4</span>일 간의 데이터 수집 ===</span><br></pre></td></tr></table></figure><p><code>days_range</code> 라는 이름의 리스트에 날짜를 %Y-%m-%d 형태로 저장해 놓았다.</p><h2 id="트윗-수집하기"><a href="#트윗-수집하기" class="headerlink" title="트윗 수집하기"></a>트윗 수집하기</h2><p>이제 본격적으로 트위터에서 데이터를 크롤링할 차례이다.</p><p><strong>GetOldTweet3</strong>는 <code>tweetCriteria</code>로 수집 기준을 정의할 수 있다.</p><p>앞에서 설정한 수집 기간에서 <u>어벤져스</u> 또는 <u>스포</u> 가 포함된 트윗을 모두 수집해 보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 특정 검색어가 포함된 트윗 검색하기 (quary search)</span></span><br><span class="line"><span class="comment"># 검색어 : 어벤져스, 스포</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 수집 기간 맞추기</span></span><br><span class="line">start_date = days_range[<span class="number">0</span>]</span><br><span class="line">end_date = (datetime.datetime.strptime(days_range[<span class="number">-1</span>], <span class="string">"%Y-%m-%d"</span>) </span><br><span class="line">            + datetime.timedelta(days=<span class="number">1</span>)).strftime(<span class="string">"%Y-%m-%d"</span>) <span class="comment"># setUntil이 끝을 포함하지 않으므로, day + 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 트윗 수집 기준 정의</span></span><br><span class="line">tweetCriteria = got.manager.TweetCriteria().setQuerySearch(<span class="string">'어벤져스 OR 스포'</span>)\</span><br><span class="line">                                           .setSince(start_date)\</span><br><span class="line">                                           .setUntil(end_date)\</span><br><span class="line">                                           .setMaxTweets(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 수집 with GetOldTweet3</span></span><br><span class="line">print(<span class="string">"Collecting data start.. from &#123;&#125; to &#123;&#125;"</span>.format(days_range[<span class="number">0</span>], days_range[<span class="number">-1</span>]))</span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line">tweet = got.manager.TweetManager.getTweets(tweetCriteria)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Collecting data end.. &#123;0:0.2f&#125; Minutes"</span>.format((time.time() - start_time)/<span class="number">60</span>))</span><br><span class="line">print(<span class="string">"=== Total num of tweets is &#123;&#125; ==="</span>.format(len(tweet)))</span><br><span class="line"></span><br><span class="line">&gt; Collecting data start.. <span class="keyword">from</span> <span class="number">2019</span><span class="number">-04</span><span class="number">-21</span> to <span class="number">2019</span><span class="number">-04</span><span class="number">-24</span></span><br><span class="line">&gt; Collecting data end.. <span class="number">41.39</span> Minutes</span><br><span class="line">&gt; === Total num of tweets <span class="keyword">is</span> <span class="number">22964</span> ===</span><br></pre></td></tr></table></figure><p>수집하는 데 시간이 조금 걸린다. 참고로 너무 많은 트윗을 한번에 수집하려다 보면, 트위터 측에서 나가라고 쫒아낸다.. (Error 104)</p><p><em>An error occured during an HTTP request: [Errno 104] Connection reset by peer</em></p><p>Connection 관련한 에러가 뜨면, 지정한 날짜 범위에 기준을 만족하는 트윗의 수가 너무 많은 것이니 범위를 좁혀서 다시 시도해 보자.</p><p>수집하는 데 얼마나 시간이 걸렸는지 알아보기 위해 <code>time</code> 을 임포트 해서 코드 몇줄을 추가했다. 참고로 나는 이 과정에서 1시간 넘게 소요된 적도 있었으니 참을성있게 기다려보자.</p><p>위 코드는 41분 가량 소요되었다. 몇개의 트윗이 수집되었는지 출력되면, 아래 단계로 넘어가자.</p><h2 id="변수-저장하기"><a href="#변수-저장하기" class="headerlink" title="변수 저장하기"></a>변수 저장하기</h2><p>이제 원하는 정보만을 저장해 보자. <strong>GetOldTweet3</strong> 에서 제공하는 기본 변수 중 유저 아이디, 트윗 링크, 트윗 내용, 날짜, 리트윗 수, 관심글 수를 수집한다.</p><p>또한, 이 패키지에서 제공하지 않는 변수 중 각 유저의 가입일, 전체 트윗 수, 팔로잉 수, 팔로워 수도 같이 수집한다. 이때, 앞서 준비한 <strong>BeautifulSoup4</strong> 를 사용한다. 자세한 사용 방법은 이전 포스트들을 참고하자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 원하는 변수 골라서 저장하기</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> uniform</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm_notebook</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize</span></span><br><span class="line">tweet_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> tqdm_notebook(tweet):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 메타데이터 목록 </span></span><br><span class="line">    username = index.username</span><br><span class="line">    link = index.permalink </span><br><span class="line">    content = index.text</span><br><span class="line">    tweet_date = index.date.strftime(<span class="string">"%Y-%m-%d"</span>)</span><br><span class="line">    tweet_time = index.date.strftime(<span class="string">"%H:%M:%S"</span>)</span><br><span class="line">    retweets = index.retweets</span><br><span class="line">    favorites = index.favorites</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># === 유저 정보 수집 시작 ===</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        personal_link = <span class="string">'https://twitter.com/'</span> + username</span><br><span class="line">        bs_obj = get_bs_obj(personal_link)</span><br><span class="line">        uls = bs_obj.find(<span class="string">"ul"</span>, &#123;<span class="string">"class"</span>: <span class="string">"ProfileNav-list"</span>&#125;).find_all(<span class="string">"li"</span>)</span><br><span class="line">        div = bs_obj.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"ProfileHeaderCard-joinDate"</span>&#125;).find_all(<span class="string">"span"</span>)[<span class="number">1</span>][<span class="string">"title"</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 가입일, 전체 트윗 수, 팔로잉 수, 팔로워 수</span></span><br><span class="line">        joined_date = div.split(<span class="string">'-'</span>)[<span class="number">1</span>].strip()</span><br><span class="line">        num_tweets = uls[<span class="number">0</span>].find(<span class="string">"span"</span>, &#123;<span class="string">"class"</span>: <span class="string">"ProfileNav-value"</span>&#125;).text.strip()</span><br><span class="line">        num_following = uls[<span class="number">1</span>].find(<span class="string">"span"</span>, &#123;<span class="string">"class"</span>: <span class="string">"ProfileNav-value"</span>&#125;).text.strip()</span><br><span class="line">        num_follower = uls[<span class="number">2</span>].find(<span class="string">"span"</span>, &#123;<span class="string">"class"</span>: <span class="string">"ProfileNav-value"</span>&#125;).text.strip()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">except</span> AttributeError:</span><br><span class="line">        print(<span class="string">"=== Attribute error occurs at &#123;&#125; ==="</span>.format(link))</span><br><span class="line">        print(<span class="string">"link : &#123;&#125;"</span>.format(personal_link))   </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 결과 합치기</span></span><br><span class="line">    info_list = [tweet_date, tweet_time, username, content, link, retweets, favorites, </span><br><span class="line">                 joined_date, num_tweets, num_following, num_follower]</span><br><span class="line">    tweet_list.append(info_list)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 휴식 </span></span><br><span class="line">    time.sleep(uniform(<span class="number">1</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p><strong>(주의: 실행 시 약 24시간이 소요됩니다. 결과를 빨리 확인하려면 유저 정보 수집 부분을 전부 주석처리 해주세요.)</strong></p><p>유저의 가입일, 전체 트윗 수, 팔로잉 수, 팔로워 수 와 같은 변수는 <strong>GetOldTweet3</strong>으로 얻은 <code>username</code>으로 <code>personal_link</code>을 만들어 수집하였다.</p><p>중간에 <code>try-except</code> 구문을 사용하였는데, 이는 수집을 시도해 보니 몇몇 사용자의 팔로잉 수 혹은 팔로워 수가 공개되어 있지 않아 <code>AttributeError</code>을 발생시키고 있었다. 이런 에러를 발생시키는 계정은 보통 광고용 찌라시 계정이었는데, 이를 확인하기 위해 에러 발생시 그 link를 출력하도록 코드를 구성하였다.</p><p>또한, 공격적인 크롤링 방지를 위해 <code>random.uniform()</code>을 활용하여 아래에 1~2초 사이로 랜덤하게 for문을 쉬게 하는 코드를 추가했다.  </p><p>트윗 수집 결과는 <code>tweet_list</code>에 저장된다.</p><h2 id="파일-저장-후-확인"><a href="#파일-저장-후-확인" class="headerlink" title="파일 저장 후 확인"></a>파일 저장 후 확인</h2><p>이제 결과를 csv 파일로 저장하고, 저장된 파일을 불러와서 확인해 보자. <code>Pandas</code> 패키지를 사용할 것이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파일 저장하기</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">twitter_df = pd.DataFrame(tweet_list, </span><br><span class="line">                          columns = [<span class="string">"date"</span>, <span class="string">"time"</span>, <span class="string">"user_name"</span>, <span class="string">"text"</span>, <span class="string">"link"</span>, <span class="string">"retweet_counts"</span>, <span class="string">"favorite_counts"</span>,</span><br><span class="line">                                    <span class="string">"user_created"</span>, <span class="string">"user_tweets"</span>, <span class="string">"user_followings"</span>, <span class="string">"user_followers"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># csv 파일 만들기</span></span><br><span class="line">twitter_df.to_csv(<span class="string">"sample_twitter_data_&#123;&#125;_to_&#123;&#125;.csv"</span>.format(days_range[<span class="number">0</span>], days_range[<span class="number">-1</span>]), index=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"=== &#123;&#125; tweets are successfully saved ==="</span>.format(len(tweet_list)))</span><br><span class="line"></span><br><span class="line">&gt; === <span class="number">22964</span> tweets are successfully saved ===</span><br></pre></td></tr></table></figure><p>위 코드를 실행시키면, working directory 내에 sample_twitter_data_2019-04-21_to_2019-04-24.csv 파일이 생성되었음을 확인할 수 있다.</p><p>생성한 파일을 로드해서 내용을 확인해 보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파일 확인하기</span></span><br><span class="line"></span><br><span class="line">df_tweet = pd.read_csv(<span class="string">'sample_twitter_data_&#123;&#125;_to_&#123;&#125;.csv'</span>.format(days_range[<span class="number">0</span>], days_range[<span class="number">-1</span>]))</span><br><span class="line">df_tweet.head(<span class="number">10</span>) <span class="comment"># 위에서 10개만 출력</span></span><br></pre></td></tr></table></figure><p><img src="https://user-images.githubusercontent.com/25416425/59205137-2d90ac00-8bdd-11e9-9d3b-fdef803b7e6c.png" width="700"></p><h2 id="데이터-통계-확인"><a href="#데이터-통계-확인" class="headerlink" title="데이터 통계 확인"></a>데이터 통계 확인</h2><p>수집한 데이터라 어떤 특징을 보이고 있는지 간단하게 확인해 보자. </p><p><u>어벤져스</u> 또는 <u>스포</u>가 포함된 트윗을 수집하였는데, 각각의 빈도는 어느 정도일까?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 키워드 빈도 분석하기</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_keywords</span><span class="params">(dataframe)</span>:</span></span><br><span class="line">    keywords = []</span><br><span class="line">    text = dataframe[<span class="string">"text"</span>].lower()</span><br><span class="line">    <span class="keyword">if</span> <span class="string">"어벤져스"</span> <span class="keyword">in</span> text:</span><br><span class="line">        keywords.append(<span class="string">"어벤져스"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">"스포"</span> <span class="keyword">in</span> text:</span><br><span class="line">        keywords.append(<span class="string">"스포"</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">","</span>.join(keywords)</span><br><span class="line"></span><br><span class="line">df_tweet[<span class="string">"keyword"</span>] = df_tweet.apply(get_keywords,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># barplot 그리기</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">counts = df_tweet[<span class="string">"keyword"</span>].value_counts()</span><br><span class="line">plt.bar(range(len(counts)), counts)</span><br><span class="line">plt.title(<span class="string">"Tweets mentioning keywords"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"# of tweets"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(counts)</span><br></pre></td></tr></table></figure><p><img src="https://user-images.githubusercontent.com/25416425/59205869-bcea8f00-8bde-11e9-910e-a7e1ba347476.png" width="450"></p><p>barplot을 그릴 때에는 파이썬의 visualization package 중 가장 유명한 <code>matplotlib</code>을 사용했다.</p><p><u>스포</u> 가 단일로 포함된 트윗이 14,782개로 가장 많았고, 그 뒤로 <u>어벤져스</u> 단일이 6,902개 , 그리고 <u>어벤져스</u> 와 <u>스포</u> 모두 포함된 트윗이 1,248개 로 파악된다.</p><p>이번에는 어벤져스 개봉일이 다가오면서 변화하는 트윗의 빈도를 출력해 보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 날짜별 빈도 분석하기</span></span><br><span class="line"></span><br><span class="line">counts = df_tweet[<span class="string">"date"</span>].value_counts().sort_index()</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"Tweets mentioning keywords in time series"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"# of tweets"</span>)</span><br><span class="line">counts.plot(kind = <span class="string">'bar'</span>)</span><br><span class="line">print(counts)</span><br></pre></td></tr></table></figure><p><img src="https://user-images.githubusercontent.com/25416425/59206240-7c3f4580-8bdf-11e9-93b4-8803db1e7084.png" width="450"></p><p>역시나 예상했던 대로, 영화 개봉일인 4월 24일이 되자 트윗이 14,989개로 폭발적으로 증가했음을 확인할 수 있다.</p><p>여기까지 간단하게 데이터의 shape 정도를 확인해 보았다. 이외에도 다양한 방법으로 데이터를 분석할 수 있으니 그건 각자 해보는 걸로..</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;어떤 사건에 대한 사람들의 즉각적인 관심과 반응, 그리고 영향력자들이 어떻게 일반 사용자에게 영향을 미치는지 분석하는 데는 트위터 만한 SNS가 없다.&lt;/p&gt;
&lt;p&gt;본 포스트에서는 예제로 19년 4월 4주차에서 &lt;u&gt;어벤져스&lt;/u&gt; 또는 &lt;u&gt;스포&lt;/u&gt;가 포함된 트윗을 수집해 볼 것이다.&lt;/p&gt;
&lt;p&gt;어벤져스 엔드게임은 19년 4월 24일에 개봉해서 엄청난 인기를 끌었는데, 한동안 SNS에서 일명 스포 (스포일러)를 한다고 절대 아무거나 클릭하지 말라고 소동이 있었던 것으로 기억한다.&lt;/p&gt;
&lt;p&gt;개봉일 전후로 트윗 수가 차이가 나고, &lt;u&gt;스포&lt;/u&gt;와 같이 작성된 트윗이 많을 것으로 기대된다. &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25416425/59194117-ebf30780-8bc2-11e9-91f1-ab6b84883cd6.png&quot; width=&quot;400&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Data Crawling" scheme="https://jeongwookie.github.io/categories/Programming/Data-Crawling/"/>
    
    
      <category term="Python" scheme="https://jeongwookie.github.io/tags/Python/"/>
    
      <category term="Datamining" scheme="https://jeongwookie.github.io/tags/Datamining/"/>
    
      <category term="Twitter" scheme="https://jeongwookie.github.io/tags/Twitter/"/>
    
  </entry>
  
  <entry>
    <title>Data Crawling - 네이버 뉴스 데이터 수집하기</title>
    <link href="https://jeongwookie.github.io/2019/05/31/190531-naver-main-news-crawling/"/>
    <id>https://jeongwookie.github.io/2019/05/31/190531-naver-main-news-crawling/</id>
    <published>2019-05-31T07:26:09.000Z</published>
    <updated>2019-06-03T08:35:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>네이버 뉴스와 관련된 데이터로 연구실에서 일하다 보니, 여러가지 관점에서 데이터를 수집하는 경우가 생긴다.</p><p>네이버 뉴스에서 오른쪽 위쪽을 잘 살펴보면 <a href="https://news.naver.com/main/history/mainnews/index.nhn" rel="external nofollow noopener noreferrer" target="_blank">기사배열 이력</a> 이라는 코너가 있다.</p><p>2019년 4월 4일 이후부터는 메인에 뜨는 뉴스가 개인마다 다르게 적용되도록 서비스 하고 있는 것 같은데, 그 전에는 네이버가 자신들의 기준으로 메인에 기사를 걸어놓은 것 같다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58689423-f4c52b80-83c1-11e9-9561-a1e8bd4f2fec.png" width="550"></p><a id="more"></a><p>원하는 기간에 <u>네이버 뉴스 메인에 게시되었던 기사들</u>만 골라서 제목 및 링크를 수집하여 보자.</p><p>차근차근 따라올 수 있도록 코드를 최대한 구성해 보도록 노력했다.</p><p>코드는 <code>python 3.7</code> 환경에서 작성했다.</p><h2 id="수집-기간-정의하기"><a href="#수집-기간-정의하기" class="headerlink" title="수집 기간 정의하기"></a>수집 기간 정의하기</h2><p>먼저, 원하는 수집 기간을 정의해 보자. 파이썬의 기본 패키지 중 하나인 <code>datetime</code>을 사용할 것이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 가져올 범위를 정의</span></span><br><span class="line"><span class="comment"># 2015-02-25 ~ 2015-02-28 // 2015-03-01 ~ 2015-03-30</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">days_range = []</span><br><span class="line"></span><br><span class="line">start = datetime.datetime.strptime(<span class="string">"2015-02-25"</span>, <span class="string">"%Y-%m-%d"</span>)</span><br><span class="line">end = datetime.datetime.strptime(<span class="string">"2015-03-31"</span>, <span class="string">"%Y-%m-%d"</span>) <span class="comment"># 범위 + 1</span></span><br><span class="line">date_generated = [start + datetime.timedelta(days=x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>, (end-start).days)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> date <span class="keyword">in</span> date_generated:</span><br><span class="line">    days_range.append(date.strftime(<span class="string">"%Y-%m-%d"</span>))</span><br><span class="line"></span><br><span class="line">print(days_range)</span><br><span class="line"></span><br><span class="line">&gt;&gt; [<span class="string">'2015-02-25'</span>, <span class="string">'2015-02-26'</span>, <span class="string">'2015-02-27'</span>, <span class="string">'2015-02-28'</span>, <span class="string">'2015-03-01'</span>, ... , <span class="string">'2015-03-30'</span>]</span><br></pre></td></tr></table></figure><p>코드를 읽어보면 별로 어렵지 않다. <code>start</code> 와 <code>end</code>로 날짜 범위를 지정한 다음, <code>striptime</code> 함수를 활용하여 원하는 형식인 <u>%Y-%m-%d</u> 으로 뽑아내어 <code>days_range</code> 라는 리스트에 저장하였다.</p><h2 id="html-parser-정의하기"><a href="#html-parser-정의하기" class="headerlink" title="html parser 정의하기"></a>html parser 정의하기</h2><p>이제 원하는 페이지의 html에 접근하기 위해, parser을 정의할 것이다.</p><p>다행히도 간편하게 html parsing을 지원하는 패키지인 <code>BeautifulSoup4</code>가 존재한다.</p><p>혹시나 이 패키지를 다운받지 않으신 분들은 주피터 노트북 상에서 아래와 같은 코드를 입력하면 된다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install bs4</span><br></pre></td></tr></table></figure><p>다운로드가 완료되었다면, 이제 원래의 목적으로 돌아가보자. 아래의 코드를 작성한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bs_obj</span><span class="params">(url)</span>:</span></span><br><span class="line">    result = requests.get(url)</span><br><span class="line">    bs_obj = BeautifulSoup(result.content, <span class="string">"html.parser"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bs_obj</span><br></pre></td></tr></table></figure><p><code>get_bs_obj()</code> 라는, url을 입력하면 bs_obj을 반환하는 함수를 작성하였다.</p><h2 id="뉴스-페이지-수-구하기"><a href="#뉴스-페이지-수-구하기" class="headerlink" title="뉴스 페이지 수 구하기"></a>뉴스 페이지 수 구하기</h2><p>이제 우리가 수집할 페이지에 접속하여, 페이지가 동작하는 방식에 대해서 파악해 보자.</p><p><a href="https://news.naver.com/main/history/mainnews/list.nhn" rel="external nofollow noopener noreferrer" target="_blank">네이버 주요뉴스 배열 이력 (리스트형)</a></p><p><img src="https://user-images.githubusercontent.com/25416425/58690689-1f64b380-83c5-11e9-8630-04cd97bc2be7.png" width="550"></p><p>우리가 원하는 정보는 위의 페이지에서 우리 눈으로 보이는 정보들 (기사 제목, 링크, 날짜 등) 이다. </p><p>빨간색 네모 부분을 누르면 다음 페이지로 넘어가며, 사진과 같이 메인 뉴스로 게시되었던 기사 및 텍스트만 메인 뉴스로 게시된 기사 모두를 수집해야 한다.</p><p>날짜가 바뀔때 마다 메인 뉴스에 걸렸던 기사의 갯수가 달라질 것인데, 그러면 당연하게도 페이지 수를 알아야 한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm_notebook</span><br><span class="line"></span><br><span class="line">test = [<span class="string">"2015-03-01"</span>] <span class="comment"># 테스트를 위한 데이터 수집 구간</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> date <span class="keyword">in</span> tqdm_notebook(test):</span><br><span class="line">    </span><br><span class="line">    news_arrange_url = <span class="string">"https://news.naver.com/main/history/mainnews/list.nhn"</span></span><br><span class="line">    news_list_date_page_url = news_arrange_url + <span class="string">"?date="</span> + date</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get bs_obj</span></span><br><span class="line">    bs_obj = get_bs_obj(news_list_date_page_url)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 포토 뉴스 페이지 수 구하기</span></span><br><span class="line">    photo_news_count = bs_obj.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"eh_page"</span>&#125;).text.split(<span class="string">'/'</span>)[<span class="number">1</span>]</span><br><span class="line">    photo_news_count = int(photo_news_count)</span><br><span class="line">    </span><br><span class="line">    print(photo_news_count)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 리스트 뉴스 페이지 수 구하기</span></span><br><span class="line">    text_news_count = bs_obj.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"mtype_list_wide"</span>&#125;).find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"eh_page"</span>&#125;).text.split(<span class="string">'/'</span>)[<span class="number">1</span>]</span><br><span class="line">    text_news_count = int(text_news_count)</span><br><span class="line">    </span><br><span class="line">    print(text_news_count)</span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="number">15</span></span><br><span class="line">&gt;&gt; <span class="number">7</span></span><br></pre></td></tr></table></figure><p>위 코드에 사용된 <code>tqdm</code>은 for문의 진행 정도를 시각적으로 보여주는 좋은 패키지이다. 사용법은 위와 같이 import한 후, for문의 범위에 감싸주면 된다.</p><p>웹페이지의 html에서 값을 가져오는 데에는 <code>bs4</code>의 <code>find()</code> 함수를 사용하였다.</p><p>사용법이 궁금하다면, 예전 포스트들을 참고하자.</p><p>전체적인 흐름은, 페이지 수 주변의 html 코드를 가져온 후,  Python의 기본 함수인 <code>split()</code> 으로 쪼개어, 원하는 부분만 출력한 것이다.</p><h2 id="뉴스-정보-수집하기"><a href="#뉴스-정보-수집하기" class="headerlink" title="뉴스 정보 수집하기"></a>뉴스 정보 수집하기</h2><p>이제 진짜 우리가 원하는 정보를 수집할 준비가 완료되었다.</p><p>예시로 각 페이지에서 기사 제목, 발행 언론사, 카테고리, 기사 링크, 기사 고유번호 (10자리) 를 수집할 것이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm_notebook</span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line">test = [<span class="string">"2015-03-01"</span>]</span><br><span class="line">main_news_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> date <span class="keyword">in</span> tqdm_notebook(test):</span><br><span class="line">    </span><br><span class="line">    news_arrange_url = <span class="string">"https://news.naver.com/main/history/mainnews/list.nhn"</span></span><br><span class="line">    news_list_date_page_url = news_arrange_url + <span class="string">"?date="</span> + date</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get bs_obj</span></span><br><span class="line">    bs_obj = get_bs_obj(news_list_date_page_url)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 포토 뉴스 페이지 수 구하기</span></span><br><span class="line">    photo_news_count = bs_obj.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"eh_page"</span>&#125;).text.split(<span class="string">'/'</span>)[<span class="number">1</span>]</span><br><span class="line">    photo_news_count = int(photo_news_count)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 리스트 뉴스 페이지 수 구하기</span></span><br><span class="line">    text_news_count = bs_obj.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"mtype_list_wide"</span>&#125;).find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"eh_page"</span>&#125;).text.split(<span class="string">'/'</span>)[<span class="number">1</span>]</span><br><span class="line">    text_news_count = int(text_news_count)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 포토 뉴스 부분 링크 크롤링</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> tqdm_notebook(range(<span class="number">1</span>,photo_news_count+<span class="number">1</span>)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 포토 뉴스 링크</span></span><br><span class="line">        news_list_photo_url = <span class="string">'http://news.naver.com/main/history/mainnews/photoTv.nhn'</span></span><br><span class="line">        date_str = <span class="string">"?date="</span></span><br><span class="line">        page_str = <span class="string">"&amp;page="</span></span><br><span class="line">        news_list_photo_full_url = news_list_photo_url + <span class="string">"?date="</span> + date + <span class="string">"&amp;page="</span> + str(page)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get bs obj</span></span><br><span class="line">        photo_bs_obj = get_bs_obj(news_list_photo_full_url)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 링크 내 정보 수집</span></span><br><span class="line">        ul = photo_bs_obj.find(<span class="string">"ul"</span>, &#123;<span class="string">"class"</span>: <span class="string">"edit_history_lst"</span>&#125;)</span><br><span class="line">        lis = ul.find_all(<span class="string">"li"</span>)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> lis:</span><br><span class="line">            title = item.find(<span class="string">"a"</span>)[<span class="string">"title"</span>]</span><br><span class="line">            press = item.find(<span class="string">"span"</span>, &#123;<span class="string">"class"</span> : <span class="string">"eh_by"</span>&#125;).text</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># link</span></span><br><span class="line">            link = item.find(<span class="string">"a"</span>)[<span class="string">"href"</span>]</span><br><span class="line">            </span><br><span class="line">            sid1 = link.split(<span class="string">'&amp;'</span>)[<span class="number">-3</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">            oid = link.split(<span class="string">'&amp;'</span>)[<span class="number">-2</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">            aid = link.split(<span class="string">'&amp;'</span>)[<span class="number">-1</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 연예 TV 기사 제외</span></span><br><span class="line">            <span class="keyword">if</span> sid1 == <span class="string">"shm"</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">                </span><br><span class="line">            article_type = <span class="string">"pic"</span></span><br><span class="line">            </span><br><span class="line">            pic_list = [date, article_type, title, press, sid1, link, aid]</span><br><span class="line">            </span><br><span class="line">            main_news_list.append(pic_list)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 텍스트 뉴스 부분 링크 크롤링</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> tqdm_notebook(range(<span class="number">1</span>, text_news_count+<span class="number">1</span>)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 텍스트 뉴스 링크</span></span><br><span class="line">        news_list_text_url = <span class="string">'http://news.naver.com/main/history/mainnews/text.nhn'</span></span><br><span class="line">        date_str = <span class="string">"?date="</span></span><br><span class="line">        page_str = <span class="string">"&amp;page="</span></span><br><span class="line">        news_list_text_full_url = news_list_text_url + <span class="string">"?date="</span> + date + <span class="string">"&amp;page="</span> + str(page)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get bs obj</span></span><br><span class="line">        text_bs_obj = get_bs_obj(news_list_text_full_url)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 링크 내 정보 수집</span></span><br><span class="line">        uls = text_bs_obj.find_all(<span class="string">"ul"</span>)</span><br><span class="line">        <span class="keyword">for</span> ul <span class="keyword">in</span> uls:</span><br><span class="line">            lis = ul.find_all(<span class="string">"li"</span>)</span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> lis:</span><br><span class="line">                title = item.find(<span class="string">"a"</span>).text</span><br><span class="line">                press = item.find(<span class="string">"span"</span>, &#123;<span class="string">"class"</span> : <span class="string">"writing"</span>&#125;).text</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># link</span></span><br><span class="line">                link = item.find(<span class="string">"a"</span>)[<span class="string">"href"</span>]</span><br><span class="line"></span><br><span class="line">                sid1 = link.split(<span class="string">'&amp;'</span>)[<span class="number">-3</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">                oid = link.split(<span class="string">'&amp;'</span>)[<span class="number">-2</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">                aid = link.split(<span class="string">'&amp;'</span>)[<span class="number">-1</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 연예 TV 기사 제외</span></span><br><span class="line">                <span class="keyword">if</span> sid1 == <span class="string">"shm"</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                article_type = <span class="string">"text"</span></span><br><span class="line">                </span><br><span class="line">                text_list = [date, article_type, title, press, sid1, link, aid]</span><br><span class="line">                </span><br><span class="line">                main_news_list.append(text_list)</span><br><span class="line"></span><br><span class="line">pprint(main_news_list, width = <span class="number">20</span>)</span><br></pre></td></tr></table></figure><p>코드가 크게 두 부분으로 나뉘어져 있다.</p><p>하나는 포토 기사에 대한 데이터 수집. 앞선 step에서 <code>photo_news_count</code> 에 페이지 수를 저장했던 것을 기억할 것이다.</p><p>for문으로 페이지 수만큼 반복하여 여러가지 정보를 수집하도록 코드를 짰다.</p><p>그리고, 수집하다보면 네이버 기사 메인에 게시되긴 하였지만 <a href="https://entertain.naver.com/read?oid=416&amp;aid=0000140695" rel="external nofollow noopener noreferrer" target="_blank">뉴스가 아닌(?) 링크들</a>이 있어서 제외하였다.</p><p>두 번째는 <code>text_news_count</code> 으로 페이지 수를 저장하였던 텍스트 기사에 대한 데이터 수집이다.</p><p>또한, 수집된 기사가 포토 기사인지 텍스트 기사인지 알고 싶어서 중간에 <code>article_type</code>을 추가해 놓았다.</p><p>모든 수집된 데이터는 <code>main_news_list</code> 라는 이름의 리스트에 차례대로 append 되도록 설정했다.</p><p>마지막 줄의 <code>pprint()</code>는 단지 리스트를 깔끔하게 출력하고 싶어서 추가한 코드이니, 크롤러랑은 무관하다.</p><blockquote><p>test 기간에 대해 네이버 메인 뉴스 기사배열이력 수집 결과</p></blockquote><p><img src="https://user-images.githubusercontent.com/25416425/58693771-7326cb00-83cc-11e9-943a-996c599aa672.png" width="550"></p><h2 id="CSV-파일로-저장하기"><a href="#CSV-파일로-저장하기" class="headerlink" title="CSV 파일로 저장하기"></a>CSV 파일로 저장하기</h2><p>수집을 했으면 관리하기 쉽도록 적절한 형태로 저장하는 것이 필수!</p><p>가장 보편적인 형태인 .csv 파일로 저장해 보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># make .csv file</span></span><br><span class="line">naver_news_df = pd.DataFrame(main_news_list, </span><br><span class="line">                                  columns = [<span class="string">"date"</span>, <span class="string">"type"</span>, <span class="string">"title"</span>, <span class="string">"press"</span>, <span class="string">"category"</span>, <span class="string">"link"</span>, <span class="string">"aid"</span>])</span><br><span class="line">naver_news_df.to_csv(<span class="string">"naver_main_news.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>실행하면, working directory 내에 <code>naver_main_news.csv</code> 파일이 생성되어 있음을 확인할 수 있다.</p><p>csv file을 만들기 위해, 데이터를 다루는 데에 매우 편리한 패키지인 <code>pandas</code>를 사용하였다.</p><p>참고로, 주피터 노트북에서 작업하는 사용자의 경우 위에 <code>bs4</code> 패키지를 설치할 때와 유사하게 코드를 입력하면 쉽게 패키지를 다운로드 받을 수 있다.</p><p>제대로 수집되었는지 csv file을 열어서 확인해 보자. 아래와 같은 코드를 입력한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># open .csv file</span></span><br><span class="line"></span><br><span class="line">df_naver_news = pd.read_csv(<span class="string">'naver_main_news.csv'</span>, dtype = &#123;<span class="string">"aid"</span> : <span class="string">"str"</span>&#125;)</span><br><span class="line">df_naver_news.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>위 코드에서 <code>read_csv()</code>으로 파일을 읽을 때 <code>dtype</code> 을 추가한 이유는, 저장했었던 <code>aid</code>가 뉴스 고유 번호 10자리인데 경우에 따라 0000003455와 같이 앞에 0이 있어서, 이를 그대로 출력하고 싶어서 이다.</p><p>만약 저 옵션을 추가하지 않으면 방금 전 예로 든 뉴스 고유 번호는 10자리가 아니라 0을 다 빼버린 3455 라고 출력된다.. (ㅠㅠ)</p><p><img src="https://user-images.githubusercontent.com/25416425/58694773-c9950900-83ce-11e9-814e-4acb2aeaefb4.png" width="550"></p><h2 id="전체-코드"><a href="#전체-코드" class="headerlink" title="전체 코드"></a>전체 코드</h2><p>수집을 원하는 기간을 가장 위의 <code>start</code>, <code>end</code>에 입력하면 그 결과가 csv file 로 저장되는 코드이다.</p><p>본 코드는 정적 데이터를 수집하기 위해 작성되었으며, 동적 데이터가 존재하는 경우 다른 패키지를 사용해야 한다. (참고!) </p><p>그리 깔끔한 코드는 아니니까 공부용으로 사용하시길..</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 가져올 범위를 정의</span></span><br><span class="line"><span class="comment"># 2015-02-25 ~ 2015-02-28 // 2015-03-01 ~ 2015-03-31</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">days_range = []</span><br><span class="line"></span><br><span class="line">start = datetime.datetime.strptime(<span class="string">"2015-02-25"</span>, <span class="string">"%Y-%m-%d"</span>) <span class="comment"># 수집 시작 날짜</span></span><br><span class="line">end = datetime.datetime.strptime(<span class="string">"2015-03-31"</span>, <span class="string">"%Y-%m-%d"</span>) <span class="comment"># 수집 종료 날짜 + 1</span></span><br><span class="line">date_generated = [start + datetime.timedelta(days=x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>, (end-start).days)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> date <span class="keyword">in</span> date_generated:</span><br><span class="line">    days_range.append(date.strftime(<span class="string">"%Y-%m-%d"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 크롤러 작성</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm_notebook</span><br><span class="line"></span><br><span class="line">main_news_list = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># html parser 정의</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bs_obj</span><span class="params">(url)</span>:</span></span><br><span class="line">    result = requests.get(url)</span><br><span class="line">    bs_obj = BeautifulSoup(result.content, <span class="string">"html.parser"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bs_obj</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> date <span class="keyword">in</span> tqdm_notebook(days_range):</span><br><span class="line">    </span><br><span class="line">    news_arrange_url = <span class="string">"https://news.naver.com/main/history/mainnews/list.nhn"</span></span><br><span class="line">    news_list_date_page_url = news_arrange_url + <span class="string">"?date="</span> + date</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get bs_obj</span></span><br><span class="line">    bs_obj = get_bs_obj(news_list_date_page_url)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 포토 뉴스 페이지 수 구하기</span></span><br><span class="line">    photo_news_count = bs_obj.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"eh_page"</span>&#125;).text.split(<span class="string">'/'</span>)[<span class="number">1</span>]</span><br><span class="line">    photo_news_count = int(photo_news_count)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 리스트 뉴스 페이지 수 구하기</span></span><br><span class="line">    text_news_count = bs_obj.find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"mtype_list_wide"</span>&#125;).find(<span class="string">"div"</span>, &#123;<span class="string">"class"</span>: <span class="string">"eh_page"</span>&#125;).text.split(<span class="string">'/'</span>)[<span class="number">1</span>]</span><br><span class="line">    text_news_count = int(text_news_count)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 포토 뉴스 부분 링크 크롤링</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> tqdm_notebook(range(<span class="number">1</span>,photo_news_count+<span class="number">1</span>)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 포토 뉴스 링크</span></span><br><span class="line">        news_list_photo_url = <span class="string">'http://news.naver.com/main/history/mainnews/photoTv.nhn'</span></span><br><span class="line">        date_str = <span class="string">"?date="</span></span><br><span class="line">        page_str = <span class="string">"&amp;page="</span></span><br><span class="line">        news_list_photo_full_url = news_list_photo_url + <span class="string">"?date="</span> + date + <span class="string">"&amp;page="</span> + str(page)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get bs obj</span></span><br><span class="line">        photo_bs_obj = get_bs_obj(news_list_photo_full_url)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 링크 내 정보 수집</span></span><br><span class="line">        ul = photo_bs_obj.find(<span class="string">"ul"</span>, &#123;<span class="string">"class"</span>: <span class="string">"edit_history_lst"</span>&#125;)</span><br><span class="line">        lis = ul.find_all(<span class="string">"li"</span>)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> lis:</span><br><span class="line">            title = item.find(<span class="string">"a"</span>)[<span class="string">"title"</span>]</span><br><span class="line">            press = item.find(<span class="string">"span"</span>, &#123;<span class="string">"class"</span> : <span class="string">"eh_by"</span>&#125;).text</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># link</span></span><br><span class="line">            link = item.find(<span class="string">"a"</span>)[<span class="string">"href"</span>]</span><br><span class="line">            </span><br><span class="line">            sid1 = link.split(<span class="string">'&amp;'</span>)[<span class="number">-3</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">            oid = link.split(<span class="string">'&amp;'</span>)[<span class="number">-2</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">            aid = link.split(<span class="string">'&amp;'</span>)[<span class="number">-1</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 연예 TV 기사 제외</span></span><br><span class="line">            <span class="keyword">if</span> sid1 == <span class="string">"shm"</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">                </span><br><span class="line">            article_type = <span class="string">"pic"</span></span><br><span class="line">            </span><br><span class="line">            pic_list = [date, article_type, title, press, sid1, link, aid]</span><br><span class="line">            </span><br><span class="line">            main_news_list.append(pic_list)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 텍스트 뉴스 부분 링크 크롤링</span></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> tqdm_notebook(range(<span class="number">1</span>, text_news_count+<span class="number">1</span>)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 텍스트 뉴스 링크</span></span><br><span class="line">        news_list_text_url = <span class="string">'http://news.naver.com/main/history/mainnews/text.nhn'</span></span><br><span class="line">        date_str = <span class="string">"?date="</span></span><br><span class="line">        page_str = <span class="string">"&amp;page="</span></span><br><span class="line">        news_list_text_full_url = news_list_text_url + <span class="string">"?date="</span> + date + <span class="string">"&amp;page="</span> + str(page)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># get bs obj</span></span><br><span class="line">        text_bs_obj = get_bs_obj(news_list_text_full_url)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 링크 내 정보 수집</span></span><br><span class="line">        uls = text_bs_obj.find_all(<span class="string">"ul"</span>)</span><br><span class="line">        <span class="keyword">for</span> ul <span class="keyword">in</span> uls:</span><br><span class="line">            lis = ul.find_all(<span class="string">"li"</span>)</span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> lis:</span><br><span class="line">                title = item.find(<span class="string">"a"</span>).text</span><br><span class="line">                press = item.find(<span class="string">"span"</span>, &#123;<span class="string">"class"</span> : <span class="string">"writing"</span>&#125;).text</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># link</span></span><br><span class="line">                link = item.find(<span class="string">"a"</span>)[<span class="string">"href"</span>]</span><br><span class="line"></span><br><span class="line">                sid1 = link.split(<span class="string">'&amp;'</span>)[<span class="number">-3</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">                oid = link.split(<span class="string">'&amp;'</span>)[<span class="number">-2</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">                aid = link.split(<span class="string">'&amp;'</span>)[<span class="number">-1</span>].split(<span class="string">'='</span>)[<span class="number">1</span>]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 연예 TV 기사 제외</span></span><br><span class="line">                <span class="keyword">if</span> sid1 == <span class="string">"shm"</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                article_type = <span class="string">"text"</span></span><br><span class="line">                </span><br><span class="line">                text_list = [date, article_type, title, press, sid1, link, aid]</span><br><span class="line">                </span><br><span class="line">                main_news_list.append(text_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># make .csv file</span></span><br><span class="line">naver_news_main_df = pd.DataFrame(main_news_list, </span><br><span class="line">                                  columns = [<span class="string">"date"</span>, <span class="string">"type"</span>, <span class="string">"title"</span>, <span class="string">"press"</span>, <span class="string">"category"</span>, <span class="string">"link"</span>, <span class="string">"aid"</span>])</span><br><span class="line">naver_news_main_df.to_csv(<span class="string">"naver_main_news_&#123;&#125;_to_&#123;&#125;.csv"</span>.format(days_range[<span class="number">0</span>], days_range[<span class="number">-1</span>]), index=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"=== total # of articles is &#123;&#125; ==="</span>.format(len(main_news_list)))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;네이버 뉴스와 관련된 데이터로 연구실에서 일하다 보니, 여러가지 관점에서 데이터를 수집하는 경우가 생긴다.&lt;/p&gt;
&lt;p&gt;네이버 뉴스에서 오른쪽 위쪽을 잘 살펴보면 &lt;a href=&quot;https://news.naver.com/main/history/mainnews/index.nhn&quot; rel=&quot;external nofollow noopener noreferrer&quot; target=&quot;_blank&quot;&gt;기사배열 이력&lt;/a&gt; 이라는 코너가 있다.&lt;/p&gt;
&lt;p&gt;2019년 4월 4일 이후부터는 메인에 뜨는 뉴스가 개인마다 다르게 적용되도록 서비스 하고 있는 것 같은데, 그 전에는 네이버가 자신들의 기준으로 메인에 기사를 걸어놓은 것 같다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25416425/58689423-f4c52b80-83c1-11e9-9561-a1e8bd4f2fec.png&quot; width=&quot;550&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Data Crawling" scheme="https://jeongwookie.github.io/categories/Programming/Data-Crawling/"/>
    
    
      <category term="Python" scheme="https://jeongwookie.github.io/tags/Python/"/>
    
      <category term="WEB Crawling" scheme="https://jeongwookie.github.io/tags/WEB-Crawling/"/>
    
      <category term="Datamining" scheme="https://jeongwookie.github.io/tags/Datamining/"/>
    
  </entry>
  
  <entry>
    <title>Server Setting - 고정 ip 없이 주피터(jupyter) 원격 접속 설정하기</title>
    <link href="https://jeongwookie.github.io/2019/05/23/190523-ubuntu-jupyter-notebook-remote/"/>
    <id>https://jeongwookie.github.io/2019/05/23/190523-ubuntu-jupyter-notebook-remote/</id>
    <published>2019-05-23T03:36:42.000Z</published>
    <updated>2019-05-23T10:07:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>오늘은 머신러닝과 데이터 사이언스 분야에서 거의 필수적으로 사용하고 있는 주피터 (Jupyter notebook)를 원격으로 접속할 수 있도록 세팅해 보겠다.</p><p>이 과정은 동일한 ip를 사용중인 wifi 환경에서 벗어난 (예를 들어 카페나 학교 등) 곳에서도 집에 있는 서버를 접속하여 편안한 코딩을 할 수 있도록 하기 위함이다.</p><p>셋팅은 Ubuntu 18.04 기준으로 진행하였다.</p><a id="more"></a><h2 id="공유기-설정하기"><a href="#공유기-설정하기" class="headerlink" title="공유기 설정하기"></a>공유기 설정하기</h2><p>먼저, 주피터 노트북을 원격으로 접속하려면 서버가 있는 환경의 고정 ip가 필요하다.</p><p>마치 택배를 배송받을 집 주소가 있어야 하는 것인데, 일반적으로 학교나 연구실, 회사가 아닌 이상 고정 ip가 아닌 유동 ip를 사용한다.</p><p>쉽게 말하면, 우리집 주소가 계속 바뀐다는 것. 이러면 제대로 찾아갈 수가 없다..</p><p>그래서 먼저 <strong>DDNS</strong> 라는 기능을 제공하는 공유기를 준비하여 우리의 유동 ip와 특정 문자열을 연결시킬 것이다.</p><p>택배 회사와 계약해서 ‘정욱이네’ 로 물건을 배송해 주세요~ 하면 우리집으로 배송되는 것과 비슷하다.</p><p>하지만 그 전에 집에 있는 공유기가 DDNS 기능을 제공하는지 검색해서 확인해보자.</p><p>아니라면 새로 하나 장만하기를 추천한다. 세팅이 가장 쉬운 것은 iptime 공유기이다.</p><p>다나와에서 아래와 같이 검색하면 쉽게 제품을 확인할 수 있다. 나는 마침 굴러다니는 12,900원짜리 iptime N702R 공유기가 있어 이것을 사용했다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58224546-2baf9780-7d59-11e9-86d0-cdd5f5e9d578.png" width="600"></p><h3 id="DDNS-설정-진입"><a href="#DDNS-설정-진입" class="headerlink" title="DDNS 설정 진입"></a>DDNS 설정 진입</h3><p>iptime 공유기 설정을 위해 192.168.0.1로 접속한다. 기본 아이디 비밀번호는 admin으로 동일하다. 혹시 비밀번호를 설정했는데 잊어버렸다면 공유기 reset 버튼을 꾹 눌러서 초기화하자.</p><p>접속해서 관리 설정을 누르면 아래와 같은 화면이 보일 것이다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58224807-17b86580-7d5a-11e9-9d38-2fdd1d88e13d.png" width="600"></p><p>여기서 고급 설정 -&gt; 특수 기능 -&gt; <strong>DDNS 설정</strong> 으로 들어간다.</p><p>호스트 이름은 원하는 것으로 자유롭게 입력한다. 이 주소로 서버에 접속할 것이다.</p><p>사용자 아이디는 자주 사용하는 메일을 입력하고 (예시 : <a href="mailto:OOO@gmail.com" rel="external nofollow noopener noreferrer" target="_blank">OOO@gmail.com</a>)</p><p>오른쪽에 보이는 보안문자를 입력 후 DDNS 등록을 누르면 끝!</p><p><img src="https://user-images.githubusercontent.com/25416425/58224946-b8a72080-7d5a-11e9-8bd3-97751756093d.png" width="600"></p><p>아래에 등록 IP 주소가 나오는데, 이것으로 외부에서 접속이 가능하게 한다. 방금 전 입력하였던 호스트 이름으로도 접속이 가능하다.</p><h3 id="포트포워딩-설정"><a href="#포트포워딩-설정" class="headerlink" title="포트포워딩 설정"></a>포트포워딩 설정</h3><p>이번에는 포트포워딩 설정을 해보자. 특정 포트를 열고, 아까 전 설정한 DDNS 으로 등록된 ip 주소와 함께 입력하면 그것이 완전한 우리 서버의 주소이다. 등록된 ip는 전체 주소, 특정 포트 번호는 상세 주소라고 생각하면 된다.</p><p>고급 설정 -&gt; NAT/라우터 관리 -&gt; <strong>포트포워드 설정</strong> 으로 들어간다.</p><p>적당한 규칙 이름을 지정하고, 내부 ip 주소는 현재 세팅하고 있는 서버의 주소를 입력하면 된다. 현재 접속된 IP 주소를 체크하여도 괜찮다.</p><p>프로토콜은 TCP로 하고, 외부 포트 및 내부 포트 번호를 모두 3389로 입력한다. (기본)</p><p><img src="https://user-images.githubusercontent.com/25416425/58225218-fce6f080-7d5b-11e9-913e-93ddf12bc6ed.png" width="600"></p><p>여기까지 진행한 내용을 간단히 정리해 보면,</p><p>외부에서 서버에 접속할 때 DDNS 설정 시 입력하였던 적당한 호스트 이름 주소 (OOO.iptime.org) 및 포트번호 (3389)를 입력하면, 포트포워딩을 설정한 내부 ip + 해당 포트(3389) 로 접속되는 것이다.</p><h3 id="공유기-설정-접속-관리"><a href="#공유기-설정-접속-관리" class="headerlink" title="공유기 설정 접속 관리"></a>공유기 설정 접속 관리</h3><p>마지막으로, 외부에서도 서버에 연결된 공유기의 설정을 변경하고 싶을때를 대비하여 세팅을 해보자.</p><p>고급 설정 -&gt; 보안 기능 -&gt; <strong>공유기 접속/보안관리</strong> 로 들어간다.</p><p>외부 접속 보안에서 <strong>[원격 관리 포트 사용]</strong> 을 체크하고, 적당한 포트 번호를 입력한다. (예시: 1234)</p><p>적용 버튼을 누르면 끝! 이제 외부에서 공유기를 설정하고 싶을 때는 <strong>OOO.iptime.org:1234</strong> 로 접속하면 된다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58225493-39671c00-7d5d-11e9-8b2c-35e3264e9df8.png" width="600"></p><p>그리고 공유기를 혹시 초기화 하였다면 공유기 자체의 무선 비밀번호 또한 초기화 되어 있을 것이므로, 기본 설정에서 각자 세팅해 주자.</p><h2 id="우분투에서-포트-열기"><a href="#우분투에서-포트-열기" class="headerlink" title="우분투에서 포트 열기"></a>우분투에서 포트 열기</h2><p>전반적인 공유기 설정은 완료되었다. 이제 주피터를 설정해야 하는데, 그 전에 아까 전 입력했던 포트를 우분투에서 열어놓자.</p><p>터미널을 열고, 아래와 같은 코드를 순차적으로 입력한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3389 포트 오픈</span></span><br><span class="line">sudo iptables -I INPUT <span class="number">1</span> -p tcp --dport <span class="number">3389</span> -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment"># 포트가 열렸는지 확인</span></span><br><span class="line">sudo iptables -L -v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3389 포트의 방화벽 해제</span></span><br><span class="line">sudo ufw allow <span class="number">3389</span></span><br></pre></td></tr></table></figure><h2 id="주피터-노트북-설정하기"><a href="#주피터-노트북-설정하기" class="headerlink" title="주피터 노트북 설정하기"></a>주피터 노트북 설정하기</h2><h3 id="비밀번호-설정"><a href="#비밀번호-설정" class="headerlink" title="비밀번호 설정"></a>비밀번호 설정</h3><p>먼저 주피터 노트북의 설정 파일을 만들고, 이를 우리의 세팅으로 수정할 것이다.</p><p>우분투 터미널을 켜고, 아래와 같은 코드를 순차적으로 입력한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda activate ds_fox <span class="comment"># 나의 가상 환경 activate</span></span><br><span class="line">jupyter notebook –generate-config <span class="comment"># 주피터 설정 파일 만들기</span></span><br><span class="line">ipython</span><br><span class="line"><span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">passwd()</span><br></pre></td></tr></table></figure><p><code>passwd()</code> 을 입력하면 아래에 패스워드를 입력하라고 뜨는데, 이것이 차후 주피터 서버에 접속할 때 요구하는 자신만의 비밀번호이다.</p><p>입력을 완료하면 Output으로 <code>sha1:----</code> 이런 형태의 긴 암호화된 문자열이 출력되는데, <strong>반드시 어딘가에 복사/붙여넣기 를 해놓도록 하자.</strong></p><p><img src="https://user-images.githubusercontent.com/25416425/58225860-09207d00-7d5f-11e9-8b05-e3de12559791.png" width="600"></p><h3 id="주피터-환경-설정"><a href="#주피터-환경-설정" class="headerlink" title="주피터 환경 설정"></a>주피터 환경 설정</h3><p>이제, 아까 만든 주피터 설정 파일을 vi 편집기로 열어서 수정할 것이다. 아래와 같은 코드를 입력한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config file 열기</span></span><br><span class="line"><span class="comment"># jeongwook 자리에는 username 을 입력할 것</span></span><br><span class="line">vi /home/jeongwook/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><p>입력하면 vi 편집기가 열릴 것이다. 처음 다루는 사람이라면 조금 어려울 수 있지만 간단한 규칙 몇가지만 알면 삽질을 피할 수 있을 것이다.</p><ol><li>방향키를 함부로 누르지 말것. 항상 <strong>ctrl 키</strong>를 누른 채로 움직인다.</li><li>명령어를 입력하고 싶으면 <strong>esc 키</strong>를 한번 누른 후 : 을 입력할 것</li><li>코드를 추가하고 싶을 때는 <strong>i (커서 오른쪽 시작)</strong> or <strong>a (커서 왼쪽 시작)</strong> 을 한번 누르고 입력할 것</li><li>코드를 수정하고 싶을 때는 backspace 키가 아닌 <strong>delete 키</strong>를 사용할 것</li></ol><p>이정도면 따로 찾아보지 않아도 대충은 코드 입력/수정이 가능하지 않을까 싶다.</p><p>이제 차례대로 수정해보자. 순서대로 입력하면 된다.</p><p><strong>a. 비밀번호 등록</strong></p><p><code>:/pp.pass/</code> 명령어를 입력하여 <strong>password</strong> 가 있는 위치를 찾는다. 찾으면 주석 아래에 코드를 입력한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = get_config()</span><br><span class="line">c.NotebookApp.password = u‘내비밀번호붙여넣기’ <span class="comment"># sha1:--- 붙여넣기</span></span><br></pre></td></tr></table></figure><p><strong>b. 외부 접속 허용</strong></p><p><code>:/pp.allow/</code> 를 입력 후 대충 위쪽을 보면 <code>#c.NotebookApp.alloworigin = &#39;&#39;</code> 이런 주석이 있다. 바로 아래에다가 코드를 추가한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.allow_origin = ‘*’</span><br></pre></td></tr></table></figure><p><strong>c. 아이피 설정</strong></p><p><code>:/pp.ip/</code> 로 <strong>ip</strong> 가 있는 위치를 찾고, 주석 아래에 추가한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip = <span class="string">'192.168.0.2'</span> <span class="comment"># DDNS 설정 시 입력했던 내부 ip주소</span></span><br></pre></td></tr></table></figure><p><strong>d. 포트 설정</strong></p><p><code>:/pp.port/</code> 로 <strong>port</strong> 가 있는 위치를 찾고, 주석 아래에 열어놓은 포트 번호를 추가한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.port = <span class="number">3389</span></span><br></pre></td></tr></table></figure><p><strong>e. 시작 시 브라우저 실행 여부</strong></p><p><code>:/pp.op/</code> 로 <strong>open</strong> 이 있는 위치를 찾고, 주석 아래에 추가한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.open_browser = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>차례대로 주피터 설정 파일을 수정하고, 명령어 <code>:wq</code> 를 입력하여 저장하고 밖으로 나온다.</p><p>이것으로 주피터 외부 접속 환경설정이 끝났다.</p><p>잘 설정되었는지 확인해 보려면, 서버 컴퓨터에서 <code>jupyter notebook</code>을 입력해서 서버를 켠 후, 다른 외부망을 사용중인 PC에서 <strong>OOO.iptime.org:3389</strong> 를 입력해 보면 된다.</p><p>아래와 같은 창이 뜨면 성공!!</p><p><img src="https://user-images.githubusercontent.com/25416425/58226511-e6439800-7d61-11e9-97aa-e1e3dc6207e2.png" width="600"></p><p><em>세팅하는데 참고한 블로그들</em><br><em><a href="https://light-tree.tistory.com/111" rel="external nofollow noopener noreferrer" target="_blank">https://light-tree.tistory.com/111</a></em><br><em><a href="https://breath91.tistory.com/" rel="external nofollow noopener noreferrer" target="_blank">https://breath91.tistory.com/</a></em><br><em><a href="https://ironmask.net/354" rel="external nofollow noopener noreferrer" target="_blank">https://ironmask.net/354</a></em><br><em><a href="https://linguist79.tistory.com/45" rel="external nofollow noopener noreferrer" target="_blank">https://linguist79.tistory.com/45</a></em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;오늘은 머신러닝과 데이터 사이언스 분야에서 거의 필수적으로 사용하고 있는 주피터 (Jupyter notebook)를 원격으로 접속할 수 있도록 세팅해 보겠다.&lt;/p&gt;
&lt;p&gt;이 과정은 동일한 ip를 사용중인 wifi 환경에서 벗어난 (예를 들어 카페나 학교 등) 곳에서도 집에 있는 서버를 접속하여 편안한 코딩을 할 수 있도록 하기 위함이다.&lt;/p&gt;
&lt;p&gt;셋팅은 Ubuntu 18.04 기준으로 진행하였다.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Server" scheme="https://jeongwookie.github.io/categories/Programming/Server/"/>
    
    
      <category term="Server setting" scheme="https://jeongwookie.github.io/tags/Server-setting/"/>
    
      <category term="Ubuntu" scheme="https://jeongwookie.github.io/tags/Ubuntu/"/>
    
      <category term="Linux" scheme="https://jeongwookie.github.io/tags/Linux/"/>
    
      <category term="Jupyter notebook" scheme="https://jeongwookie.github.io/tags/Jupyter-notebook/"/>
    
  </entry>
  
  <entry>
    <title>Server Setting - 우분투(Ubuntu) 18.04 설치하기</title>
    <link href="https://jeongwookie.github.io/2019/05/21/190521-ubuntu-install-usb/"/>
    <id>https://jeongwookie.github.io/2019/05/21/190521-ubuntu-install-usb/</id>
    <published>2019-05-21T11:55:21.000Z</published>
    <updated>2019-05-21T18:39:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>지금까지 가상머신으로 우분투를 사용하다가 Pre-trained model을 불러오는데 Memory Error가 떠서..ㅠㅠ 이참에 집에 남아있는 컴퓨터로 서버를 세팅하고자 마음먹었다.</p><p>딥러닝에 사용할 서버로, 설치 OS는 <strong>Ubuntu 18.04.2 LTS</strong> 이다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58094898-afa83900-7c0c-11e9-8220-e91b063b03ed.png" width="300"></p><a id="more"></a><h2 id="준비하기"><a href="#준비하기" class="headerlink" title="준비하기"></a>준비하기</h2><p>먼저, 공식 웹사이트에서 설치 파일을 다운받자.</p><p><a href="https://www.ubuntu.com/download/desktop" rel="external nofollow noopener noreferrer" target="_blank">Download Ubuntu 18.04.2 LTS Version</a></p><p>ISO 파일을 다운받았다면, 이제 부팅 USB를 만들어야 한다.</p><p>적어도 2GB 이상의 USB를 준비하자.</p><p>USB 메모리 내부에 있는 모든 데이터가 삭제되므로, 빈 USB가 아니라면 백업을 권장한다.</p><h2 id="부팅-USB-만들기"><a href="#부팅-USB-만들기" class="headerlink" title="부팅 USB 만들기"></a>부팅 USB 만들기</h2><p>이제, 윈도우 환경 기준으로 우분투 부팅 USB를 만들어 볼 것이다.</p><p>아래의 사이트에 방문하여, <strong>Rufus</strong>라는 프로그램을 다운로드 하자. Ubuntu에서 공식으로 지원하는 프로그램이니 안심해도 된다.</p><p><a href="https://rufus.ie/" rel="external nofollow noopener noreferrer" target="_blank">https://rufus.ie/</a></p><p>프로그램을 다운받은 후 USB를 연결한 후 실행한다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58095561-3dd0ef00-7c0e-11e9-8a2b-6f05a7912583.png" width="350"></p><p>장치는 부팅용으로 만들 USB를 클릭하고 (대부분의 경우 기본으로 잡힘) 부트 선택에서 빨간색 네모가 있는 부분을 클릭. 그리고 방금 전 다운로드 받은 ISO 파일을 선택한다.</p><p>그리고 제일 아래 시작을 누르면 되는데, 중간에 두 번의 팝업이 뜬다.</p><p>첫 번째는 추가 파일 다운로드 여부를 묻는 것으로 <strong>[예 (Y)]</strong> 를 클릭하면 되고, 두 번째는 <strong>[ISO 이미지 모드로 쓰기 (권장)]</strong> 을 선택하면 된다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58095997-12023900-7c0f-11e9-9cbf-ffa5eeea69c7.png" width="350"></p><p>ISO 파일 복사가 완료되면, 이제 이것으로 부팅을 하면 된다.</p><h2 id="Ubuntu-설치하기"><a href="#Ubuntu-설치하기" class="headerlink" title="Ubuntu 설치하기"></a>Ubuntu 설치하기</h2><p>부팅 시 바이오스창을 F2, F11 등으로 열어서 Boot option을 해당 USB으로 바꾸어 주는 것은 기본.</p><p>부팅 후 첫 화면은 아래와 같다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58120188-a8e6e980-7c3f-11e9-8c7c-7d9201770593.jpg" width="500"></p><p>여기서 <strong>[install Ubuntu]</strong> 버튼을 클릭해서 설치를 시작한다.</p><p>키보드 레이아웃 및 업데이트 등의 선택 사항들이 차례대로 나오는데 default로 쭉쭉 넘어간다.</p><p>설치 유형을 선택하는 창이 아래와 같이 나오면, 이때부터 조심해야 한다.</p><p>여러 옵션 중에서 <strong>[Something else]</strong> 을 클릭하여 <u>세부적인 파티션 설정</u>을 시작할 것이다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58120367-109d3480-7c40-11e9-97b2-4395a838edb3.jpg" width="500"></p><h2 id="파티션-설정하기"><a href="#파티션-설정하기" class="headerlink" title="파티션 설정하기"></a>파티션 설정하기</h2><p>이 부분이 Ubuntu 설치에서 가장 복잡한 단계이다.</p><p>먼저, 아래의 <strong>[New Partition Table]</strong> 버튼을 클릭한다. 이때, 아래의 그림과 달리 사용자의 HDD나 다른 SSD가 존재할 경우 잘못된 위치에 셋업하지 않도록 주의한다. 새 파티션을 구성할 장치는 포맷되기 때문에..</p><p><img src="https://user-images.githubusercontent.com/25416425/58120679-ad5fd200-7c40-11e9-8b21-9e73776bded4.jpg" width="500"></p><p>이제 생성한 새 파티션 테이블 아래에 2개의 파티션을 생성할 것이다. 왼쪽 아래의 (+) 버튼을 눌러서 새 파티션을 추가한다.</p><ul><li>스왑 (Swap) 파티션 : 시스템에서 물리메모리가 부족할 경우 사용되는 파티션이며, 일반적으로 메모리 용량의 2배 정도로 설정하여 생성한다.</li><li>Ext4 파티션 : 실제 파일들이 저장되게 될 파티션.</li></ul><p>스왑 파티션의 용량 설정에 관하여는 <a href="https://access.redhat.com/ko/solutions/744483" rel="external nofollow noopener noreferrer" target="_blank">RedHat Linux 권장 Swap 크기</a>를 참고하자.</p><p>먼저 <strong>스왑 파티션</strong>을 추가한다. 필자는 8기가 메모리에 16기가만큼 할당하였으며 아래 그림은 1024 MB (1 GB) 공간을 할당한 스크린샷을 가져왔다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58121375-46431d00-7c42-11e9-8184-3702ea40aa67.png" width="500"></p><p>그리고 다시 (+) 버튼을 눌러서 이번에는 <strong>Ext4 파티션</strong>을 추가한다. 나머지 모든 공간을 할당하였으며 마운트 위치는 / 으로 설정한다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58121375-46431d00-7c42-11e9-8184-3702ea40aa67.png" width="500"></p><p>파티션 설정은 완료되었다. 이제 아래의 <strong>[Install Now]</strong> 버튼을 눌러 진행하면 설치 끝!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;지금까지 가상머신으로 우분투를 사용하다가 Pre-trained model을 불러오는데 Memory Error가 떠서..ㅠㅠ 이참에 집에 남아있는 컴퓨터로 서버를 세팅하고자 마음먹었다.&lt;/p&gt;
&lt;p&gt;딥러닝에 사용할 서버로, 설치 OS는 &lt;strong&gt;Ubuntu 18.04.2 LTS&lt;/strong&gt; 이다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25416425/58094898-afa83900-7c0c-11e9-8220-e91b063b03ed.png&quot; width=&quot;300&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Server" scheme="https://jeongwookie.github.io/categories/Programming/Server/"/>
    
    
      <category term="Server setting" scheme="https://jeongwookie.github.io/tags/Server-setting/"/>
    
      <category term="Ubuntu" scheme="https://jeongwookie.github.io/tags/Ubuntu/"/>
    
      <category term="Linux" scheme="https://jeongwookie.github.io/tags/Linux/"/>
    
      <category term="Boot USB" scheme="https://jeongwookie.github.io/tags/Boot-USB/"/>
    
  </entry>
  
  <entry>
    <title>R을 사용한 데이터 시각화 - 3. histogram과 barplot 그리기</title>
    <link href="https://jeongwookie.github.io/2019/03/25/190325-data-visualization-using-R-3/"/>
    <id>https://jeongwookie.github.io/2019/03/25/190325-data-visualization-using-R-3/</id>
    <published>2019-03-25T13:23:09.000Z</published>
    <updated>2019-05-31T15:19:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>이번 포스트에서는 Histogram 과 Boxplot 다루어 볼 것이다.</p><p>내장 데이터셋인 <code>cane</code>을 활용하여 먼저 Histogram을 그려보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/58715926-d9c6db80-8402-11e9-986b-85a5cd951f12.png" width="450"></p><a id="more"></a><h2 id="Simple-histogram"><a href="#Simple-histogram" class="headerlink" title="Simple histogram"></a>Simple histogram</h2><p><code>hist()</code> 함수가 기본적으로 내장되어 있다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(boot)</span><br><span class="line">head(cane)</span><br><span class="line">ratio &lt;- cane$r/cane$n <span class="comment"># dataset_name$column_name</span></span><br><span class="line">hist(ratio, breaks = <span class="number">20</span>, xlab = <span class="string">"Diseased Shoot Ratio"</span>, </span><br><span class="line">    col = <span class="string">'aquamarine'</span>, main = <span class="string">"Histogram of Diseased Shoot Ratio"</span>)</span><br></pre></td></tr></table></figure><p>코드에서 <code>breaks</code> 옵션은 블록의 크기를 얼마나 잘게 쪼갤지 조절한다.</p><blockquote><p>왼쪽은 breaks = 20, 오른쪽은 breaks = 40 일때</p></blockquote><div style="width:50%; height:450px; float:left;"><br><img src="https://user-images.githubusercontent.com/25416425/58709127-7635b180-83f4-11e9-82b1-f0aa805f3e23.png" width="450"><br></div><div style="width:50%; height:450px; float:right;"><br><img src="https://user-images.githubusercontent.com/25416425/58709161-8fd6f900-83f4-11e9-9694-471d2de5b72f.png" width="450"><br></div><h2 id="ggplot으로-histogram-그리기"><a href="#ggplot으로-histogram-그리기" class="headerlink" title="ggplot으로 histogram 그리기"></a>ggplot으로 histogram 그리기</h2><p>ggplot으로 histogram을 그릴 때는, <code>geom_histogram()</code> 함수를 사용한다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">ggplot(cane, aes(x=r/n, fill=block)) + </span><br><span class="line">  geom_histogram(col=<span class="string">"black"</span>, bins=<span class="number">20</span>) + theme_bw() + </span><br><span class="line">  facet_grid(block ~ .) + xlab(<span class="string">'Diseased Shoot Ratio'</span>) + </span><br><span class="line">  theme(legend.position = <span class="string">"none"</span>) <span class="comment"># legend를 제거</span></span><br></pre></td></tr></table></figure><p><code>fill</code> 옵션으로 block 별로 다르게 색상을 표시하라고 지정했다.</p><p>히스토그램 내 <code>bins</code> 옵션은 위의 <code>breaks</code>와 동일한 효과이며, <code>facet_grid()</code> 를 활용하여 cane 내의 block 값 별로 분리하여 출력했다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58710078-7df65580-83f6-11e9-8436-e8374169e332.png" width="450"></p><h2 id="Simple-boxplot"><a href="#Simple-boxplot" class="headerlink" title="Simple boxplot"></a>Simple boxplot</h2><p>이번에는 <strong>Boxplot</strong> 을 그려 보자. 주로 그룹 간의 차이를 나타낼 때 사용하며, 꽤나 자주 등장하는 타입이므로 중요하다.</p><p>데이터는 내장 데이터셋인 <code>ToothGrowth</code>를 사용한다. 데이터를 먼저 살펴보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/58710691-de39c700-83f7-11e9-9855-7e041a70ed07.png" width="450"></p><p>기본적으로 <code>boxplot()</code> 함수로 그릴 수 있다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># simple boxplot</span></span><br><span class="line">boxplot(len~supp, data=ToothGrowth, xlab=<span class="string">'Supplement Type'</span>, ylab=<span class="string">'Tooth Length'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Boxplot of len against dose and supp factors</span></span><br><span class="line">boxplot(len~supp*dose, data=ToothGrowth, notch=<span class="literal">T</span>, </span><br><span class="line">        xlab=<span class="string">'Suppliment and Dose'</span>, ylab=<span class="string">'Tooth length'</span>, col=c(<span class="string">'cyan'</span>, <span class="string">'magenta'</span>))</span><br></pre></td></tr></table></figure><blockquote><p>왼쪽은 <strong>supp</strong>을 기준으로, 오른쪽은 <strong>supp</strong>와 <strong>dose</strong>를 조합하여 그린 boxplot</p></blockquote><div style="width:50%; height:450px; float:left;"><br><img src="https://user-images.githubusercontent.com/25416425/58710962-759f1a00-83f8-11e9-91ef-d743bce361e4.png" width="450"><br></div><div style="width:50%; height:450px; float:right;"><br><img src="https://user-images.githubusercontent.com/25416425/58710999-894a8080-83f8-11e9-9eec-28b45072b2a1.png" width="450"><br></div><h2 id="ggplot으로-boxplot-그리기"><a href="#ggplot으로-boxplot-그리기" class="headerlink" title="ggplot으로 boxplot 그리기"></a>ggplot으로 boxplot 그리기</h2><p>ggplot으로 boxplot을 그릴 때는, <code>geom_boxplot()</code> 함수를 사용한다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">ggplot(ToothGrowth, aes(x=factor(dose), y=len)) +</span><br><span class="line">  geom_boxplot(aes(fill=supp)) +</span><br><span class="line">  xlab(<span class="string">'Dose'</span>) + ylab(<span class="string">'Tooth length'</span>) + ggtitle(<span class="string">'Analyzing ToothGrowth Data'</span>)</span><br></pre></td></tr></table></figure><p>x 값에 우리가 원하는 <code>dose</code>를 넣을 때, 자료형을 num 에서 factor로 바꿔주기 위해서 (category화) <code>factor()</code> 함수를 사용했다.</p><p>boxplot의 색상은 <code>supp</code>에 따라 바뀌도록 <code>fill</code> 옵션을 지정하였다.</p><p><img src="https://user-images.githubusercontent.com/25416425/58711160-d9294780-83f8-11e9-8689-429a08b549be.png" width="450"></p><p>확실히 ggplot으로 그린 그래프들이 전반적으로 깔끔하다. 기본적인 graph 그리기 포스트는 끝!</p><blockquote><p><em>본 포스트는 KAIST 전산학부 대학원 과정에서 수강하고 있는 Big Data Analytics using R (CS564)을 실습하며 작성하였음을 밝힙니다.</em></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;이번 포스트에서는 Histogram 과 Boxplot 다루어 볼 것이다.&lt;/p&gt;
&lt;p&gt;내장 데이터셋인 &lt;code&gt;cane&lt;/code&gt;을 활용하여 먼저 Histogram을 그려보자.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25416425/58715926-d9c6db80-8402-11e9-986b-85a5cd951f12.png&quot; width=&quot;450&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="R programming" scheme="https://jeongwookie.github.io/categories/Programming/R-programming/"/>
    
    
      <category term="KAIST" scheme="https://jeongwookie.github.io/tags/KAIST/"/>
    
      <category term="R programming" scheme="https://jeongwookie.github.io/tags/R-programming/"/>
    
      <category term="Class" scheme="https://jeongwookie.github.io/tags/Class/"/>
    
  </entry>
  
  <entry>
    <title>Data Crawling - 네이버 검색 api 사용하여 데이터 수집하기</title>
    <link href="https://jeongwookie.github.io/2019/03/20/190320-collect-data-using-naver-search-api/"/>
    <id>https://jeongwookie.github.io/2019/03/20/190320-collect-data-using-naver-search-api/</id>
    <published>2019-03-20T02:17:11.000Z</published>
    <updated>2019-05-21T05:34:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>이번 포스트는 네이버 개발자 센터에서 제공하는 검색 api를 통해 우리가 원하는 키워드를 검색하고, 그 결과를 json파일로 저장하는 것을 다루어보겠다.</p><p>json파일로 저장하는 이유는 <code>pandas</code>와 같은 데이터 분석 툴을 사용할 때, import하기 좋은 파일 형식이기 때문이다.</p><p>아래와 같은 순서로 진행한다.</p><ul><li>Step 1: 네이버 개발자센터 등록 및 키 값 받아오기</li><li>Step 2: api caller 만들기</li><li>Step 3: 데이터 수집 후 json 파일로 저장하기</li><li>Step 4: pandas 사용하여 분석하기</li></ul><a id="more"></a><h4 id="네이버-개발자센터-등록하기"><a href="#네이버-개발자센터-등록하기" class="headerlink" title="네이버 개발자센터 등록하기"></a>네이버 개발자센터 등록하기</h4><p>먼저 <a href="https://developers.naver.com/main/" rel="external nofollow noopener noreferrer" target="_blank">네이버 개발자 센터</a>에 접속하여 Application -&gt; 애플리케이션 등록 을 클릭한다.</p><p>아래와 같은 화면이 뜨는데, 검색 api 누르고 안드로이드 설정에서 적당한 이름을 넣으면 된다. (com.블라블라)</p><p><img src="https://user-images.githubusercontent.com/25416425/54655089-e75b0d80-4b03-11e9-80cd-fdb8a1c8e654.png" width="550"></p><p>완료하게 되면, 애플리케이션 정보에 client ID와 client secret 코드가 보이는데 이걸 어딘가 안전한 곳에 복사해둔다. (노출 절대 금지)</p><h4 id="api-caller-만들기"><a href="#api-caller-만들기" class="headerlink" title="api caller 만들기"></a>api caller 만들기</h4><p>이제, 네이버 검색 api를 불러올 caller 을 만들어 보자.</p><p>아까 전 네이버 개발자 센터 홈페이지에서 Documents -&gt; 서비스 API -&gt; 검색을 누르면 아래와 같은 페이지가 뜬다. 코드는 python으로 클릭.</p><p><img src="https://user-images.githubusercontent.com/25416425/54655436-43726180-4b05-11e9-8385-1dc66ae2e745.png" width="550"></p><p>여기 있는 코드를 긁어서 오면 아래와 같다. 중간에 <code>YOUR_CLIENT_ID</code>와 <code>YOUR_CLIENT_SECRET</code>에는 아까 복사해 두었던 키값들을 기입한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 네이버 검색 API예제는 블로그를 비롯 전문자료까지 호출방법이 동일하므로 blog검색만 대표로 예제를 올렸습니다.</span></span><br><span class="line"><span class="comment"># 네이버 검색 Open API 예제 - 블로그 검색</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">client_id = “YOUR_CLIENT_ID”</span><br><span class="line">client_secret = “YOUR_CLIENT_SECRET”</span><br><span class="line">encText = urllib.parse.quote(“검색할 단어”)</span><br><span class="line">url = “https://openapi.naver.com/v1/search/blog?query=" + encText # json 결과</span><br><span class="line"><span class="comment"># url = ”https://openapi.naver.com/v1/search/blog.xml?query=" + encText # xml 결과</span></span><br><span class="line">request = urllib.request.Request(url)</span><br><span class="line">request.add_header(“X-Naver-Client-Id”,client_id)</span><br><span class="line">request.add_header(“X-Naver-Client-Secret”,client_secret)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">rescode = response.getcode()</span><br><span class="line"><span class="keyword">if</span>(rescode==<span class="number">200</span>):</span><br><span class="line">    response_body = response.read()</span><br><span class="line">    print(response_body.decode(‘utf<span class="number">-8</span>’))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(“Error Code:” + rescode)</span><br></pre></td></tr></table></figure><p>샘플 코드의 주석에 적힌 대로, 현재 이 코드는 키워드를 받아서 블로그를 검색한 결과를 보여준다.</p><p>코드가 너무 기니까 조금 간략하게 바꾸어 보자. 대충 이런 식이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> </span><br><span class="line">keyword = “”</span><br><span class="line">url = “https://openapi.naver.com/v1/search/blog?query=" + keyword</span><br><span class="line">result = requests.get(urlparse(url).geturl(),</span><br><span class="line">                      headers=&#123;“X-Naver-Client-Id”:“YOUR_CLIENT_ID”,</span><br><span class="line">                             “X-Naver-Client-Secret”:“YOUR_CLIENT_SECRET”&#125;)</span><br><span class="line">json_obj = result.json()</span><br><span class="line">print(json_obj)</span><br></pre></td></tr></table></figure><p>위 코드는 <code>keyword</code> 를 받아서 네이버 검색 api를 거친 후 <code>json_obj</code>를 반환한다.</p><p>이제 call을 하는 함수를 만들어 보자. 키워드를 입력한 후, 그 검색 결과를 한번에 100개씩 받아오는 함수를 작성한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(keyword, start)</span>:</span></span><br><span class="line">    encText = quote(keyword)</span><br><span class="line">    url = “https://openapi.naver.com/v1/search/blog?query=" + encText </span><br><span class="line">        + “&amp;display=<span class="number">100</span>” + “&amp;start=” + str(start)</span><br><span class="line">    result = requests.get(url=url, headers=&#123;“X-Naver-Client-Id”:“YOUR_CLIENT_ID”,</span><br><span class="line">                                          “X-Naver-Client-Secret”:“YOUR_CLIENT_SECRET”&#125;)</span><br><span class="line">    print(result)  <span class="comment"># Response [200]</span></span><br><span class="line">    <span class="keyword">return</span> result.json()</span><br></pre></td></tr></table></figure><p><code>&amp;display=100</code>은 한번에 100개의 검색 결과를 보여준다는 것이고, <code>&amp;start=</code>은 그 뒤의 숫자에 따라 어느 순서의 검색 결과부터 출력하는지를 결정한다. </p><p>뒤의 숫자는 그냥 두면 <code>int</code> 속성을 가지게 되므로, <code>str()</code>을 사용하여 <code>string</code>으로 바꾸어 url을 완성시킨다.</p><h4 id="데이터-수집-후-저장하기"><a href="#데이터-수집-후-저장하기" class="headerlink" title="데이터 수집 후 저장하기"></a>데이터 수집 후 저장하기</h4><p>이제, 검색 결과를 한번에 1000개 수집하여 json 파일로 저장해 보자.</p><p>모듈화를 위해 libs 폴더 내 naver_api_call 폴더를 만들고, 그 내부에 먼저 api_caller.py 파일을 만들어 작성하였다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># api_caller.py</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 네이버 api call</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(keyword, start)</span>:</span></span><br><span class="line">    encText = quote(keyword)</span><br><span class="line">    url = “https://openapi.naver.com/v1/search/blog?query=" + encText + </span><br><span class="line">    “&amp;display=<span class="number">100</span>” + “&amp;start=” + str(start)</span><br><span class="line">    result = requests.get(url=url, headers=&#123;“X-Naver-Client-Id”:“YOUR_CLIENT_ID”,</span><br><span class="line">                                          “X-Naver-Client-Secret”:“YOUR_CLIENT_SECRET”&#125;)</span><br><span class="line">    print(result)  <span class="comment"># Response [200]</span></span><br><span class="line">    <span class="keyword">return</span> result.json()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 1000개의 검색 결과 받아오기</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get1000results</span><span class="params">(keyword)</span>:</span></span><br><span class="line">    list = []</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">        list = list + call(keyword, num * <span class="number">100</span> + <span class="number">1</span>)[‘items’] <span class="comment"># list 안에 키값이 ’item’인 애들만 넣기</span></span><br><span class="line">    <span class="keyword">return</span> list</span><br></pre></td></tr></table></figure><p>함수들을 만들었으니, 이제 실제로 사용해 보자. 우리가 할 일은 네이버 api call을 하고, 1000개의 키워드 검색 결과를 받아온 후, 이를 json파일에 저장하는 것이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 01_collect.py</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> libs.naver_api_call.api_caller <span class="keyword">import</span> get1000results</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"> </span><br><span class="line">list = []</span><br><span class="line">result = get1000results(“강남역 맛집”)</span><br><span class="line">result_2 = get1000results(“강남역 찻집”)</span><br><span class="line">list = list + result + result_2</span><br><span class="line"> </span><br><span class="line">file = open(“./gangnam.json”, “w+”)  <span class="comment"># gangnam.json 파일을 쓰기 가능한 상태로 열기 (만들기)</span></span><br><span class="line">file.write(json.dumps(list))  <span class="comment"># 쓰기</span></span><br></pre></td></tr></table></figure><p>위 코드를 출력하면 정상적으로 <code>gangnam.json</code>이 디렉토리에 생성됨을 확인할 수 있다.</p><h4 id="pandas-사용하여-분석하기"><a href="#pandas-사용하여-분석하기" class="headerlink" title="pandas 사용하여 분석하기"></a>pandas 사용하여 분석하기</h4><p><code>pandas</code>는 여러가지 분석에 유용한 함수들을 제공하여, 길고 긴 json 파일을 보다 쉽고 빠르게 파악할 수 있도록 돕는다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 02_analyze_pd.py</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"> </span><br><span class="line">df = pd.read_json(“./gangnam.json”)</span><br><span class="line">print(df.count())  <span class="comment"># 각 key 별 숫자 출력</span></span><br><span class="line"> </span><br><span class="line">df_sum = df.groupby(“bloggername”).count()  <span class="comment"># groupby() 함수를 사용하여 bloggername별로 출력</span></span><br><span class="line">print(df_sum)</span><br><span class="line"> </span><br><span class="line">bloggernames = df[‘bloggername’]  <span class="comment"># bloggername 만 출력</span></span><br><span class="line">print(bloggernames)</span><br></pre></td></tr></table></figure><p><code>count()</code> 는 json 파일에 저장된 각각의 key값에 대한 데이터 숫자를 세어준다. 우리는 위에서 “강남역 맛집”으로 검색한 결과 1000개, 그리고 “강남역 찻집”으로 검색한 결과 1000개로 총 2000개의 결과값을 저장해 두었었다.</p><p><code>groupby()</code> 는 지정한 key값으로 결과값을 그룹화하여 보여준다. 결과값을 보면 어떤 bloggername을 가진 사람이 우리가 지정한 키워드에 대해서 블로깅을 많이 하였는지 알 수 있다.</p><p><img src="https://user-images.githubusercontent.com/25416425/54686400-8a3f7600-4b5c-11e9-8715-315304fda0d8.png" width="550"></p><p>세 번째 출력값은 현재 저장된 json 파일에서 bloggername 을 전부 출력해 본 것이다. 총 1640개의 블로그가 수집되었음을 알 수 있다.</p><p><img src="https://user-images.githubusercontent.com/25416425/54687073-100ff100-4b5e-11e9-8eda-8465816a34be.png" width="550"></p><blockquote><p>위 포스트는 Kyeongrok Kim님의 네이버 api 불러오기 예시를 실습해보고, 이를 간단히 정리한 것임을 밝힙니다.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;이번 포스트는 네이버 개발자 센터에서 제공하는 검색 api를 통해 우리가 원하는 키워드를 검색하고, 그 결과를 json파일로 저장하는 것을 다루어보겠다.&lt;/p&gt;
&lt;p&gt;json파일로 저장하는 이유는 &lt;code&gt;pandas&lt;/code&gt;와 같은 데이터 분석 툴을 사용할 때, import하기 좋은 파일 형식이기 때문이다.&lt;/p&gt;
&lt;p&gt;아래와 같은 순서로 진행한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step 1: 네이버 개발자센터 등록 및 키 값 받아오기&lt;/li&gt;
&lt;li&gt;Step 2: api caller 만들기&lt;/li&gt;
&lt;li&gt;Step 3: 데이터 수집 후 json 파일로 저장하기&lt;/li&gt;
&lt;li&gt;Step 4: pandas 사용하여 분석하기&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Data Crawling" scheme="https://jeongwookie.github.io/categories/Programming/Data-Crawling/"/>
    
    
      <category term="Python" scheme="https://jeongwookie.github.io/tags/Python/"/>
    
      <category term="WEB Crawling" scheme="https://jeongwookie.github.io/tags/WEB-Crawling/"/>
    
      <category term="Datamining" scheme="https://jeongwookie.github.io/tags/Datamining/"/>
    
  </entry>
  
  <entry>
    <title>Data Crawling - 네이버 금융, 주식 가격 수집하기</title>
    <link href="https://jeongwookie.github.io/2019/03/18/190318-naver-finance-data-crawling-using-python/"/>
    <id>https://jeongwookie.github.io/2019/03/18/190318-naver-finance-data-crawling-using-python/</id>
    <published>2019-03-18T01:36:15.000Z</published>
    <updated>2019-05-24T14:57:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>19년 3월부터 카이스트 데이터사이언스 연구실 (KAIST DS LAB)에서 일하기 시작했다.</p><p>가장 처음 맡은 일이 파이썬으로 특정 데이터들을 웹에서 크롤링하는 것인데, 예전에 <code>BeautlfulSoup4</code>으로 간단하게 몇 가지 다루어 본 것을 복습하는 겸 포스트를 작성하기로 했다. </p><p>기본적으로 작업은 파이참 (pyCharm)으로 진행했다.</p><a id="more"></a><h3 id="네이버-금융-웹에서-봉차트-데이터-수집하기"><a href="#네이버-금융-웹에서-봉차트-데이터-수집하기" class="headerlink" title="네이버 금융 웹에서 봉차트 데이터 수집하기"></a>네이버 금융 웹에서 봉차트 데이터 수집하기</h3><p>아래와 같은 순서로 진행한다.</p><ul><li>Step 1: 특정 종목의 가격 받아오기</li><li>Step 2: 여러 종목의 가격 받아오기</li><li>Step 3: 특정 종목의 봉차트 데이터 받아오기</li><li>Step 4: 여러 종목의 봉차트 데이터 받아오기</li></ul><p>시작하기 전, interpreter가 제대로 설치되어 있는지 체크하자.</p><p>우리가 이번에 사용할 패키지는 <code>requests</code> 와 <code>beautifulsoup4</code>이다.</p><p>맥에서는 preference -&gt; project interpreter에 가면 다운로드 및 확인이 가능하고,</p><p>윈도우에서는 file -&gt; setting -&gt; project interpreter에 가면 마찬가지로 확인이 가능하다.<br><img src="https://user-images.githubusercontent.com/25416425/54502101-45ed8380-496c-11e9-9ae6-d4de73d8a23b.png" width="550"><center> 맥에서 파이참 preference을 열었을 때 </center></p><h4 id="특정-종목의-가격-받아오기"><a href="#특정-종목의-가격-받아오기" class="headerlink" title="특정 종목의 가격 받아오기"></a>특정 종목의 가격 받아오기</h4><p>요즘 펄어비스가 신작을 발표하면서 주식 가격이 떡상(?)하고 있다. 개인적으로 관심이 가는 회사니까.. 여기 데이터를 한번 가져와 보자.</p><p>먼저 <a href="https://finance.naver.com/" rel="external nofollow noopener noreferrer" target="_blank">네이버 금융</a>에 접속해서 펄어비스를 검색해 보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/54502363-8ef20780-496d-11e9-92b9-06bb0abe0c6e.png" width="550"><center> 빨간색 네모가 우리가 가져가고 싶은 데이터 </center></p><p>저 빨간색 네모 안의 숫자가 우리가 크롤링 하고 싶은 펄어버스 주식의 현재 가격이다. 크롬 (chrome)의 개발자 도구를 켜서 위치를 확인하자.</p><p><img src="https://user-images.githubusercontent.com/25416425/54502950-7800e480-4970-11e9-85ac-7924cfa59680.png" width="550"><br><img src="https://user-images.githubusercontent.com/25416425/54503082-f9587700-4970-11e9-9615-e26feae53448.png" width="550"><br>이제 코드를 짜면 된다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"> </span><br><span class="line">url = “https://finance.naver.com/item/main.nhn?code=263750"</span><br><span class="line">result = requests.get(url)</span><br><span class="line">bs_obj = BeautifulSoup(result.content, “html.parser”)</span><br><span class="line"> </span><br><span class="line">no_today = bs_obj.find(“p”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “no_today”&#125;) <span class="comment"># 태그 p, 속성값 no_today 찾기</span></span><br><span class="line">blind = no_today.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;) <span class="comment"># 태그 span, 속성값 blind 찾기</span></span><br><span class="line">now_price = blind.text</span><br><span class="line"> </span><br><span class="line">print(now_price)</span><br></pre></td></tr></table></figure><p>실행하면 <code>now_price</code>가 187,500이 제대로 출력됨을 확인할 수 있다.<br>(실시간으로 가격 변동이 발생 ㅠㅠ)</p><h4 id="여러-종목의-가격-받아오기"><a href="#여러-종목의-가격-받아오기" class="headerlink" title="여러 종목의 가격 받아오기"></a>여러 종목의 가격 받아오기</h4><p>자세히 보면, 우리가 입력한 url의 제일 뒤 숫자 6자리가 <strong>회사 코드</strong>임을 알 수 있다.</p><p>이를 원하는 코드로 바꿈으로써, 여러 회사들의 주식 가격을 가져올 수 있다.</p><p>위 코드에 대해서 <code>company_code</code>를 입력하면 <code>now_price</code>를 출력할 수 있도록 리팩토링 (refactoring) 해보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"> </span><br><span class="line"><span class="comment"># company_code를 입력받아 bs_obj를 출력</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bs_obj</span><span class="params">(company_code)</span>:</span></span><br><span class="line">    url = “https://finance.naver.com/item/main.nhn?code=" + company_code</span><br><span class="line">    result = requests.get(url)</span><br><span class="line">    bs_obj = BeautifulSoup(result.content, “html.parser”)</span><br><span class="line">    <span class="keyword">return</span> bs_obj</span><br><span class="line"> </span><br><span class="line"><span class="comment"># company_code를 입력받아 now_price를 출력</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_price</span><span class="params">(company_code)</span>:</span></span><br><span class="line">    bs_obj = get_bs_obj(company_code)</span><br><span class="line">    no_today = bs_obj.find(“p”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “no_today”&#125;)</span><br><span class="line">    blind = no_today.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;)</span><br><span class="line">    now_price = blind.text</span><br><span class="line">    <span class="keyword">return</span> now_price</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 펄어비스 회사 코드는 ”263750”</span></span><br><span class="line"><span class="comment"># 삼성전자 회사 코드는 ”005930”</span></span><br><span class="line"><span class="comment"># 셀트리온 회사 코드는 ”068270”</span></span><br><span class="line">company_codes = [“<span class="number">263750</span>”, “<span class="number">005930</span>”, “<span class="number">068270</span>”]</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> company_codes:</span><br><span class="line">    now_price = get_price(item)</span><br><span class="line">    print(now_price)</span><br></pre></td></tr></table></figure><p>실행하면, 펄어비스와 삼성전자, 그리고 셀트리온의 현재 주식 가격을 잘 받아오고 있음을 확인할 수 있다.</p><h4 id="특정-종목의-봉차트-데이터-받아오기"><a href="#특정-종목의-봉차트-데이터-받아오기" class="headerlink" title="특정 종목의 봉차트 데이터 받아오기"></a>특정 종목의 봉차트 데이터 받아오기</h4><p>이번에는 펄어비스의 전일, 고가, 시가, 저가 주식 데이터를 가져와 보자.</p><p>아까와 마찬가지로 먼저 데이터의 위치를 개발자 도구를 사용하여 알아보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/54504757-4a1f9e00-4978-11e9-86ae-c3a715769c4a.png" width="550"><br><img src="https://user-images.githubusercontent.com/25416425/54504673-e4cbad00-4977-11e9-8a58-1abd7043586f.png" width="550"><br>빨간색 네모 안에 전일 주식 가격이 들어있음을 확인한 후, 코드를 짜보자.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"> </span><br><span class="line">url = “https://finance.naver.com/item/main.nhn?code=263750"</span><br><span class="line">result = requests.get(url)</span><br><span class="line">bs_obj = BeautifulSoup(result.content, “html.parser”)</span><br><span class="line"> </span><br><span class="line">td_first = bs_obj.find(“td”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “first”&#125;)  <span class="comment"># 태그 td, 속성값 first 찾기</span></span><br><span class="line">blind = td_first.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;)  <span class="comment"># 태그 span, 속성값 blind 찾기</span></span><br><span class="line">close = blind.text</span><br><span class="line"> </span><br><span class="line">print(close)</span><br></pre></td></tr></table></figure><p>전일 (close)에 해당하는 펄어비스의 주식 가격이 제대로 출력되고 있다.</p><p>위와 같은 방법으로, 고가 (high), 시가 (open), 저가 (low)에 대한 데이터의 위치를 찾아서 코딩하면 된다.</p><p>주의할 점은, <code>find()</code> 함수는 가장 처음 만나는 태그를 반환하므로, 그 뒤의 동일한 이름의 태그를 찾고 싶다면 <code>find_all()</code> 함수를 사용해야 한다.</p><p>또한, <code>find_all()</code>은 리스트 (list)를 반환한다.</p><p>대략적으로 각 데이터의 위치를 아래에 그려 보았다.<br><img src="https://user-images.githubusercontent.com/25416425/54505445-2ad64000-497b-11e9-8ac2-05785fa16ed5.png" width="550"></p><p><code>tr</code> 태그가 두 개, 그리고 각각의 태그 안에 <code>td</code> 태그가 세 개씩 있고, 첫 번째 <code>td</code>는 다행히도 속성값이 명시되어 있지만 두 번째 <code>td</code>부터는 그렇지 않다.</p><p>위와 같은 형태일 때는 모두를 포함하면서 속성값을 가지고 있어, 전체 데이터에서 특정지을 수 있는 태그부터 찾아서 narrow down해야 한다.</p><p>이번에는 그 역할로 <code>table</code> 태그의 속성값 <code>no_info</code>를 사용할 것이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"> </span><br><span class="line">url = “https://finance.naver.com/item/main.nhn?code=263750"</span><br><span class="line">result = requests.get(url)</span><br><span class="line">bs_obj = BeautifulSoup(result.content, “html.parser”)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#close 종가(전일)</span></span><br><span class="line">td_first = bs_obj.find(“td”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “first”&#125;)  <span class="comment"># 태그 td, 속성값 first 찾기</span></span><br><span class="line">blind = td_first.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;)  <span class="comment"># 태그 span, 속성값 blind 찾기</span></span><br><span class="line">close = blind.text</span><br><span class="line"> </span><br><span class="line"><span class="comment"># high 고가</span></span><br><span class="line">table = bs_obj.find(“table”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “no_info”&#125;)  <span class="comment"># 태그 table, 속성값 no_info 찾기</span></span><br><span class="line">trs = table.find_all(“tr”)  <span class="comment"># tr을 list로 []</span></span><br><span class="line">first_tr = trs[<span class="number">0</span>]  <span class="comment"># 첫 번째 tr 지정</span></span><br><span class="line">tds = first_tr.find_all(“td”)  <span class="comment"># 첫 번째 tr 안에서 td를 list로</span></span><br><span class="line">second_tds = tds[<span class="number">1</span>]  <span class="comment"># 두 번째 td 지정</span></span><br><span class="line">high = second_tds.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;).text</span><br><span class="line"> </span><br><span class="line"><span class="comment"># open 시가</span></span><br><span class="line">second_tr = trs[<span class="number">1</span>]  <span class="comment"># 두 번째 tr 지정</span></span><br><span class="line">tds_second_tr = second_tr.find_all(“td”)  <span class="comment"># 두 번째 tr 안에서 td를 list로</span></span><br><span class="line">first_td_in_second_tr = tds_second_tr[<span class="number">0</span>]  <span class="comment"># 첫 번째 td 지정</span></span><br><span class="line">open = first_td_in_second_tr.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;).text</span><br><span class="line"> </span><br><span class="line"><span class="comment"># low 저가</span></span><br><span class="line">second_td_in_second_tr = tds_second_tr[<span class="number">1</span>]  <span class="comment"># 두 번째 td 지정</span></span><br><span class="line">low = second_td_in_second_tr.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;).text</span><br><span class="line"> </span><br><span class="line">print(close)</span><br><span class="line">print(high)</span><br><span class="line">print(open)</span><br><span class="line">print(low)</span><br></pre></td></tr></table></figure><p>위 코드를 실행시키면, 펄어비스의 봉차트 주식 데이터가 제대로 출력됨을 확인할 수 있다.</p><h4 id="여러-종목의-봉차트-데이터-받아오기"><a href="#여러-종목의-봉차트-데이터-받아오기" class="headerlink" title="여러 종목의 봉차트 데이터 받아오기"></a>여러 종목의 봉차트 데이터 받아오기</h4><p>이제 위 코드에 대해서 <code>company_code</code>를 입력하면 봉차트 데이터를 출력할 수 있도록 리팩토링 (refactoring) 해보자.</p><p>입력값은 <code>company_code</code>이고, 리턴값은 <code>close</code>, <code>high</code>, <code>open</code>, <code>low</code>인 함수를 짜면 된다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bs_obj</span><span class="params">(company_code)</span>:</span></span><br><span class="line">    url = “https://finance.naver.com/item/main.nhn?code=" + company_code</span><br><span class="line">    result = requests.get(url)</span><br><span class="line">    bs_obj = BeautifulSoup(result.content, “html.parser”)</span><br><span class="line">    <span class="keyword">return</span> bs_obj</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_candle_chart</span><span class="params">(company_code)</span>:</span></span><br><span class="line">    bs_obj = get_bs_obj(company_code)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># close 종가(전일)</span></span><br><span class="line">    td_first = bs_obj.find(“td”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “first”&#125;)  <span class="comment"># 태그 td, 속성값 first 찾기</span></span><br><span class="line">    blind = td_first.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;)  <span class="comment"># 태그 span, 속성값 blind 찾기</span></span><br><span class="line">    close = blind.text</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># high 고가</span></span><br><span class="line">    table = bs_obj.find(“table”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “no_info”&#125;)  <span class="comment"># 태그 table, 속성값 no_info 찾기</span></span><br><span class="line">    trs = table.find_all(“tr”)  <span class="comment"># tr을 list로 []</span></span><br><span class="line">    first_tr = trs[<span class="number">0</span>]  <span class="comment"># 첫 번째 tr 지정</span></span><br><span class="line">    tds = first_tr.find_all(“td”)  <span class="comment"># 첫 번째 tr 안에서 td를 list로</span></span><br><span class="line">    second_tds = tds[<span class="number">1</span>]  <span class="comment"># 두 번째 td 지정</span></span><br><span class="line">    high = second_tds.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;).text</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># open 시가</span></span><br><span class="line">    second_tr = trs[<span class="number">1</span>]  <span class="comment"># 두 번째 tr 지정</span></span><br><span class="line">    tds_second_tr = second_tr.find_all(“td”)  <span class="comment"># 두 번째 tr 안에서 td를 list로</span></span><br><span class="line">    first_td_in_second_tr = tds_second_tr[<span class="number">0</span>]  <span class="comment"># 첫 번째 td 지정</span></span><br><span class="line">    open = first_td_in_second_tr.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;).text</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># low 저가</span></span><br><span class="line">    second_td_in_second_tr = tds_second_tr[<span class="number">1</span>]  <span class="comment"># 두 번째 td 지정</span></span><br><span class="line">    low = second_td_in_second_tr.find(“span”, &#123;“<span class="class"><span class="keyword">class</span>”:</span> “blind”&#125;).text</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> &#123;“close”: close, “high”: high, “open”: open, “low”: low&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 펄어비스 회사 코드는 ”263750”</span></span><br><span class="line"><span class="comment"># 삼성전자 회사 코드는 ”005930”</span></span><br><span class="line"><span class="comment"># 셀트리온 회사 코드는 ”068270”</span></span><br><span class="line">company_codes = [“<span class="number">263750</span>”, “<span class="number">005930</span>”, “<span class="number">068270</span>”]</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> company_codes:</span><br><span class="line">    candle_chart = get_candle_chart(item)</span><br><span class="line">    print(candle_chart)</span><br></pre></td></tr></table></figure><p>위 코드를 실행시키면 펄어비스, 삼성전자, 셀트리온의 주식 봉차트 데이터가 순서대로 출력됨을 확인할 수 있다.</p><p>다음 포스트에서는 조금 더 복잡한 데이터 크롤링을 다루어 보겠다.</p><blockquote><p>위 포스트는 Kyeongrok Kim님의 데이터 크롤링 예시를 실습해보고, 이를 간단히 정리한 것임을 밝힙니다.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;19년 3월부터 카이스트 데이터사이언스 연구실 (KAIST DS LAB)에서 일하기 시작했다.&lt;/p&gt;
&lt;p&gt;가장 처음 맡은 일이 파이썬으로 특정 데이터들을 웹에서 크롤링하는 것인데, 예전에 &lt;code&gt;BeautlfulSoup4&lt;/code&gt;으로 간단하게 몇 가지 다루어 본 것을 복습하는 겸 포스트를 작성하기로 했다. &lt;/p&gt;
&lt;p&gt;기본적으로 작업은 파이참 (pyCharm)으로 진행했다.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="Data Crawling" scheme="https://jeongwookie.github.io/categories/Programming/Data-Crawling/"/>
    
    
      <category term="Python" scheme="https://jeongwookie.github.io/tags/Python/"/>
    
      <category term="WEB Crawling" scheme="https://jeongwookie.github.io/tags/WEB-Crawling/"/>
    
      <category term="Datamining" scheme="https://jeongwookie.github.io/tags/Datamining/"/>
    
  </entry>
  
  <entry>
    <title>KAIST 창업석사 신입생 모집</title>
    <link href="https://jeongwookie.github.io/2019/03/14/190314-kschool-2019-fall-admission/"/>
    <id>https://jeongwookie.github.io/2019/03/14/190314-kschool-2019-fall-admission/</id>
    <published>2019-03-14T08:36:59.000Z</published>
    <updated>2019-03-25T11:58:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>안녕하세요! 오늘은 제가 다니고 있는 카이스트 창업석사 모집 소식을 들고 찾아왔습니다.</p><p>창업석사 자체에 대한 소개글은 이전 포스트인 <a href="https://jeongwookie.github.io/2018/09/14/180914-kschool-student/">KAIST 창업석사</a>를 참고해 주세요.</p><p>이번에 입학하시는 분들은 k-school 7기 생으로 입학하시는 거에요. (19년 가을학기 시작)</p><a id="more"></a><p><img src="https://user-images.githubusercontent.com/25416425/54342510-ff540c80-467f-11e9-88ed-8dbc343577fa.png" width="450"></p><p>학교에서 하는 오피셜한 설명회가 총 2번 있는데 각각 서울 (3월 27일), 대전 (3월 20일)에서 열립니다. 여기에는 제 선배나 동기분들도 참여하셔서 직접 질의응답을 진행하실 꺼에요. 학교 생활이나 군대 등 궁금하신 부분이 있다면 직접 참여하셔서 물어봐도 좋을 것 같군요.</p><p>이번에도 많은 학과들이 창업석사로 학생을 선발하고 있습니다. 참여 학과는 <a href="https://kschool.kaist.ac.kr/Page/Kschool_Area" rel="external nofollow noopener noreferrer" target="_blank">여기</a></p><p>제 주변에도 그렇고, 제 다음으로 입학하신 분들도 수업때 만나 보았었는데요. 학과도 다양하고, 나이대도 다양합니다. 옛날에 카이스트 학부를 졸업하시고 근 10년만에 다시 창업석사로 오신 분도 계십니다 ^^ 그만큼 메리트가 있다는 것이겠죠.</p><p>결국 창업의 성패는 같이 하는 사람이 누구인가에 따라 크게 좌우됩니다. 좋은 분들과 만날 기회를 가지고, 같이 공부하고 디스커션 하다 보면 공동 창업자가 되실 분을 만날 수 있지 않을까요?</p><p>특히, 현재 전산학부를 베이스로 가지고 계신 분이나, 개발자로 전향하고자 하시는 분들이 주목해야 할 모집 학과를 아래에 정리해 보았습니다.</p><ul><li>전산학부 (<a href="https://cs.kaist.ac.kr/" rel="external nofollow noopener noreferrer" target="_blank">https://cs.kaist.ac.kr/</a>)</li><li>산업및시스템공학과 (<a href="http://ie.kaist.ac.kr/" rel="external nofollow noopener noreferrer" target="_blank">http://ie.kaist.ac.kr/</a>)</li><li>지식서비스공학대학원 (<a href="http://kse.kaist.ac.kr/insiter.php?design_file=1246.php" rel="external nofollow noopener noreferrer" target="_blank">http://kse.kaist.ac.kr/insiter.php?design_file=1246.php</a>)</li><li>문화기술대학원 (<a href="https://ct.kaist.ac.kr/main.php?lang=2" rel="external nofollow noopener noreferrer" target="_blank">https://ct.kaist.ac.kr/main.php?lang=2</a>)</li></ul><p>전산학부는 모두 다 아시듯이 전산 전공을 통해서 특화하여 나가실 분이 좋겠지요. 이에 비해 아래 세가지 전공의 대학원들은 <u>개발 능력을 베이스로 실전에 적용하는 분야들이 다른 케이스</u>입니다.</p><p>창업석사 선발은 기본적으로 두 가지 기준을 가지고 있는데, 하나가 창업원을 운영하는 K-School이고 나머지 하나는 각 학과 교수님 입니다. </p><p>창업원 교수님들의 면접에 통과하셨더라도 학과에서 일반 대학원생을 모집하는 것과 동일한 기준으로 한번 더 시험을 보게 되므로, 각 학과 홈페이지에서 어떤 모집 요강을 가지고 있는지 상세하게 살피시길 바랍니다.</p><p>예를 들어, 전산학부는 코딩 테스트를 보고 이를 전공 면접에서 활용하고, 문화기술대학원은 전공과 자기소개와 관련된 ppt를 준비하여야 합니다. 또한 산업및시스템공학과 에서는 필기 시험을 치르는 등 각 학과 별로 모집하는 방법이 다양합니다.</p><p>하나의 모집 분야에만 지원이 가능하니, 잘 살펴보시고 본인에게 맞는 선택을 하시길 바랍니다.</p><p>선택에 도움이 되시라고 위 전공 분야에서 열린 과목들을 간단하게 아래에 적어봤습니다. (현재 저도 수강중입니다)</p><ul><li>산업및시스템공학부: Machine Learning for Knokledge Service</li><li>문화기술대학원: Big Data Analysis Using R</li><li>지식서비스공학대학원 : Datamining</li></ul><p>보시다시피, 기본적으로 전산학부와 유사한 수업들이 열립니다. 코드 쉐어를 하기도 하구요.</p><p>하지만, 이런 지식들을 가지고 추구하는 방향이 다르기 때문에 각 학과에서 주요 연구실 홈페이지를 들어가셔서 자신이 관심있는 분야를 서칭하시면 도움이 많이 됩니다.</p><p>입학과 관련해서 궁금하신 부분이 있으면 <a href="mailto:captainfox@kaist.ac.kr" rel="external nofollow noopener noreferrer" target="_blank">captainfox@kaist.ac.kr</a>로 메일 주세요.</p><p>많은 도움은 드리지 못하여도, 제가 아는 선에서 답변 드릴 수 있도록 하겠습니다.</p><p>꼭 붙으시길 바랄께요!</p><p><img src="https://user-images.githubusercontent.com/25416425/54344075-8e165880-4683-11e9-90bd-778452d7df7a.jpg" width="450"></p><center> 19년 가을학기 모집 포스터<br>(상세 내용은 kschool 홈페이지 참고) </center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;안녕하세요! 오늘은 제가 다니고 있는 카이스트 창업석사 모집 소식을 들고 찾아왔습니다.&lt;/p&gt;
&lt;p&gt;창업석사 자체에 대한 소개글은 이전 포스트인 &lt;a href=&quot;https://jeongwookie.github.io/2018/09/14/180914-kschool-student/&quot;&gt;KAIST 창업석사&lt;/a&gt;를 참고해 주세요.&lt;/p&gt;
&lt;p&gt;이번에 입학하시는 분들은 k-school 7기 생으로 입학하시는 거에요. (19년 가을학기 시작)&lt;/p&gt;
    
    </summary>
    
      <category term="School" scheme="https://jeongwookie.github.io/categories/School/"/>
    
    
      <category term="KAIST" scheme="https://jeongwookie.github.io/tags/KAIST/"/>
    
      <category term="K-school" scheme="https://jeongwookie.github.io/tags/K-school/"/>
    
  </entry>
  
  <entry>
    <title>R을 사용한 데이터 시각화 - 2. barplot 그리기</title>
    <link href="https://jeongwookie.github.io/2019/03/08/190308-data-visualization-using-R-2/"/>
    <id>https://jeongwookie.github.io/2019/03/08/190308-data-visualization-using-R-2/</id>
    <published>2019-03-08T07:06:15.000Z</published>
    <updated>2019-05-31T13:22:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>지난 포스트에서는 기본적인 plot을 그리는 방법과, 가장 널리 쓰이는 패키지인 ggplot2을 간단히 다루어 보았다.</p><p>이번 포스트에서는 내장 데이터셋을 Barplot (막대 그래프) 으로 표현해 볼 것이다.</p><p>library는 마찬가지로 <code>gglpot2</code> <code>plotrix</code> <code>boot</code> <code>scatterplot3d</code> <code>lattice</code> <code>MASS</code> 을 기본으로 한다.</p><a id="more"></a><h3 id="데이터셋-sleep으로-Barplot-그리기"><a href="#데이터셋-sleep으로-Barplot-그리기" class="headerlink" title="데이터셋 sleep으로 Barplot 그리기"></a>데이터셋 sleep으로 Barplot 그리기</h3><p>먼저 <strong>sleep</strong>가 어떤 형태인지부터 보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/54420514-84f1be00-474d-11e9-833d-22eb7ab4cf8c.png" alt></p><p>열이 세개로, 각각 extra, group, ID 라는 이름을 가지고 있다.</p><p>ID는 각각의 사람을 의미하고, group은 실험의 상태를 분류하며, extra는 이에 대한 결과값이다.</p><p>이제 x축은 ID, y축은 extra, 그리고 group별로 다른 색상을 나타내는 Barplot을 그려 보자.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">head(sleep)</span><br><span class="line"><span class="keyword">attach</span>(sleep) <span class="comment">#sleep내의 objects(extra, group, ID)를 이름으로 쉽게 접근가능</span></span><br><span class="line">y &lt;- rbind(extra[<span class="number">1</span>:<span class="number">10</span>], extra[<span class="number">11</span>:<span class="number">20</span>]) <span class="comment">#group별로 extra값을 나누어 row bind 시킴. form : num</span></span><br><span class="line">barplot(y, names.arg = ID[<span class="number">1</span>:<span class="number">10</span>], col = <span class="number">5</span>:<span class="number">6</span>, xlab = <span class="string">"ID"</span>, ylab = <span class="string">"Extra Sleep Hour"</span>, beside = <span class="literal">T</span>)</span><br><span class="line"><span class="comment"># beside는 논리값으로, True이면 값을 병렬적으로, False이면 값을 쌓아서 반환함</span></span><br><span class="line">abline(h=<span class="number">0</span>) <span class="comment">#높이가 0인 line을 덧그림</span></span><br><span class="line">legend(<span class="string">'topleft'</span>, title = <span class="string">'group'</span>, legend = <span class="number">1</span>:<span class="number">2</span>, fill = <span class="number">5</span>:<span class="number">6</span>) <span class="comment"># legend 추가</span></span><br></pre></td></tr></table></figure><p><strong>결과</strong><br><img src="https://user-images.githubusercontent.com/25416425/54420947-adc68300-474e-11e9-9d39-6d4eaae4db11.png" width="450"></p><p><code>attach()</code>는 데이터를 붙인다는 뜻인데, 데이터 내의 오브젝트에 이름으로 접근이 가능해진다.</p><p>그래서 바로 아랫줄에서 <code>extra[1:10]</code> 이런 식으로 서술이 가능하고, 이를 <code>rbind()</code>을 사용하여 행렬로 만든다.</p><p><code>barplot()</code>을 사용하려면 제일 앞에 들어가는 y를 이처럼 vector 혹은 matrix 형태로 바꾸어 주어야 한다.</p><p>함수 내부의 <code>beside=</code> 옵션은 TRUE일 때 데이터를 병렬적으로 보여주고, FALSE이면 하나의 막대에 전부 합쳐서 출력한다.</p><h3 id="데이터셋-sleep으로-Barplot-그리기-ggplot2"><a href="#데이터셋-sleep으로-Barplot-그리기-ggplot2" class="headerlink" title="데이터셋 sleep으로 Barplot 그리기 (ggplot2)"></a>데이터셋 sleep으로 Barplot 그리기 (ggplot2)</h3><p>이번에는 동일한 barplot을 <code>ggplot2</code> 패키지를 사용해서 그려보자. 함수는 <code>geom_bar()</code>을 사용한다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">ggplot(sleep, aes(x=ID, y=extra, fill = group))+ <span class="comment"># fill은 면의 컬러를 채울때 사용 </span></span><br><span class="line">  geom_bar(stat = <span class="string">"identity"</span>, position = <span class="string">"dodge"</span>)+ <span class="comment"># stat값을 identity로 지정하면 y 데이터를 높이로 사용</span></span><br><span class="line">  <span class="comment"># position을 default로 놔두면 값이 쌓아지고, dodge를 할당하면 병렬적으로 출력됨</span></span><br><span class="line">  theme_bw() <span class="comment"># data가 더 잘보이기 위한 테마</span></span><br></pre></td></tr></table></figure><p><strong>결과</strong><br><img src="https://user-images.githubusercontent.com/25416425/54421682-4ad5eb80-4750-11e9-9866-e0b3094a2b87.png" width="450"></p><p>먼저 <code>ggplot()</code>에서 인풋 데이터를 지정하고, aes 안에 x,y축 데이터를 지정하였는데 그다음에 <code>fill = group</code>이라는 옵션을 사용하였다.</p><p>대단히 자주 쓰이는 표현으로, <u>group에 따라 면의 컬러를 다르게 칠하라는 뜻</u>이다.</p><p><code>geom_bar()</code>은 본 패키지에서 barplot을 그리게 하는 함수로, 가장 자주 쓰이는 옵션이 바로 <code>stat=</code> 과 <code>position=</code>이다.</p><p><strong>stat</strong>은 statistic의 약자로, 바 그래프의 형태에 대해서 지정하는 옵션이다. <code>stat=&#39;identity&#39;</code>라고 쓰게 되면, y축 데이터를 높이로 하는 바 그래프를 그리라는 뜻이다.</p><p><strong>position</strong>은 막대의 위치를 의미하며, <code>position=&#39;dodge&#39;</code>는 여러 데이터를 독립적인 바 그래프로 나란히 표현할 때 사용한다. 이 옵션을 표기하지 않으면 데이터가 하나의 막대로 표시된다. </p><p><img src="https://user-images.githubusercontent.com/25416425/54422623-401c5600-4752-11e9-9ba1-526d71c32915.png" width="450"></p><center> position=dodge 를 표기하지 않았을 때 </center><h3 id="데이터셋-USPersonalExpenditure으로-Barplot-그리기"><a href="#데이터셋-USPersonalExpenditure으로-Barplot-그리기" class="headerlink" title="데이터셋 USPersonalExpenditure으로 Barplot 그리기"></a>데이터셋 USPersonalExpenditure으로 Barplot 그리기</h3><p>먼저 <strong>USPersonalExpenditure</strong>가 어떤 형태인지 알아보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/54423376-e74dbd00-4753-11e9-9c56-3c657d5f3e6a.png" width="500"></p><p>Matrix의 형태로, 행에는 지출 항목, 그리고 열에는 년도가 기록되어 있다. 이들이 만나는 곳에는 실제 지출의 크기가 표기되어 있다.</p><p>이제 이 데이터를 최대한 잘 파악하기 위해, 년도별로 지출의 크기를 각 항목으로 나누어 바 그래프를 그려 보자.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data(USPersonalExpenditure) <span class="comment"># Global Environment에 데이터 추가</span></span><br><span class="line">UPE &lt;- USPersonalExpenditure <span class="comment"># 너무 길어서 간략화</span></span><br><span class="line">str(UPE) <span class="comment">#데이터의 form확인 : num</span></span><br><span class="line">barplot(UPE, beside = <span class="literal">T</span>, col = <span class="number">2</span>:<span class="number">6</span>, xlab = <span class="string">"Year"</span>, ylab = <span class="string">"Expenditure ($)"</span>, main = <span class="string">"United States Personal Expenditures"</span>)</span><br><span class="line">legend(<span class="string">'topleft'</span>, legend = row.names(UPE), fill = <span class="number">2</span>:<span class="number">6</span>, cex = <span class="number">0.7</span>) <span class="comment"># legend의 이름을 data의 row.name으로</span></span><br></pre></td></tr></table></figure><p><strong>결과</strong><br><img src="https://user-images.githubusercontent.com/25416425/54423687-968a9400-4754-11e9-9f62-b28064818b10.png" width="450"></p><p>원하는 대로 깔끔하게 data visualization 에 성공했다!</p><p>앞선 <strong>sleep</strong>와 마찬가지로 데이터를 병렬적으로 표기하기 위해 <code>beside=T</code> 옵션을 넣었고, 각각의 막대 컬러를 2번 ~ 6번 색상으로 다르게 지정했다.</p><p>그리고 <code>legend()</code>를 사용하여 따로 범례를 추가하였는데, 범례 이름은 <strong>UPE</strong>의 행 이름으로 지정했고, 색상은 <code>fill=</code>을 사용하여 앞서 지정하였던 막대 컬러 2~6번과 동일하게 하였다.</p><p>또한 <code>cex=</code>가 등장하였는데, 이는 글자 크기를 조절하는 데에 사용하며 비율로 결정되므로 적절히 조절해서 숫자를 넣으면 된다. 자주 쓰이는 옵션이니 기억해 두자.</p><p>이어지는 포스트에서는 barplot 외에 자주 쓰이는 histogram 및 boxplot 에 대해 다루어 볼 예정이다.</p><blockquote><p><em>본 포스트는 KAIST 전산학부 대학원 과정에서 수강하고 있는 Big Data Analytics using R (CS564)을 실습하며 작성하였음을 밝힙니다.</em></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;지난 포스트에서는 기본적인 plot을 그리는 방법과, 가장 널리 쓰이는 패키지인 ggplot2을 간단히 다루어 보았다.&lt;/p&gt;
&lt;p&gt;이번 포스트에서는 내장 데이터셋을 Barplot (막대 그래프) 으로 표현해 볼 것이다.&lt;/p&gt;
&lt;p&gt;library는 마찬가지로 &lt;code&gt;gglpot2&lt;/code&gt; &lt;code&gt;plotrix&lt;/code&gt; &lt;code&gt;boot&lt;/code&gt; &lt;code&gt;scatterplot3d&lt;/code&gt; &lt;code&gt;lattice&lt;/code&gt; &lt;code&gt;MASS&lt;/code&gt; 을 기본으로 한다.&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="R programming" scheme="https://jeongwookie.github.io/categories/Programming/R-programming/"/>
    
    
      <category term="KAIST" scheme="https://jeongwookie.github.io/tags/KAIST/"/>
    
      <category term="R programming" scheme="https://jeongwookie.github.io/tags/R-programming/"/>
    
      <category term="Class" scheme="https://jeongwookie.github.io/tags/Class/"/>
    
  </entry>
  
  <entry>
    <title>R을 사용한 데이터 시각화 - 1. ggplot2 기본</title>
    <link href="https://jeongwookie.github.io/2019/03/06/190306-data-visualization-using-R-1/"/>
    <id>https://jeongwookie.github.io/2019/03/06/190306-data-visualization-using-R-1/</id>
    <published>2019-03-06T12:54:08.000Z</published>
    <updated>2019-05-31T14:13:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>R은 데이터 시각화에 강력한 라이브러리들을 여럿 가지고 있어서 매우 유용하다.</p><p>이번 포스트에서 사용할 라이브러리는 아래와 같다.</p><p>library : <code>gglpot2</code> <code>plotrix</code> <code>boot</code> <code>scatterplot3d</code> <code>lattice</code> <code>MASS</code></p><a id="more"></a><p>먼저, 내장 데이터셋인 <strong>faithful</strong> 을 기본 함수 <code>plot()</code>을 통해 그려보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/54251976-80ca7280-458c-11e9-99f8-fe3fbbb703b6.png" alt="faithful의 개략적 형태"></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">head(faithful)</span><br><span class="line">fa &lt;- faithful[order(faithful$waiting),] </span><br><span class="line"><span class="comment"># waiting을 오름차순으로 정렬한 값을 fa에 저장</span></span><br><span class="line">head(fa)</span><br><span class="line">x &lt;- fa[,<span class="number">2</span>]; y &lt;- fa[,<span class="number">1</span>] </span><br><span class="line"><span class="comment"># waiting을 x에, eruptions을 y에 저장</span></span><br><span class="line">plot(x,y,type=<span class="string">"l"</span>, col=<span class="number">4</span>, xlab = <span class="string">"Waiting Time"</span>, ylab = <span class="string">"Eruption Time"</span>, main = <span class="string">"Old Faithful Eruptions"</span>)</span><br><span class="line">points(x,y, pch=<span class="number">20</span>, col=<span class="number">3</span>) </span><br><span class="line"><span class="comment"># pch는 점의 생김새를 의미, 20은 작은 점</span></span><br></pre></td></tr></table></figure><p><img src="https://user-images.githubusercontent.com/25416425/54250526-76f24080-4587-11e9-8024-211abb8730cf.png" width="450"></p><p>fa는 <em>waiting</em> 열을 <code>order()</code>을 통해 정렬한 <strong>faithful</strong> 데이터를 기록한 것이다.</p><p>즉, fa[,1] = fa$eruptions 이고, fa[,2] = fa$waiting 이다.</p><p><code>plot()</code>은 type을 지정할 수 있으며, 여기서는 line을 나타내었다.</p><p><code>point()</code>는 말 그대로 점을 찍는 함수이며, 여기서 점의 생김새를 결정하는 <strong>pch=</strong> 를 한번 보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/54250863-bbcaa700-4588-11e9-9085-411761ab6e28.png" width="300"></p><p>여러가지 타입이 있다는 것만 알면 된다. 중요한 것은 <strong>pch=</strong> 가 점의 타입을 결정하는 요소이며 자주 쓰이는 것이 <strong>pch = 20(점), 21, 22, 23, 24, 25</strong> 라는 사실이다.</p><p>이번에는 동일한 데이터를 <code>ggplot2</code> 패키지를 통해서 그려 보자.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">ggplot(fa, aes(x,y))+</span><br><span class="line">  geom_point(col=<span class="number">3</span>) + geom_line(col=<span class="number">4</span>) + <span class="comment"># 점을 찍을 때 geom_point를 사용</span></span><br><span class="line">  xlab(<span class="string">'Waiting time'</span>) + ylab(<span class="string">'Eruption time'</span>)+</span><br><span class="line">  ggtitle(<span class="string">'Old Faithful Eruption'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://user-images.githubusercontent.com/25416425/54251159-bc177200-4589-11e9-8d10-8a0cbbc34cb9.png" width="450"></p><p>역시 <code>ggplot2</code> 답게 그래프 디자인이 조금 더 깔끔해 보인다.</p><p><code>ggplot()</code>함수는 어떤 데이터로 어느 축에 할당할 것인지 정한다. 이 함수만 출력하면 <u>아무것도 plot되지 않음에 주의하자.</u> 실제로 점을 찍고 선을 긋는 함수는 뒤에 있다.</p><p>자주 쓰이는 형태는 <code>ggplot(data, aes(x = , y = , fill = ))</code>으로, fill값에 대해서는 뒤에 다시 설명하겠다.</p><p><code>geom_point()</code>는 말그대로 점을 찍는 함수이다. 여기에도 여러가지 디자인 요소들이 있지만 자주 쓰이는 것은 <strong>col=</strong> 으로, 점의 색상을 결정한다.</p><p>이외에도 점의 모양을 결정하는 <strong>shape=</strong>, 점의 색상을 요소(factor)에 따라 변화하게 하는 <strong>aes(fill=)</strong>, 점의 크기를 결정하는 <strong>size=</strong> 등이 쓰이는데 자세한 것은 <a href="https://ggplot2.tidyverse.org/reference/geom_point.html" rel="external nofollow noopener noreferrer" target="_blank">여기</a>를 참고하자.</p><p><code>geom_line()</code>은 선을 긋는 함수이다. 마찬가지로 <strong>col=4</strong>라고 적음으로써 4번 색상인 파랑을 출력하고 있다. 자세한 디자인 요소 변경은 <a href="https://m.blog.naver.com/coder1252/221031694057" rel="external nofollow noopener noreferrer" target="_blank">여기</a>를 참고하자.</p><p>이번에는 또 다른 내장 데이터인 <strong>diamonds</strong> 를 <code>ggplot2</code>로 그려 보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/54252070-c6873b00-458c-11e9-934b-2f1d1fed3da7.png" alt="diamonds 데이터의 형태"></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">head(diamonds)</span><br><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">ggplot(diamonds, aes(x=carat, y=price)) +</span><br><span class="line">  geom_point(aes(col=color)) + <span class="comment"># point의 컬러(col)를 color라는 변수값에 따라 변하도록 설정</span></span><br><span class="line">  facet_grid(color ~ .) <span class="comment"># ~을 사용한 format</span></span><br></pre></td></tr></table></figure><p><code>ggplot()</code>에서 x축 데이터를 carat열, 그리고 y축 데이터를 price열 로 설정하였다.</p><p>그리고 <code>geom_point()</code>로 점을 찍는데, 그 점들의 색상을 <u>color열의 요소에 따라 달라지도록 지정하였다.</u></p><p>마지막으로, 새로운 함수가 하나 나타났는데 바로 <code>facet_grid()</code>.</p><p>주로 <strong>집단(group) 간의 효과적인 비교를 위해 면을 분할하고 싶을 때</strong> 사용한다.</p><p>또한, 많은 경우 <code>x ~ y</code>와 같은 형태로 작성하는데, 이는 표현 그대로 좌측은 x (input), 우측은 y (output)을 의미하는 간략한 함수 형태이다.</p><p>위의 <code>facet_grid(color ~ .)</code>은 color에 따라 나머지 모든 데이터를 그룹화 하여 면을 분할 해서 표현하라는 의미로 해석된다. 참고는 <a href="https://rfriend.tistory.com/85" rel="external nofollow noopener noreferrer" target="_blank">여기</a></p><p>만약 <code>facet_grid(. ~ color)</code>로 쓴다면, 쉽게 말해서 x와 y가 반전되어 출력된다. (세로)</p><div style="width:50%; height:350px; float:left;"><br><img src="https://user-images.githubusercontent.com/25416425/54253074-ba9d7800-4590-11e9-8bf7-eb477fea57fa.png" width="350"><br></div><div style="width:50%; height:350px; float:right;"><br><img src="https://user-images.githubusercontent.com/25416425/54253032-95106e80-4590-11e9-8cd2-03386699c406.png" width="350"><br></div><p>왼쪽은 <strong>facet_grid(color ~ .)</strong> 이고, 오른쪽은 <strong>facet_grid(. ~ color)</strong> 의 결과물이다. 위의 <code>geom_point()</code> 세팅에 따라, 점의 색상이 <strong>diamonds$color</strong>의 값에 따라 달라짐을 알 수 있다.</p><p>이어지는 포스트에서는 기본 plot외의 bar, histogram 등의 형태를 다루어 볼 것이다.</p><blockquote><p><em>본 포스트는 KAIST 전산학부 대학원 과정에서 수강하고 있는 Big Data Analytics using R (CS564)을 실습하며 작성하였음을 밝힙니다.</em></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;R은 데이터 시각화에 강력한 라이브러리들을 여럿 가지고 있어서 매우 유용하다.&lt;/p&gt;
&lt;p&gt;이번 포스트에서 사용할 라이브러리는 아래와 같다.&lt;/p&gt;
&lt;p&gt;library : &lt;code&gt;gglpot2&lt;/code&gt; &lt;code&gt;plotrix&lt;/code&gt; &lt;code&gt;boot&lt;/code&gt; &lt;code&gt;scatterplot3d&lt;/code&gt; &lt;code&gt;lattice&lt;/code&gt; &lt;code&gt;MASS&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Programming" scheme="https://jeongwookie.github.io/categories/Programming/"/>
    
      <category term="R programming" scheme="https://jeongwookie.github.io/categories/Programming/R-programming/"/>
    
    
      <category term="KAIST" scheme="https://jeongwookie.github.io/tags/KAIST/"/>
    
      <category term="R programming" scheme="https://jeongwookie.github.io/tags/R-programming/"/>
    
      <category term="Class" scheme="https://jeongwookie.github.io/tags/Class/"/>
    
  </entry>
  
  <entry>
    <title>좋은 서비스 디자인하기</title>
    <link href="https://jeongwookie.github.io/2019/02/08/190208-goal-directed-design/"/>
    <id>https://jeongwookie.github.io/2019/02/08/190208-goal-directed-design/</id>
    <published>2019-02-08T09:17:43.000Z</published>
    <updated>2019-03-25T03:35:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="목표-지향-디자인-방법론"><a href="#목표-지향-디자인-방법론" class="headerlink" title="목표 지향 디자인 방법론"></a>목표 지향 디자인 방법론</h2><p>좋은 서비스, 소프트웨어를 만들기 위해서는 어떻게 해아 할까?</p><p>이는 비즈니스 초기부터 서비스 출시, 그리고 그 이후의 고객과의 인터렉션을 모두 포함하고 있는 궁극적인 질문일 것이다.</p><p>창업하는 이들 모두가 궁금한 이 질문에 대해 멘탈 모델을 기반으로 하는 방법론을 소개하고자 한다.<br>(멘탈모델이 무엇인가는 <a href="https://jeongwookie.github.io/2018/11/06/181106-customer-interview-mental-model/">멘탈 모델을 적용한 고객 인터뷰</a>를 참고하자)</p><p>바로 <strong>목표 지향 디자인 방법론 (Goal Directed Design)</strong> 이다. 이것은 사용자의 목표를 이해하기 위한 하나의 프로세스 라고 이해하면 되는데, 여기서 사용자의 목표란 제품을 활용해서 자신이 성취하고자 하는 바가 무엇인지 또는 경쟁사 상품 대신 왜 이걸 선택하는지 와 같은 것이다.</p><a id="more"></a><p><img src="https://user-images.githubusercontent.com/25416425/52469737-a3edb500-2bcf-11e9-857e-f279f50bdecf.png" width="500"></p><p>많은 기존의 서비스들을 생각해보자. 혹시 사용하다가 <strong>스스로 멍청하다고 느껴본 적이 없는가?</strong> 서비스가 사용하기 너무 복잡하거나, 사용자가 실수할 법한 여지를 남겨두어 여지없이 거기에 걸렸다던가.. 나는 한글(.hwp)과 같은 문서 편집기를 사용하다가 키보드를 내려친 적이 몇번 있다. </p><p>우리는 사용자를 가르치려 드는 서비스를 만들려고 하는 것이 아니다.</p><p>이런 이상적인 서비스 개발에는 <strong>디자이너</strong>의 역할이 매우 중요하다. 또한 이들의 개입 순서가 어디인가에 따라 결과물이 크게 달라지는데, 윗 그림의 아래와 같은 방법을 따라가 보자.</p><p><strong>목표 지향 디자인</strong>은 사용자의 목표와 니즈를 만족시킬 뿐 아니라, 사업 측면의 요구 사항과 기술 구현 가능성을 효과적으로 조화시킬 수 있는 접근 방식이다.</p><p>이 방식은 크게 리서치, 사용자 모델, 요구사항 도출, 디자인 설계, 수정, 관리 등 총 6가지 스텝으로 구성되어 있다. 이 중에서 비즈니스를 설계하는 최초부터 중요한 insight를 제공할 수 있는 첫번째 스텝을 본 포스트에서는 다루고자 한다.</p><p>디자인에 필수적인 요소를 깊이 이해하려면 반드시 <strong>정성적 리서치</strong>를 도입해야 한다. <code>무엇을</code>, <code>어떻게</code>, <code>왜</code> 해야 하는지 명확한 답을 얻기 위한 것이 우리의 목표이다. 이때, 인터뷰 대상을 선정하거나 임원진 혹은 투자자에게 제품 구축을 납득시키기 위해서 기존의 정량적 리서치 (마케팅 리서치) 자료를 참고할 수 있다.</p><p>정성적 리서치는 훌륭한 디자인을 이끌어내는 지름길이다. 그렇다면 구체적으로 어떻게 정성적 리서치를 진행해야 할까?</p><p><img src="https://user-images.githubusercontent.com/25416425/52470582-e3b59c00-2bd1-11e9-913d-e24f506063bc.png" width="300"></p><p>아래의 구체적 프로세스를 설명하기 전, 실행하는 주체는 <strong>실제 서비스를 설계하는 사람</strong> 이라고 정한다.</p><h3 id="킥오프-미팅"><a href="#킥오프-미팅" class="headerlink" title="킥오프 미팅"></a>킥오프 미팅</h3><p>킥오프 미팅은 초기 창업자들 서로에게 질문하는 것으로도 유용하다. 앞으로의 제품, 사용자, 디자인 문제에 대해 어떻게 생각하는지에 대해 서로간 통찰이 생긴다. 핵심 질문은 아래와 같다.</p><ul><li>제품은 무엇인가? (솔루션)</li><li>누가 사용하며, 사용할 것인가? (고객)</li><li>사용자가 무엇을 가장 필요로 하는가? (핵심 가치 및 기능)</li><li>어떤 고객과 사용자가 사업에 가장 중요한가? (핵심 고객 및 키 파트너)</li><li>팀은 프로젝트 진행 중 어떤 과제에 직면하는가? (운영)</li><li>어디를 가장 큰 경쟁사로 보는가? 그 이유는 무엇인가? (경쟁사 분석)</li><li>제품의 사업 및 기술 영역에 친숙해지려면 어떤 분야를 공부해야 하는가?</li></ul><p>위 질문을 찬찬히 살펴보면, 창업자 스스로가 어떤 부분을 초기에 답변하려 아이템을 설계해야 하는지 알 수 있다.</p><p>또한, 초기 스타트업에 합류하는 팀원의 경우에도 같은 질문을 기존 경영진들에게 질문할 수 있을 것이다.</p><p>위의 질문들이 <strong>가장 첫번째 스텝</strong>임에 유의하라. 이는 창업자 개인의 생각일 수 있고, 사전에 진행한 리서치의 결과물일 수 있다. 답변을 충분히 듣고 (생각하고), 실제 서비스를 설계하는 것은 <u>정성적 리서치의 모든 프로세스를 진행한 후에 해야 한다.</u></p><h3 id="문헌-조사"><a href="#문헌-조사" class="headerlink" title="문헌 조사"></a>문헌 조사</h3><p>킥오프 미팅을 진행함과 동시에 기존 문서의 내용을 잘 파악해야 한다. 창업자가 설계한 제품 (서비스)과 관련된 영역에 관한 문헌을 조사한다. 여행 관련 플렛폼에 대한 아이디어라면 여행 트렌드, 관련 기사, 플렛폼에 관한 이해 등이 여기에 해당한다.</p><p>이는 인터뷰를 진행하기 전, 질문할 내용을 준비할 때에 큰 도움이 된다. 또한 사용자 조사에서 얻은 자료를 분석할 때에도 중요한 기반이 된다.</p><h3 id="기존-제품-및-경쟁사-분석"><a href="#기존-제품-및-경쟁사-분석" class="headerlink" title="기존 제품 및 경쟁사 분석"></a>기존 제품 및 경쟁사 분석</h3><p>기존의 제품과 주요 경쟁사 제품 분석은 현재 사용자에게 제공되고 있는 기능을 분명히 이해할 수 있도록 돕는다. 다만, 너무 구체적인 기능에 대한 조사는 지양하는 것이 좋아 보인다. 자칫하다가는 기존의 프레임에 생각이 갇힐 위혐이 있다. 가볍게 경쟁사의 고객 전달 가치가 무엇인지, 그리고 마켓 포지셔닝을 어떻게 하고 있는지 정도가 적절하다. (투자 및 매출 현황도 같이 살피면 유용할 것이다)</p><h3 id="임원진-인터뷰"><a href="#임원진-인터뷰" class="headerlink" title="임원진 인터뷰"></a>임원진 인터뷰</h3><p>임원진 인터뷰는 제품을 디자인하기 전 사업 목표를 분명히 이해하는 데에 큰 도움이 된다. 초기 스타트업은 창업자의 초기 비전을 들어보는 것이다. 임원진 인터뷰에서 얻을 수 있는 중요한 정보는 다음과 같다.</p><ul><li>초기 비전과 기대: 팀원들의 완성 제품에 대한 기대치가 제각각인 경우가 많다.</li><li>비용과 일정: 얼마나 많은 자원을 투자할 수 있는지 명확히 하는 것은 매우 중요하다.</li><li>기술적 제약과 가능성: 현재 구현할 수 있는 범위가 어느 정도인지 파악하여야 한다.</li><li>사업 전략: 성취하려는 사업 목표가 무엇인지 알면 차후 중요한 의사결정을 내릴때 큰 도움이 된다.</li><li>임원진이 생각하는 사용자: 팀원마다 주요 예상 사용자가 다를 수 있다.</li></ul><h3 id="영역-전문가-인터뷰"><a href="#영역-전문가-인터뷰" class="headerlink" title="영역 전문가 인터뷰"></a>영역 전문가 인터뷰</h3><p>디자인 프로젝트 초기에 영역 전문가에게 자문을 구하는 것은 때론 매우 중요할 수 있다. 영역 전문가란 디자인하려는 제품의 분야를 매우 잘 아는 사람을 의미한다.</p><p>예를 들어 의료 분야의 스타트업은 보통 설계하려는 아이템이 복잡하고, 기술집약적이며, 법적 고려를 해야하는 경우가 많은데, 디자인을 시작하기 전 자문을 구하는 것은 필수적이다.</p><p>다만, 영역 전문가들을 인터뷰할 때에는 아래와 같은 주의가 필요하다.</p><ul><li><strong>영역 전문가는 고급 사용자임에 유의하여야 한다.</strong> 이들은 기존 제품에 너무 익숙하여 새로운 디자인을 제시하기는 어렵다. 또한 같은 기능이라도 전문가만 사용할 수 있는 기능을 선호하는 경향이 있다.</li><li><strong>영역 전문가의 의견은 중요하지만, 그들이 디자이너는 아니다.</strong> 실제로 그들이 내놓은 아이디어가 도움이 될수도 있고, 아닐 수도 있다. <em> “이 가능이 사용자에게 어떤 도움이 될까?” </em>를 항상 염두하자.</li><li><strong>복잡하고 전문적인 분야일수록 영역 전문가가 필수적이다.</strong> 이들의 지식을 활용하면 복잡한 사용자 리서치를 보다 쉽게 설계할 수 있다.</li><li><strong>지속적으로 영역 전문가를 만나서 자문을 구해야 한다.</strong> 이들이 꼭 필요한 제품을 만든다면, 디자인을 진행하는 동안 여러 번 만나서 자문을 구하여야 한다.</li></ul><h3 id="구매자-인터뷰"><a href="#구매자-인터뷰" class="headerlink" title="구매자 인터뷰"></a>구매자 인터뷰</h3><p>구매자는 사용자와 다른 개념이다. <strong>구매자</strong>란 제품의 구매 의사결정을 내라는 사람을 말한다. 일반적인 소비 제품의 경우에는 구매자와 사용자가 동일하지만, 대부분의 경우 다르다. </p><p>어린이용 제품의 사용자는 어린이지만 구매자는 부모이며, 기술 제품의 경우에도 사용자는 직원이지만 구매자는 이사급 임원진 혹은 IT관리자이다.</p><p>성공적인 제품을 디자인하려면 반드시 구매자의 목표를 이해하고, 이들을 만족시켜야 매출로 연결된다.</p><ul><li>제품을 구입하는 목표</li><li>현재 사용중인 제품에서 겪는 어려움</li><li>유사한 제품 구입 시 의사결정 과정</li><li>제품의 설치, 유지, 관리에서 구매자의 역할</li><li>사업 영역의 특징과 이슈</li></ul><p>때때로 이들을 인터뷰하는 도중 더 훌륭한 아이디어를 발견할 수 있다. 구매자가 왜 그런 의견을 냈는지 분석하는 과정이 그래서 매우 중요하다. </p><h3 id="사용자-인터뷰"><a href="#사용자-인터뷰" class="headerlink" title="사용자 인터뷰"></a>사용자 인터뷰</h3><p>디자인의 중심은 항상 사용자이다. 이들이 바로 실제로 제품을 가장 활발하게 사용하기 때문이다. 이들을 이해하려면 실제로 사용자가 제품을 이용하는 공간에서 조사를 진행햐난 것이 바람직하다.</p><p>그렇다면 구체적으로 어떻게 사용자를 인터뷰 하는 것이 좋을까? 가장 효과적인 방법은 <code>사용자 관찰</code>과 <code>일대일 인터뷰</code>를 병행하는 것이다. 이러한 방법론을 <strong>에스노그라피 인터뷰 (Ethnographic Interview)</strong> 라고도 한다.</p><p>에스노그래피는 원래 인류학에서 주로 사용하는 용어인데, 인류의 문화에 대한 체계적이고 집중적인 연구를 의미한다. 인류학자들은 연구를 진행할 때 해당 문화를 직접 체험하면서 수녕간 자료를 기록하고 분석한다. 이러한 기본 컨셉을 소규모 리서치에 적용한 것이다.</p><p>사용자 조사에서는 사용자와 제품이 어떻게 인터렉션 하는지 사용자의 <code>행동 패턴</code>과 <code>태도</code>를 이해하는 게 목표이다. 구체적으로 에스노그라피 인터뷰를 진행하는 데에 염두해야 하는 내용은 아래와 같다.</p><ul><li>직접 사용자를 찾아가 인터뷰하라.</li><li>정해진 질문지를 피하라.</li><li>전문가가 아니라 초심자의 역할을 가정하라.</li><li>끝이 있는 질문과 없는 질문을 적절히 하여 논의를 유도하라.</li><li>사용자의 목표를 먼저, 과업은 나중에 파악하라.</li><li>사용자는 디자이너가 아니다.</li><li>기술에 대한 토론은 하지 않는다.</li><li>사용자의 이야기를 들어라.</li><li>사용자의 업무와 작업물을 관찰하라.</li><li>유도 질문을 피하라.</li></ul><p>인터뷰를 진행하면서, 사용자가 제품에 대한 디자인 의견을 제시하는 경우가 있다. 이때, 의견 자체에 초점을 맞추기 보다는 <strong>왜 그렇게 생각하는지 뒤에 깔린 목표를 파악하여야 한다.</strong> 아래와 같이 질문해 보자.</p><p><em>“그렇게 하면 어떤 문제가 해결될까요?</em></p><p><em>“왜 그렇게 바꾸고 싶은가요?</em></p><p>디자인 프로세스의 초기 단계에 에스노그라피 인터뷰를 진행하면 매우 값진 결과를 얻을 수 있다.</p><p>사용자를 깊이 이해하고, 사용자의 니즈와 목표를 파악하여 좋은 서비스를 만드는 데에 도전해 보자.</p><p><img src="https://user-images.githubusercontent.com/25416425/52473359-ed8ecd80-2bd8-11e9-86e0-7206156e869a.jpg" alt></p><blockquote><p><em>위 포스트는 About Face, 인터렉션 디자인의 본질을 읽고 작성하였습니다.</em></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;목표-지향-디자인-방법론&quot;&gt;&lt;a href=&quot;#목표-지향-디자인-방법론&quot; class=&quot;headerlink&quot; title=&quot;목표 지향 디자인 방법론&quot;&gt;&lt;/a&gt;목표 지향 디자인 방법론&lt;/h2&gt;&lt;p&gt;좋은 서비스, 소프트웨어를 만들기 위해서는 어떻게 해아 할까?&lt;/p&gt;
&lt;p&gt;이는 비즈니스 초기부터 서비스 출시, 그리고 그 이후의 고객과의 인터렉션을 모두 포함하고 있는 궁극적인 질문일 것이다.&lt;/p&gt;
&lt;p&gt;창업하는 이들 모두가 궁금한 이 질문에 대해 멘탈 모델을 기반으로 하는 방법론을 소개하고자 한다.&lt;br&gt;(멘탈모델이 무엇인가는 &lt;a href=&quot;https://jeongwookie.github.io/2018/11/06/181106-customer-interview-mental-model/&quot;&gt;멘탈 모델을 적용한 고객 인터뷰&lt;/a&gt;를 참고하자)&lt;/p&gt;
&lt;p&gt;바로 &lt;strong&gt;목표 지향 디자인 방법론 (Goal Directed Design)&lt;/strong&gt; 이다. 이것은 사용자의 목표를 이해하기 위한 하나의 프로세스 라고 이해하면 되는데, 여기서 사용자의 목표란 제품을 활용해서 자신이 성취하고자 하는 바가 무엇인지 또는 경쟁사 상품 대신 왜 이걸 선택하는지 와 같은 것이다.&lt;/p&gt;
    
    </summary>
    
      <category term="Startup Story" scheme="https://jeongwookie.github.io/categories/Startup-Story/"/>
    
    
      <category term="Startup" scheme="https://jeongwookie.github.io/tags/Startup/"/>
    
      <category term="Business" scheme="https://jeongwookie.github.io/tags/Business/"/>
    
      <category term="Service Design" scheme="https://jeongwookie.github.io/tags/Service-Design/"/>
    
  </entry>
  
  <entry>
    <title>문득 떠오른 취미</title>
    <link href="https://jeongwookie.github.io/2019/02/03/190203-find-a-new-hobby/"/>
    <id>https://jeongwookie.github.io/2019/02/03/190203-find-a-new-hobby/</id>
    <published>2019-02-02T17:38:28.000Z</published>
    <updated>2019-03-24T10:53:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>&lt;문득 든 배우고 싶은 취미에 대한 일기&gt;</p><p>나는 음악을 많이 듣는 편은 아니다.</p><p>기차를 타고, 버스를 타고 갈 때에도 잠을 청하기 위해 가끔 음악을 듣곤 하지만.. 아무것도 귀에 꼽지 않고 갈때가 훨씬 많다.</p><p>하지만 가끔 좋은 뉴에이지 음악을 들을 때, 음색이 매력적인 기타 소리를 들을 때 나도 저렇게 연주하고 싶다는 생각은 종종 든다.</p><a id="more"></a><p>어머니께서 내가 초등학생이 되기 전까지 성악을 하셨다.</p><p>연주회에 가서 어머니를 기다렸던 기억이 조금씩 난다.</p><p>종종 방학때는 같이 유명한 피아니스트의 연주회에 갔었다.</p><p>물론 어릴때는 그때마다 저녁에 양식을 먹을 수 있어서 따라갔던 것이 크지만.. 좋은 음악을 들을 때 마다 그때의 기억이 나는 것을 보면 좋았던 모양이다.</p><p>초등학교때 많은 또래들이 그랬듯이 학원을 다니면서 피아노와 플룻을 배웠다. </p><p>정말 어릴때는 집에서 친구들과 함께 어머니께 노래를 배우기도 했었다.</p><p>근데.. 어른이 된 지금은 하나도!! 내 손가락에 남아있지 않다..ㅠㅠ</p><p>대학교 1학년때는 일렉 기타를 배워보려고 수업을 신청했었는데, 조금 만져보다가 도망갔다 ㅋㅋㅋ</p><p>아버지께서 강력히 추천하셨었지만.. 나한테는 맞지 않았던 모양이다. 손이 남들보다 작고 악력이 없어서 내 맘대로 음이 울리질 않으니 흥미가 떨어졌다.<br>(락과 같은 영역의 노래를 하나도 안듣기도 했다)</p><p>지금은 피아노를 기회가 되면 한번 더 배워보고 싶다.</p><p>내가 치고싶은 곡들만 연습해서 멋지게 연주하고 싶다.</p><p>보컬 수업도 한번 듣고 싶다. 목소리는 솔직히 자신있는데.. 노래방을 거의 안가다 시피 해서 어떻게 부르는지 모른다 ㅋㅋㅋ</p><p>이번 해에 할 수 있을까? 도전해 보자..!!</p><p><img src="https://user-images.githubusercontent.com/25416425/52431372-6fd0b080-2b4b-11e9-9bc8-ff02b6160273.jpg" width="550"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;lt;문득 든 배우고 싶은 취미에 대한 일기&amp;gt;&lt;/p&gt;
&lt;p&gt;나는 음악을 많이 듣는 편은 아니다.&lt;/p&gt;
&lt;p&gt;기차를 타고, 버스를 타고 갈 때에도 잠을 청하기 위해 가끔 음악을 듣곤 하지만.. 아무것도 귀에 꼽지 않고 갈때가 훨씬 많다.&lt;/p&gt;
&lt;p&gt;하지만 가끔 좋은 뉴에이지 음악을 들을 때, 음색이 매력적인 기타 소리를 들을 때 나도 저렇게 연주하고 싶다는 생각은 종종 든다.&lt;/p&gt;
    
    </summary>
    
      <category term="Diary" scheme="https://jeongwookie.github.io/categories/Diary/"/>
    
    
      <category term="Daily life" scheme="https://jeongwookie.github.io/tags/Daily-life/"/>
    
  </entry>
  
  <entry>
    <title>IBM Watson 교육 수강</title>
    <link href="https://jeongwookie.github.io/2019/01/22/190122-AI-development-training-course-in-kaist/"/>
    <id>https://jeongwookie.github.io/2019/01/22/190122-AI-development-training-course-in-kaist/</id>
    <published>2019-01-22T09:22:49.000Z</published>
    <updated>2019-05-21T05:33:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>12월 말 정도부터 거의 매일 학교에서 저녁에 열렸던 AI 관련 특별 강좌를 이수했다.</p><p>원래는 TensorFlow 기본을 수강하려 했는데 수강 인원이 꽉 차버려서..ㅠㅠ</p><p>예전에 왓슨을 이용해서 간단한 챗봇은 구성해본 경험이 있어서, 왓슨의 다양한 기능들을 사용해 보고 프로젝트에 써먹을 수 있는 것이 있을까 파악하는 것도 재미있어 보였다.</p><p>이 수업에서는 기본적으로 <code>IBM Watson</code>의 다양한 기능들을 사용해보고, 이를 비즈니스에 실제로 적용할 때에 구성 방식에 대해서 배웠다.</p><a id="more"></a><p>제일 처음은 Watson Assistant를 사용한 피자 배달 챗봇 만들기부터 ㅋㅋㅋ</p><p>예전에 구성했던 챗봇은 선택지를 미리 정해놓고 케이스만 나누어서 응답하는 가장 기초적인 응답 챗봇이었기 때문에, 이번에는 선택지 없이 계속 케이스를 러닝 시키면서 accuracy가 올라가도록 구성해 보았다.</p><p>이후에는 Watson Discovery 및 Watson Knowledge Studio를 사용해서 기업 내부의 아카이브 된 지식 체계들을 쉽게 검색할 수 있도록 데모를 만들어 보았고.. 여러가지 검색어를 트라이 해보면서 놀았다.</p><p>Watson의 또 하나의 서비스인 Visual Recognition도 대단히 흥미로운 툴이었다. 분류자를 직접 만들어서 내가 가지고 있는 로컬 서버의 이미지 파일들을 정돈하는 데에 수 초가 걸리지 않았다. 결과도 꽤나 정확했던.. 물론 내가 직접 찍은 사진들은 너무 local스러워서 러닝이 더 필요해 보이긴 했지만.</p><p>새로운 툴을 만져 보는 것은 언제나 즐겁다. </p><p><img src="https://user-images.githubusercontent.com/25416425/51669137-250d4f80-2007-11e9-8a51-20661af4b714.png" alt="55시간에 걸친 수업 끝!!"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;12월 말 정도부터 거의 매일 학교에서 저녁에 열렸던 AI 관련 특별 강좌를 이수했다.&lt;/p&gt;
&lt;p&gt;원래는 TensorFlow 기본을 수강하려 했는데 수강 인원이 꽉 차버려서..ㅠㅠ&lt;/p&gt;
&lt;p&gt;예전에 왓슨을 이용해서 간단한 챗봇은 구성해본 경험이 있어서, 왓슨의 다양한 기능들을 사용해 보고 프로젝트에 써먹을 수 있는 것이 있을까 파악하는 것도 재미있어 보였다.&lt;/p&gt;
&lt;p&gt;이 수업에서는 기본적으로 &lt;code&gt;IBM Watson&lt;/code&gt;의 다양한 기능들을 사용해보고, 이를 비즈니스에 실제로 적용할 때에 구성 방식에 대해서 배웠다.&lt;/p&gt;
    
    </summary>
    
      <category term="Diary" scheme="https://jeongwookie.github.io/categories/Diary/"/>
    
    
      <category term="IBM Watson" scheme="https://jeongwookie.github.io/tags/IBM-Watson/"/>
    
      <category term="AI" scheme="https://jeongwookie.github.io/tags/AI/"/>
    
      <category term="Chatbot" scheme="https://jeongwookie.github.io/tags/Chatbot/"/>
    
      <category term="KAIST IT Program" scheme="https://jeongwookie.github.io/tags/KAIST-IT-Program/"/>
    
  </entry>
  
  <entry>
    <title>컴퓨터방 셀프 인테리어, 마지막</title>
    <link href="https://jeongwookie.github.io/2019/01/17/190117-house-remodeling-4/"/>
    <id>https://jeongwookie.github.io/2019/01/17/190117-house-remodeling-4/</id>
    <published>2019-01-17T12:51:53.000Z</published>
    <updated>2019-03-25T03:17:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="처음-시도하는-셀프-인테리어-도전기-4탄-마지막-마무리"><a href="#처음-시도하는-셀프-인테리어-도전기-4탄-마지막-마무리" class="headerlink" title="처음 시도하는 셀프 인테리어 도전기 4탄, 마지막 마무리"></a>처음 시도하는 셀프 인테리어 도전기 4탄, 마지막 마무리</h2><p>셀프 인테리어 마지막 포스트!</p><p>이전 포스트는 아래 링크를 누르면 볼 수 있다.</p><ul><li><a href="https://jeongwookie.github.io/2019/01/05/190105-house-remodeling-1/">처음 시도하는 셀프 인테리어 도전기 1탄, 페인트 칠하기</a> </li><li><a href="https://jeongwookie.github.io/2019/01/09/190109-house-remodeling-2/">처음 시도하는 셀프 인테리어 도전기 2탄, 바닥 깔기</a> </li><li><a href="https://jeongwookie.github.io/2019/01/12/190112-house-remodeling-3/">처음 시도하는 셀프 인테리어 도전기 3탄, 조명 설치하기</a> </li></ul><p>이번에는 마지막에 리모델링에 사용된 비용과 출처를 정리하였으니 관심있는 초보자 분들의 많은 참고가 되었으면 한다.</p><a id="more"></a><p>우리가 리모델링한 작업을 아래에 간단히 정리해 보았다.</p><ul><li><code>방문</code> &amp; <code>굽도리</code> 화이트 톤으로 채색</li><li>블랙 문고리, 화이트 경첩으로 <code>방문</code> 깔맞춤</li><li>남는 페인트로 <code>화장실문</code> 페인팅</li><li><code>방 내부</code> 데코타일 시공 </li><li><code>방 내부</code> 레일 조명 설치</li><li><code>창문</code>에 암막 커튼 설치</li><li><code>방 벽면</code>에 Lack 설치 (예정)</li></ul><p>Lack은 컴퓨터 책상 위 벽면에 설치하려고 이케아에서 주문했는데, 막상 배치를 다 하고 나니 힘들어서 나중에 하기로 결정..ㅋㅋ</p><p>여기는 적지 않았지만, 작은 방은 벽면까지 회색 계열로 풀바른 벽지를 붙였다. 이것도 스토리가 있는데.. 결론은 보기보다 쉽지 않다는 거. 작은 방 해보고 힘들어서 큰 방은 안하기로 결정했다.</p><p>이제 결과를 공개할 시간! 가구는 솔직히 비싸서 신경을 많이 못썼다. 중간중간에 여유 되는대로 채워넣을 계획이다.</p><blockquote><p><strong>리모델링 전,후 방 비교 사진</strong></p></blockquote><div style="width:50%; height:350px; float:left;"><br><img src="https://user-images.githubusercontent.com/25416425/51427411-28858d00-1c3b-11e9-8545-de53d7447d2b.jpeg" width="350"><br></div><div style="width:50%; height:350px; float:right;"><br><img src="https://user-images.githubusercontent.com/25416425/51427443-5ff43980-1c3b-11e9-85c5-31b3b37d3f0a.jpeg" width="350"><br></div><div style="width:50%; height:350px; float:left;"><br><img src="https://user-images.githubusercontent.com/25416425/51427458-9df15d80-1c3b-11e9-8823-4d12e45198fc.jpeg" width="350"><br></div><div style="width:50%; height:350px; float:right;"><br><img src="https://user-images.githubusercontent.com/25416425/51427467-b19cc400-1c3b-11e9-8220-ea5e67201b40.jpeg" width="350"><br></div><p>이렇게 보니 침구류의 퀄리티가 좀..ㅋㅋㅋ 색상이 안맞아서 새로 깔아야겠다.</p><p>여기까지 2주 조금 넘게 걸린 것 같다. 학교에서 교육듣고 와서 밤에 틈틈히 작업한거라 오래 걸리긴 했지만 이사일까지 결국 마무리지었다.</p><p>사진으로 보니까 방이 아니라 무슨 스튜디오 같은데 ㅋㅋㅋㅋ 방송이라도 해야할 것 같군.</p><p>컴퓨터방 인테리어는 뭐니뭐니해도 모니터가 이목을 확 끄는 것 같다. 이걸 위해서 두닷 콰트로에서 제일 큰 책상을 주문했다.</p><p>모니터는 알파스캔 커브드 모델로 가운데 메인 모니터는 144hz, 측면 두개는 75hz까지 가능한 제품이다.</p><div style="width:50%; height:300px; float:left;"><br><img src="https://user-images.githubusercontent.com/25416425/51427537-7a7ae280-1c3c-11e9-9de8-b4c13d6d46be.jpeg" width="350"><br></div><div style="width:50%; height:300px; float:right;"><br><img src="https://user-images.githubusercontent.com/25416425/54880683-a3ad2e80-4e8a-11e9-8228-629bd88db2fe.jpg" width="350"><br></div><p>마지막으로, 리모델링 비용과 가구 구매 비용을 아래에 정리했다. </p><blockquote><p><strong>집 예산 최종 정리</strong></p></blockquote><p><img src="https://user-images.githubusercontent.com/25416425/51427895-515c5100-1c40-11e9-9a0a-f67da6ca9c3c.png" width="750"></p><p>혹시 구체적인 구매처가 궁금하시면 댓글 달아 주세요! 답 달아 드리겠습니다 :)</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;처음-시도하는-셀프-인테리어-도전기-4탄-마지막-마무리&quot;&gt;&lt;a href=&quot;#처음-시도하는-셀프-인테리어-도전기-4탄-마지막-마무리&quot; class=&quot;headerlink&quot; title=&quot;처음 시도하는 셀프 인테리어 도전기 4탄, 마지막 마무리&quot;&gt;&lt;/a&gt;처음 시도하는 셀프 인테리어 도전기 4탄, 마지막 마무리&lt;/h2&gt;&lt;p&gt;셀프 인테리어 마지막 포스트!&lt;/p&gt;
&lt;p&gt;이전 포스트는 아래 링크를 누르면 볼 수 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://jeongwookie.github.io/2019/01/05/190105-house-remodeling-1/&quot;&gt;처음 시도하는 셀프 인테리어 도전기 1탄, 페인트 칠하기&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jeongwookie.github.io/2019/01/09/190109-house-remodeling-2/&quot;&gt;처음 시도하는 셀프 인테리어 도전기 2탄, 바닥 깔기&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jeongwookie.github.io/2019/01/12/190112-house-remodeling-3/&quot;&gt;처음 시도하는 셀프 인테리어 도전기 3탄, 조명 설치하기&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;이번에는 마지막에 리모델링에 사용된 비용과 출처를 정리하였으니 관심있는 초보자 분들의 많은 참고가 되었으면 한다.&lt;/p&gt;
    
    </summary>
    
      <category term="Hobby" scheme="https://jeongwookie.github.io/categories/Hobby/"/>
    
    
      <category term="Daily life" scheme="https://jeongwookie.github.io/tags/Daily-life/"/>
    
      <category term="Self Interior" scheme="https://jeongwookie.github.io/tags/Self-Interior/"/>
    
      <category term="Computer room" scheme="https://jeongwookie.github.io/tags/Computer-room/"/>
    
  </entry>
  
  <entry>
    <title>컴퓨터방 셀프 인테리어, 세번째</title>
    <link href="https://jeongwookie.github.io/2019/01/12/190112-house-remodeling-3/"/>
    <id>https://jeongwookie.github.io/2019/01/12/190112-house-remodeling-3/</id>
    <published>2019-01-11T15:07:03.000Z</published>
    <updated>2019-03-25T03:17:16.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="처음-시도하는-셀프-인테리어-도전기-3탄-조명-설치하기"><a href="#처음-시도하는-셀프-인테리어-도전기-3탄-조명-설치하기" class="headerlink" title="처음 시도하는 셀프 인테리어 도전기 3탄, 조명 설치하기"></a>처음 시도하는 셀프 인테리어 도전기 3탄, 조명 설치하기</h2><p>셀프 인테리어 세 번째 포스트. (1탄 포스트 <a href="https://jeongwookie.github.io/2019/01/05/190105-house-remodeling-1/">링크</a>, 2탄 포스트 <a href="https://jeongwookie.github.io/2019/01/09/190109-house-remodeling-2/">링크</a>)</p><p>지금까지 진행한 사항을 아래에 간단히 정리해 보면,</p><ul><li>방 두개 문 &amp; 굽도리 페인트칠 완료</li><li>방문 문고리 부착 완료</li><li>화장실 문 1/3 완료</li><li>방 두개 데코타일 시공 완료</li></ul><p>이제 남은 작업은 조명 설치와 가구 조립.</p><p>1월 8일은 그전날 바닥 작업의 여파로 쉬었습니다..ㅠㅠ (진짜 완전 힘듬)</p><a id="more"></a><h3 id="조명-설치하기"><a href="#조명-설치하기" class="headerlink" title="조명 설치하기"></a>조명 설치하기</h3><blockquote><p><strong>1월 9일 ~ 1월 10일 : 레일 조명 설치하기</strong></p></blockquote><p>역시 방 인테리어는 <strong>조명빨</strong> 아니겠는가?</p><p>패기있게 요즘 유행하는 레일조명을 LED램프로 구매했다. (대부분의 카페에서 사용하는 예쁜 조명)</p><p>LED 전구는 개당 12W의 밝기를 가지고 있으며 6개니 총 72W. 방 전체를 커버할 수 있을까 의심이 되었지만.. 하나 더 사기에는 자금이 ㅠㅠ 그래서 일단 달아보고 판단하기로 했다.</p><p>레일 조명 설치는 <a href="https://ohou.se/advices/24" rel="external nofollow noopener noreferrer" target="_blank">실전! 레일조명 설치해보기, 오늘의집</a> 포스트를 참고했다.</p><p>작업 순서는 대략적으로 아래와 같이 진행했다.</p><ol><li>기존 등을 조심스럽게 제거한다.</li><li>전원선의 상태를 확인하고 이를 기준으로 레일 등의 위치를 정한다.</li><li>천장에 레일을 설치한다.</li><li>레일 시작점 부분까지 전원선을 확장하여 연결한다.</li><li>레일에 조명을 달고 마무리한다.</li></ol><p>우리 집의 기존 조명은 그냥 일반적인 형광등이었다. 분해를 위해 주인아저씨께 전동 드라이버를 빌렸다.</p><p><img src="https://user-images.githubusercontent.com/25416425/51259295-2e872e00-19ef-11e9-855c-217d0103b1fe.jpg" width="350"></p><p>이걸 분해하면 전원선이 나오는데, 너무 전원선과 멀리 설치를 하면 연장선이 길어져서 보기 싫다고 한다. </p><p>그래서 적당한 거리를 두고 ㄱ자 형태로 레일 조명을 설치하기로 결정!</p><p><img src="https://user-images.githubusercontent.com/25416425/51259784-33001680-19f0-11e9-8f5e-e5c338a0261c.jpg" width="500"></p><p>초록색 선이 전원선. 떼어내니까 주변 벽지들이 제대로 도배가 안되어 있었다.. 보수 필요</p><p>근데.. 문제가 생겼다. 포스트를 보면 전원선을 길게 뽑아낼 수 있던데, <strong>저 사진에 보이는 길이가 한계였다!!</strong></p><p>전원선을 많이 연장해야 해서 다른 전선이 필요했다. 포스트에서는 기존 조명에서 전선을 떼어다가 전기테이프로 이어붙이라고 하던데.. 기존 전선의 길이들이 전부 답이 없게 짧았다 ㅠㅠ</p><p>이미 밤 11시라 주변 철물점들은 전부 문을 닫아서, 일단 놔둬놓고 레일부터 고정을 해보기로 했다.</p><p>목공용 드릴인가? 드릴이 생각보다 잘 안뚫리더라</p><p><img src="https://user-images.githubusercontent.com/25416425/51260094-d81aef00-19f0-11e9-9996-65724783d11d.jpg" width="500"></p><p>그리고 결론적으로 오늘 작업은 더이상 못하게 되었다.. </p><p>레일 조명을 설치할때, 주의할 점은 바로 <strong>천장의 상태이다.</strong> 레일이 단단히 고정되어야 하는데, 천장이 석고라면 떨어질 위험이 있어서 나무 천장에다가 박아야 한다.</p><p>근데 천장을 톡톡 치고 나무임을 확인한 후에 레일을 박으려 했는데 드릴에 문제가 있는건지 <strong>나사가 박히지 않는것이다..!!</strong> 그러다가 홧김에 다른 곳에다 드릴을 댔는데 여기는 석고였는지 구멍이 뻥 뚫렸다 으악!</p><p>얼른 실리콘으로 막아버리고 전선도 없겠다 포기 선언. 다음날 그냥 전문가를 부르기로 했다..</p><p>다음날 출장 설치 오신 <a href="https://m.blog.naver.com/nhuh11/221358036698" rel="external nofollow noopener noreferrer" target="_blank">한빛전기조명설비</a> 전문가님. 네이버 블로그의 후기들을 여러개 참고해서 괜찮은 분을 선택해서 전화 신청했다.</p><p>역시 전문가님! 30분이 채 걸리지 않았다.</p><p><img src="https://user-images.githubusercontent.com/25416425/51261041-e2d68380-19f2-11e9-9b2a-45abe220343f.jpg" width="500"></p><p>너무 깔끔하게 작업해주신 전문가님. 출장비는 5만원 들었다. 레일 조명은 조금 복잡한 작업이라 일반 조명보다 비싸게 받으신다고 하더라. </p><p>옆에서 보았는데, 우리가 절대 따라할 수 없는 퀄리티의 작업이었다 ㅋㅋㅋㅋ</p><p>직접 조명 설치는 실패했지만, 아무튼 조명 설치 끝! 레일 조명을 혼자 작업하고 싶다면 좋은 드릴과 예비 전선을 구비하고 시도하시길.</p><h3 id="가구-조립하기"><a href="#가구-조립하기" class="headerlink" title="가구 조립하기"></a>가구 조립하기</h3><blockquote><p><strong>1월 11일 : 책상과 선반 조립하기</strong></p></blockquote><p>전체적인 방 리폼은 끝났다. 벽지는 너무 넓고 힘들것 같아서 패스 (작은방만 작업했다)</p><p><strong>두닷 (Dodot)</strong>에서 주문한 1800 X 800 사이즈의 컴퓨터 책상이 도착했다. (두닷 콰트로 1808 아이보리 책상)</p><p><img src="https://user-images.githubusercontent.com/25416425/51265184-8c217780-19fb-11e9-9f4f-81d6d2f48108.jpg" width="500"></p><p>예전에는 1600 X 700 책상을 사용했는데, 확실히 크다. 큼지막해서 마음에 들고, 딱 봐도 엄청 튼튼해 보인다.</p><p>모니터를 여러개 놓을꺼라, 아무래도 전문가의 손길이 필요할 것 같아서 9천원을 추가해서 조립 포함하는 플랜으로 구매했었다.</p><p>그리고 동시에 도착한 일명 <strong>짭케아 철제 서랍장</strong>. 싼데는 이유가 있는듯. 조립하기 매우매우 귀찮다 ㅋㅋㅋ</p><p><img src="https://user-images.githubusercontent.com/25416425/51265568-6ba5ed00-19fc-11e9-9f67-b4ac9933b582.jpg" width="500"></p><p>일일히 서랍 하나하나를 만들어주어야 한다. 조립하는데 꽤나 애먹은 제품.</p><p>완성하고 보니 모양은 봐줄만한데, 마감이 그리 좋지는 않다.</p><blockquote><p>새로 리폼한 방의 첫 가구들</p></blockquote><p><img src="https://user-images.githubusercontent.com/25416425/51266034-80cf4b80-19fd-11e9-8a3d-3211a1fc5a83.jpg" width="500"></p><p>여기에다가 침대와 책장을 놓고, 옷걸이용 행거를 설치할 예정이다.</p><p>생각보다 넓어서 아늑한 느낌은 아니지만, 약간 스튜디오 느낌이 나는 방이 될 것 같다 ㅋㅋㅋ 다음 포스트는 최종 마무리로!</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;처음-시도하는-셀프-인테리어-도전기-3탄-조명-설치하기&quot;&gt;&lt;a href=&quot;#처음-시도하는-셀프-인테리어-도전기-3탄-조명-설치하기&quot; class=&quot;headerlink&quot; title=&quot;처음 시도하는 셀프 인테리어 도전기 3탄, 조명 설치하기&quot;&gt;&lt;/a&gt;처음 시도하는 셀프 인테리어 도전기 3탄, 조명 설치하기&lt;/h2&gt;&lt;p&gt;셀프 인테리어 세 번째 포스트. (1탄 포스트 &lt;a href=&quot;https://jeongwookie.github.io/2019/01/05/190105-house-remodeling-1/&quot;&gt;링크&lt;/a&gt;, 2탄 포스트 &lt;a href=&quot;https://jeongwookie.github.io/2019/01/09/190109-house-remodeling-2/&quot;&gt;링크&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;지금까지 진행한 사항을 아래에 간단히 정리해 보면,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;방 두개 문 &amp;amp; 굽도리 페인트칠 완료&lt;/li&gt;
&lt;li&gt;방문 문고리 부착 완료&lt;/li&gt;
&lt;li&gt;화장실 문 1/3 완료&lt;/li&gt;
&lt;li&gt;방 두개 데코타일 시공 완료&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;이제 남은 작업은 조명 설치와 가구 조립.&lt;/p&gt;
&lt;p&gt;1월 8일은 그전날 바닥 작업의 여파로 쉬었습니다..ㅠㅠ (진짜 완전 힘듬)&lt;/p&gt;
    
    </summary>
    
      <category term="Hobby" scheme="https://jeongwookie.github.io/categories/Hobby/"/>
    
    
      <category term="Daily life" scheme="https://jeongwookie.github.io/tags/Daily-life/"/>
    
      <category term="Self Interior" scheme="https://jeongwookie.github.io/tags/Self-Interior/"/>
    
      <category term="Computer room" scheme="https://jeongwookie.github.io/tags/Computer-room/"/>
    
  </entry>
  
</feed>
